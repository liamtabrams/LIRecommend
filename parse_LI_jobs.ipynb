{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYfV_TEsk400",
        "outputId": "b15358a9-dfa0-4a9c-dde9-b65ce80045b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n            Seniority level\\n          ']\n",
            "['\\n            Not Applicable\\n          ']\n",
            "['\\n            Employment type\\n          ']\n",
            "['\\n            Full-time\\n          ']\n",
            "['\\n              Job function\\n            ']\n",
            "['\\n              Engineering\\n            ']\n",
            "['\\n              Industries\\n            ']\n",
            "['\\n            Construction and Machinery Manufacturing\\n            ']\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import requests\n",
        "\n",
        "# request web page\n",
        "resp = requests.get(\"https://www.linkedin.com/jobs/view/3837175965/?alternateChannel=search&refId=gEpk5%2BQwe78lnqcwlOXxOA%3D%3D&trackingId=3Z6zBAIEXLuPRfFPctXz5A%3D%3D\")\n",
        "\n",
        "# get the response text. in this case it is HTML\n",
        "html = resp.text\n",
        "\n",
        "# parse the HTML\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "spans = soup.find_all('span', {'class': \"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
        "for span in spans:\n",
        "  #for key in span.__dict__.keys():\n",
        "  #print(f\"key {key}\")\n",
        "  #print(f\"value {span.__dict__[key]}\")\n",
        "  parent_tags = span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"})\n",
        "  for tag in parent_tags:\n",
        "    print(tag.contents)\n",
        "  #print(span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"}))\n",
        "  print(span.contents)\n",
        "\n",
        "\n",
        "#items = soup.select('option[value]')\n",
        "#print(items)\n",
        "'''links = soup.find_all(\"a\") # Find all elements with the tag <a>\n",
        "for link in links:\n",
        "  print(\"Link:\", link.get(\"href\"), \"Text:\", link.string)'''\n",
        "\n",
        "\n",
        "# print the HTML as text\n",
        "x = soup.get_text().split('\\n')\n",
        "\n",
        "characters_per_line = []\n",
        "\n",
        "# Extract text content from the HTML\n",
        "text_content = soup.get_text()\n",
        "\n",
        "# Split the text into lines\n",
        "lines = text_content.splitlines()\n",
        "\n",
        "# Calculate the number of characters in each line\n",
        "for line in lines:\n",
        "  characters_per_line.append(len(line))\n",
        "\n",
        "'''for i in set(characters_per_line):\n",
        "  if i>50:\n",
        "    list_index = characters_per_line.index(i)\n",
        "    print(characters_per_line[list_index])\n",
        "    print(lines[list_index])'''\n",
        "\n",
        "body_len = max(characters_per_line)\n",
        "body_idx = characters_per_line.index(body_len)\n",
        "body_text = lines[body_idx]\n",
        "#print(body_text)\n",
        "\n",
        "\n",
        "# Remove elements with only whitespace\n",
        "filtered_list = [string for string in x if string.strip()]\n",
        "\n",
        "#print(filtered_list)\n",
        "\n",
        "for i in filtered_list:\n",
        "  if i.find('Join now') != -1 and filtered_list[filtered_list.index(i) + 1].find('Sign in') != -1:\n",
        "    position = filtered_list[filtered_list.index(i) + 2]\n",
        "    company = filtered_list[filtered_list.index(i) + 3]\n",
        "    location = filtered_list[filtered_list.index(i) + 4]\n",
        "    break\n",
        "\n",
        "for i in filtered_list:\n",
        "  if i.find('Base pay range') != -1:\n",
        "    salary_index = filtered_list.index(i) + 1\n",
        "    salary = filtered_list[salary_index]\n",
        "    salary = salary.lstrip()\n",
        "    salary = \"N/A\" if (salary.find(\"$\") == salary.find(\"€\") == salary.find(\"£\") == -1) else salary\n",
        "    break\n",
        "  else:\n",
        "    salary = \"N/A\"\n",
        "\n",
        "\n",
        "#print(f\"position is {position}\")\n",
        "#print(f\"company is {company}\")\n",
        "#print(f\"location is {location}\")\n",
        "#print(f\"salary is {salary}\")\n",
        "\n",
        "\n",
        "#print(soup)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get position, company, location, pay\n",
        "\n",
        "x = soup.get_text().split('\\n')\n",
        "\n",
        "# Remove elements with only whitespace\n",
        "filtered_list = [string for string in x if string.strip()]\n",
        "\n",
        "for i in filtered_list:\n",
        "  if i.find('Join now') != -1 and filtered_list[filtered_list.index(i) + 1].find('Sign in') != -1:\n",
        "    position = filtered_list[filtered_list.index(i) + 2]\n",
        "    company = filtered_list[filtered_list.index(i) + 3]\n",
        "    location = filtered_list[filtered_list.index(i) + 4]\n",
        "    break\n",
        "\n",
        "for i in filtered_list:\n",
        "  if i.find('Base pay range') != -1:\n",
        "    salary_index = filtered_list.index(i) + 1\n",
        "    salary = filtered_list[salary_index]\n",
        "    salary = salary.lstrip()\n",
        "    salary = \"N/A\" if (salary.find(\"$\") == salary.find(\"€\") == salary.find(\"£\") == -1) else salary\n",
        "    break\n",
        "  else:\n",
        "    salary = \"N/A\"\n",
        "\n",
        "\n",
        "print(f\"position is {position}\")\n",
        "print(f\"company is {company}\")\n",
        "print(f\"location is {location}\")\n",
        "print(f\"salary is {salary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGqNfJxy7hNG",
        "outputId": "bc3ac090-e265-409f-aa05-150e786c7c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "position is Embedded Cybersecurity Software Engineer\n",
            "company is Caterpillar Inc.\n",
            "location is Chillicothe, IL\n",
            "salary is N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get main body text\n",
        "\n",
        "characters_per_line = []\n",
        "\n",
        "# Extract text content from the HTML\n",
        "text_content = soup.get_text()\n",
        "\n",
        "# Split the text into lines\n",
        "lines = text_content.splitlines()\n",
        "\n",
        "# Calculate the number of characters in each line\n",
        "for line in lines:\n",
        "  characters_per_line.append(len(line))\n",
        "\n",
        "'''for i in set(characters_per_line):\n",
        "  if i>50:\n",
        "    list_index = characters_per_line.index(i)\n",
        "    print(characters_per_line[list_index])\n",
        "    print(lines[list_index])'''\n",
        "\n",
        "body_len = max(characters_per_line)\n",
        "body_idx = characters_per_line.index(body_len)\n",
        "body_text = lines[body_idx]\n",
        "print(body_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZV726Fj8DpJ",
        "outputId": "d6161a1f-db04-4998-b6eb-f1c5691b58d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Career Area: Engineering Job Description:  Your Work Shapes the World at Caterpillar Inc. When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it. Your Work Shapes the World Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.When you join Caterpillar, you are joining a team of makers, innovators, and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that, because it helps our customers build and power the world we live in – the roads, hospitals, homes, and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us. Description Come work on the Embedded Product Cybersecurity team for Caterpillar machines & engines. Our team is developing embedded cybersecurity solutions & software for Caterpillar Display, Telematics, Machine, Engine, & Autonomy product lines that will be used for years to come.The ideal candidate will be passionate about developing cybersecurity software solutions for Caterpillar’s embedded product line. They must also be willing to learn new areas of expertise in cybersecurity while working with multiple systems, software, component, and product teams to produce world class cybersecurity solutions for Caterpillar.You will help design, develop, configure, and test our security-enabling software that targets a wide range of scopes, including full-stack ECU component software, system-wide onboard security software, offboard internal development tools, and ECU supplier manufacturing tools. You will participate in iterative development and fast delivery of features utilizing up-to-date technologies and practices like Linux, Python, C/C++, Agile, and emergent design. Join us and be a part of this exciting team! Job Duties This role will be responsible for developing, designing, implementing, and testing software of embedded devices and systems; as well as monitoring and enhancing the efficiency and stability of the systems. Specific Duties And Responsibilities:  Gathering and analyzing user/client requirements to create the software requirements specification (SRS) document.  Writing and implementing source codes of embedded systems and enhancing code samples of existing systems.  Testing and debugging embedded system software using different tools/methods available to improve code quality and optimize system performance.  Collaborating with other teams to provide post-production support.  Design & Document Cybersecurity features  Develop Embedded Cybersecurity software solutions that will be implemented into production software  Validate Embedded Cybersecurity software features used across Caterpillar’s product line  Knowledge and capability to identify Cybersecurity software risks  Skill Descriptors  Effective Communications:  Understanding of effective communication concepts, tools and techniques; ability to effectively transmit, receive, and accurately interpret ideas, information, and needs through the application of appropriate communication behaviors. Level Working Knowledge:  Delivers helpful feedback that focuses on behaviors without offending the recipient. Listens to feedback without defensiveness and uses it for own communication effectiveness. Makes oral presentations and writes reports needed for own work. Avoids technical jargon when inappropriate. Looks for and considers non-verbal cues from individuals and groups. Teamwork:  Knowledge of the necessity and value of teamwork; experience with; ability to work cooperatively towards shared goals and being supportive of others at all levels. Level Working Knowledge:  Explains own role and responsibility within team. Actively participates in team meetings. Shares information, knowledge, and experiences openly and proactively. Describes team mission and objectives in the context of results to be achieved. Demonstrates open, friendly, accepting, and supportive behaviors with team members. Software Development:  Knowledge of software development tools and activities; ability to produce software products or systems in line with product requirements. Level Working Knowledge:  Describes common tools for component-based, object-oriented development. Describes the objectives, activities and results of unit testing. Has developed programs in a specific language and for a specific platform. Interprets functional and technical blueprints; participates in structuring technical components. Participates in technical and code reviews. System Testing:  Knowledge of system and software testing; ability to design, plan and execute system testing strategies and tactics to ensure the quality of software at all stages of the system life cycle. Level Working Knowledge:  Supports the project leader in developing and executing system test plans. Evaluates system documentation and user manuals for usability, accuracy and completeness. Executes test cases, analyzes test results and reports on findings regularly. Tests system components for compliance with functional requirements. Participates in the testing of a system's ability to recover from hardware or software failures. Technical Troubleshooting:  Knowledge of technical troubleshooting approaches, tools and techniques; ability to anticipate, recognize, and resolve technical issues on hardware, software, application or operation. Level Working Knowledge:  Discovers, analyzes, and resolves hardware, software or application problems. Works with vendor-specific diagnostic guides, tools and utilities. Handles calls related to product features, applications, and compatibility standards. Analyzes code, logs, and current systems as part of advanced troubleshooting. Records and reports specific technical problems, solving processes and tools that have been used. Basic Qualifications  BSEE, BSCE, or BSCS  2+ years of development experience using C++, C, and/or Java programming languages  2+ years of experience with scripting using Python  1 year experience working in the Linux environment  1 year experience with Ethernet, TCP/IP, Wi-Fi, and analysis tools such as Wireshark  Prior experience with embedded software development, design, and architecture  Top Candidates will also have  Desire to work in a fast-paced Agile team environment  Experience in Cybersecurity  Experience with GIT configuration management tool  Ability to read electrical schematics  Experience with Linux kernel & Linux device drivers  Experience with RTOS development (i.e. Free RTOS)  Experience with CAN, J1939, and other data link protocols  Experience using Design Patterns and Object Oriented programming  Experience with developing Unit Tests and Test Driven Development  Experience using debugging tools for embedded systems (e.g. Lauterbach, GDB)  Additional Information The location for this position is Mossville, ILDomestic relocation assistance is available for this position.This position will require less than 10% travel Employee benefit details Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.Here you earn more than just a salary because we value your performance. We offer a total rewards package that provides benefits on day one (medical, dental, vision, RX, and 401K) along with the potential of an annual bonus.Additional benefits include paid vacation days and paid holidays (prorated based upon hire date). Final details Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at   www.Caterpillar.com/Careers   . For more information, visit caterpillar.com. To connect with us on social media, visit caterpillar.com/social-media Posting Dates: February 21, 2024 - March 6, 2024Any offer of employment is conditioned upon the successful completion of a drug screen.EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.Not ready to apply? Join our Talent Community .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get seniority level, employment type, job function, and industries\n",
        "\n",
        "spans = soup.find_all('span', {'class': \"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
        "for span in spans:\n",
        "  #for key in span.__dict__.keys():\n",
        "  #print(f\"key {key}\")\n",
        "  #print(f\"value {span.__dict__[key]}\")\n",
        "  parent_tags = span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"})\n",
        "  for tag in parent_tags:\n",
        "    print(tag.contents)\n",
        "  #print(span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"}))\n",
        "  print(span.contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rGfdU9b8VYy",
        "outputId": "6169f27a-6bf4-42a5-9a2c-8a3e1c1c2699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n            Seniority level\\n          ']\n",
            "['\\n            Not Applicable\\n          ']\n",
            "['\\n            Employment type\\n          ']\n",
            "['\\n            Full-time\\n          ']\n",
            "['\\n              Job function\\n            ']\n",
            "['\\n              Information Technology\\n            ']\n",
            "['\\n              Industries\\n            ']\n",
            "['\\n            Technology, Information and Internet\\n            ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've tried and tried and tried to extract the work location (remote, hybrid, or on-site) but did not succeed. I wanted to since it definitely affects job desirability, particularly for locales that I'm not crazy about (I wouldn't mind working for a company in Nebraska if I can do the job from California). Since this info is probably not one of the more important features though (or at least I don't see how it would have a strong relationship with job function or duties) I am going to move on to scraping other data. It will also be possible to infer this info for some of the job postings and add it to the dataframe (features matrix) later on with the LLM summarizing the body text, so I don't think it's a big deal that I cannot extract this right now.\n",
        "\n",
        "So what else are we still missing? Number of employees and company's industry is still missing, but that's not super useful information for our purposes. We'll at least attempt to extract that info but if we can't no biggie."
      ],
      "metadata": {
        "id": "C4aKWukh8ssv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The info mentioned above has the same story as the work location; I cannot figure out how to extract it. We should focus on extracting the skills list given in the top section of the job posting on LinkedIn. Hopefully we can do this, but if not we may have to keep trying with beautiful soup or parsing the html some other way, or we might consider leveraging the LinkedIn API repo on GitHub."
      ],
      "metadata": {
        "id": "1dJ-1NxAGKFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that the information I am trying to extract programmatically with the requests library is only visible while logged into LinkedIn. Just looking at the job posting from a browser where I haven't logged into LinkedIn, that information is not present and this is why I am not able to extract using the method I have relied on so far (simple requests and parsing html). The information I want that I see while logged into LinkedIn is not merely buried within the html source and requires more work; it is simply not there. So I don't think Selenium makes getting the info I have been failing to collect any more straightforward (although there is probably a way to log into LI with requests or with Selenium and then iterate over the job postings to extract the desired stuff). It is probably easier to log into LinkedIn using some LinkedIn API library, though there may not be any other useful info from the job post the API will allow us to get that we haven't already gotten while not logged in (via basic request.get of url). If we don't go the extra mile to extract these extra fields, I think it will be fine, though our model probably won't be as strong. I think there is a lot of good info in the body that we can use OpenAI or something equivalent to summarize and do further feature extraction.\n",
        "\n",
        "I also think we could go on and on and on enhancing and growing our methods just in the data collection phase to the point of delaying project completion by weeks or even months; we haven't even worked with the LLM yet to do text summarization. I think it would be wiser to move onto that next step because it would achieve a lot more of the feature extraction for us; this will allow us to get a working protoype running sooner, even if the data isn't even close to being optimized. That way we could get version 1 of the project done sooner and enjoy our work more. Then I can always come back to it after the first version of this project is up and running and improve the data collection phase, then retrain models, and modify the whole pipeline as needed. I think Semih would agree. Focus on prototyping now even if the data isn't great."
      ],
      "metadata": {
        "id": "u6_dN_ZLG4Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the next step will be to generate a pandas dataframe with the following fields as features:\n",
        "\n",
        "position, company, location, salary range, seniority level, employment type, job function, industries, description.\n",
        "\n",
        "From there, the LLM we use will summarize the description field to generate new fields (make more features) and we should think carefully about what additional info we want the LLM to extract from the description and populate additional columns with; let us do this by analyzing what information is in the test job posting (Robotics Engineer - Surgical Robotics , Barrington James). It would be prudent to look at several example postings so that we have good coverage of the possibilities. After we figure out and decide exactly what the LLM should look for in each body text, we will need to be clever and very explicit about the prompt we send it, as well as careful about how we process the response in order to further extract features; once we've brainstormed and prototyped this step we should come back to the previous step involving web scraping and test our methods out on other job postings to make sure they are robust. But for now we will brainstorm the goal of the prompt we send to the LLM. Fun!"
      ],
      "metadata": {
        "id": "HKrKT19UXR3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So after brainstorming and trying to encompass all information that could possibly be conveyed in the job posting, I am thinking that the final dataframe after feature extraction should have 20 columns, 1 of them being the target variable (job desirability) which is the manual label for each job posting. The fields should look something like:\n",
        "\n",
        "\n",
        "```\n",
        "FEATURE,example entry\n",
        "Seniority level,Mid-Senior level\n",
        "Employment type,Full-time\n",
        "Job function,Engineering and Information Technology\n",
        "Industries,Services for Renewable Energy\n",
        "position name,Robotics Engineer – Surgical Robotics\n",
        "company,Barrington James\n",
        "location,\"New York, United States\"\n",
        "salary/compensation range,\"$130,000.00/yr - $200,000.00/yr\"\n",
        "Company description,\n",
        "Responsibilities,\n",
        "Goals/objectives,\n",
        "Description of team/who you will be working with,\n",
        "Required qualifications,\n",
        "Preferred qualifications,\n",
        "Benefits,\n",
        "Work schedule/hours,\n",
        "Travel part of job?,\n",
        "\"Work arrangement (hybrid, on-site, remote?)\",\n",
        "Other/misc info,\n",
        "Desirability rating (1-3) (manually labelled target),3\n",
        "```"
      ],
      "metadata": {
        "id": "AYJfeMp93CY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See these screenshots I've uploaded to Drive showing my first attempt at getting ChatGPT to summarize and organize the info relevant to these fields in a table, from the body text concatenated with the other info I was able to scrape using requests and beautifulsoup. : https://drive.google.com/drive/folders/1XUGI23L5b4LnqQ9gTBb7vPrAgC6iBNCs"
      ],
      "metadata": {
        "id": "KPt15PVy0zEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After some trial-and-error experimentation with asking ChatGPT to extract and organize the info, I have gotten the best output with the following prompt:\n",
        "\n",
        "\"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, Industries, position name, company, location, salary/compensation range, most notable Responsibilities, key Goals/objectives, name of department or team, Most notable qualifications, most notable Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put \"N/A\". Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Here is the text from the job posting:  \n",
        "\"job posting text\" \"\n"
      ],
      "metadata": {
        "id": "jPI1pbUVH2s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which yielded the output shown here: https://drive.google.com/drive/folders/1-SwA1jNScYyt3v7dAOjYovhAf3nAb8L7 where \"job posting text\" was the following:\n",
        "\n",
        "\"Seniority level: Not Applicable\n",
        "Employment type: Full-time\n",
        "Job function: Information Technology, Consulting, and Engineering\n",
        "Industries: Software Development, IT Services and IT Consulting, and Technology, Information and Internet\n",
        "\n",
        "posting name: SDE - Amazon Robotics, Robotic\n",
        "company: Amazon\n",
        "location: North Reading, MA\n",
        "salary: N/A\n",
        "\n",
        "DescriptionDo you want to have a worldwide impact in Robotics? The Vulcan Stow team at Amazon Robotics builds high-performance, real-time robotic systems that can perceive, learn, and act intelligently alongside humans—at Amazon scale. We invent and scale AI systems for robotics in fulfillment.We are seeking software engineers to help with our initial robotic deployments. This includes building computer vision systems, ML and AI models, robotic control and motion planning, and process management. It also includes end-to-end ownership of decision explanation, fault detection, monitoring, A/B testing, large scale model training, simulation, hardware integration, and more. This work spans prototypes in the lab as well as wide-deployment systems. As a software engineer, you will help plan the roadmap, design ML systems, implement, test, and monitor services in our robotic fleet.We are open to hiring candidates to work out of one of the following locations:North Reading, MA, USABasic Qualifications 3+ years of non-internship professional software development experience 2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience Experience programming with at least one software programming language Experience programming with at least one modern language such as Java, C++, or C# including object-oriented design 1+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems.Preferred Qualifications 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience Bachelor's degree in computer science or equivalent Robotics experience along with experience with one of the following: * Production-level machine learning * Computer vision (e.g. a variety of neural network architectures using TensorFlow or MxNet, fusion with other sensors) * Hardware integration (e.g. hardware release cycles, heterogeneous hardware fleets) * Building and testing real-time or safety-critical systemsAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.Company - Amazon.com Services LLCJob ID: A2423847\""
      ],
      "metadata": {
        "id": "pAnEhd6qNYZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'm going to look at the original job posting and see what valuable information may have been lost in this process, and there is a fair amount. The information that was lost includes:\n",
        "\n",
        "-description of good or service you will work on\n",
        "-some details of the job (objectives)\n",
        "-broader role name (e.g. SW Engineer)\n",
        "-some responsibilities\n",
        "-some qualifications, and preferred qualifications and requiried/basic get mixed together.\n",
        "\n",
        "In light of this, we will revise our prompt so that it doesn't lose these valuable details and so that it will include new fields for the following:\n",
        "\n",
        "-Description of product/service\n",
        "-broader role name\n",
        "-split most notable qualifications into required and preferred qualifications\n",
        "\n",
        "We will try the following revised prompt:\n",
        "\n",
        "\"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put \"N/A\". Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Here is the text from the job posting:\n",
        "\"job posting text\" \""
      ],
      "metadata": {
        "id": "aU3Ysg_TPVRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my opinion, this seemed to produce better output. Follow this link for a screenshot of that output: https://drive.google.com/drive/folders/1vt4jhysrwpscVEipweApasw8K1z5tyoA"
      ],
      "metadata": {
        "id": "KEOoKH_JUtnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no doubt still some information loss and there are mistakes in the categorizations, but I don't think we can make the output much better. We will test out this revised prompt with another sample of scraped text. Here is the scraped text:\n",
        "\n",
        "\"Seniority level: Entry level\n",
        "Employment type: Full-time\n",
        "Job function: Engineering and Information Technology\n",
        "Industries: Business Consulting and Services\n",
        "\n",
        "posting name: Python Software Engineer (Robotics/Mechatronics)\n",
        "company: Fresh Consulting\n",
        "location: Redmond, WA\n",
        "salary: $30.00/hr - $45.00/hr\n",
        "\n",
        "Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment. See more at freshconsulting.comVisit freshconsulting.com/portfolio to see our project work across several industries.Title: Python Software Engineer (Robotics/Mechatronics)Duration: Project Based Long-Term Vendor ContractLocation: Redmond, WABenefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan and much more!Hours: Minimum 40 Hours/WeekRole Work with the hardware and software teams to integrate various software/hardware components and develop UI front-end for test stations. Integrate motion, lighting, and imaging controllers into test suites. Use technical expertise to investigate, troubleshoot, and verify defects. Be an active team player, while working independently. Proactively reach out to partners and ask relevant questions when blocked. Learn new systems and tools. Document and improve code quality. Working on python and computer vision applications.The Position - Focus on development and execution of test station applications for System validation for AR/VR devices. The Ideal Person will be a software developer with either robotics and/or mechatronics background that have built test stations for manufacturing specifically optical products.Skills 0-1+ years experience. Clear communication, common sense, smart and outside the box thinking. Very strong Python skills. Strong programming and problem solving skills are a must. Understanding of HW, Embedded Systems, Mechatronics, Robotics and related. Understanding of hardware/software interoperability. Understanding of software engineering principles, code version control, and Python. Strong troubleshooting skills to root cause issues. Self-starter, and someone who when they get stuck is willing to ask for help and not just flail around trying to solve it themselves. Good communication skills, both written and oral. Works well with other people and a willingness to learn ones.Education: BSCSE, MSCSE preferred.FRESH-- Work on engineering and research assignments with F500 companies and startups. The relationships that we have created with our clients are one of a kind. We help solve problems in many technologies focusing on R&D, product development, and manufacturing. We work at the most cutting-edge and latest technologies from AR/VR to Autonomous technologies. Closely working with our clients, we believe that long term investments are extremely important to maintain the culture we together have created.We’re a handpicked team of Engineers, digital strategist, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.Fresh Consulting is a participating E-Verify company.freshconsulting.comCompensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $30/hr - $45/hr+\""
      ],
      "metadata": {
        "id": "JBY0IPwvXvlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got decent output but some of the statements/sentences exceeded 5 words. See the screenshot linked here: https://drive.google.com/drive/folders/1GC2M-c90B4JE69dNOlakyuGbXcvdatW4. We need to be more explicit about this constraint in our prompt.\n",
        "\n",
        "We tried revising this specific instruction in the prompt in various ways but we weren't able to force Chat to output pieces of information that are always 5 words or less. We might have to just tolerate this and throw away pieces of information that exceed 5 words when processing our data into something we can feed a decision tree. This is unfortunate but hopefully won't harm model performance too much. We shouldn't labor over generating our text data too much because we will probably need to devote a lot time devising a strategy/mechanism for encoding sets of words up to 5 and we need order not matter. Maybe we can just use bag of words in some way? But maybe using TFIDF or something similar would be better. How we are going to mathematically represent textual data for each column is still extremely murky. One thing is clear: we are going to have a LOT of columns if we do this. Luckily, the plan is not to have too many rows or job postings in the dataset. I'm thinking something on the order of a few hundred, but that expectation could very well change. Maybe we will decide to do something simpler and not use ChatGPT at all for feature extraction, but instead do something more basic with count vectorization of words (like BOW or TFIDF) on the entire body text of the job posting so that instead of having potentially 10 to 20 x N columns where N is the size of the vocab space, we only need N columns plus a few more for the designated pieces of information we ARE able to scrape like position name, company, locale, salary. Again, this would take ChatGPT or other LLM-based summarization task out of the data processing step and would substantially shrink the complexity of the dataset/feature space. I'm just afraid that going too simple will result in a crap model."
      ],
      "metadata": {
        "id": "1KCwF2X2ZsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This got me thinking about how to represent sets of up to 5 numbers (positive integers to be specific) as unique numbers, or what 1-1 function I can use to map such a set of words (ignoring their order so that we can assume that different permutations of the same set of 1-5 words convey the same info, which will help limit the size of the feature space) to a number space. So I asked ChatGPT for the answer, and it came up with an application of Cantor's rule, which I thought was really cool! Here is a screenshot of the conversation: https://drive.google.com/drive/folders/1TUHwzsnF2vTZboaJk-W2dCd6m54EIWOy\n"
      ],
      "metadata": {
        "id": "Ri8481wJngl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to start seriously collecting and processing data. Since our methods of extracting the desired text from the html only works on a handful of job postings in our original list, we need to manually test each one and find a respectable number of them that pass the test so we can automatically prepare a text file for each posting using the scraping logic contained herein, and then we can read each text file into its corresponding prompt string. Then we will still have a lot of work to do as far as taking ChatGPTs response in the form of json or loading it into a data structure some other way and adding that data as a row into our final dataframe. I will make a new spreadsheet to keep track of these postings, which will contain only LinkedIn Jobs links and a rating for each."
      ],
      "metadata": {
        "id": "tvgbxITBpSw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will just be rerunning the scraping code from the beginning of the notebook with different links in requests.get() to see which of those links get successfully scraped. I will keep track of the ones that work and make sure each has a rating in the new spreadsheet."
      ],
      "metadata": {
        "id": "BnPGfqTOayBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give an idea of how quickly I can rate and test job posting links, I have gotten about 35 done in about 3 hours. So if I spend another 8 hours on it I should have over 100. I think 100 might be enough to start experimenting with. I am making sure to have examples in all four classes: (0, no interest - 1, a little interest - 2, some interest - 3, a lot of interest)."
      ],
      "metadata": {
        "id": "CgEo--5MtLI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update: I've done about 25 more jobs since my last update which was about 2 hrs ago. I also got sidetracked with some other small tasks so I was probably only working on collecting links for half the time. So a rough estimate is that I am able to get through about 20 postings in an hour. So it shouldn't take me TOO much time to get 100-200 links."
      ],
      "metadata": {
        "id": "lfL4n3xtKZ7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another update: I have gathered 107 links with labels. I am now going to start writing the code to scrape from all of these links using the csv file, and save to text file, and add the text file path to a third column in the csv file."
      ],
      "metadata": {
        "id": "ymPATiGnLDpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_job_postings(csv_file):\n",
        "    # Open the CSV file\n",
        "    with open(csv_file, 'r', newline='') as file:\n",
        "        reader = csv.reader(file)\n",
        "        #csv_writer = csv.writer(file)\n",
        "        row_num = 1\n",
        "\n",
        "        for row in reader:\n",
        "          text_file_path = f\"text_files2/row{row_num}.txt\"\n",
        "          with open(text_file_path, 'w') as file:\n",
        "            job_link = row[0] # Assuming the job links are in the first column\n",
        "            print(job_link)\n",
        "            # Send a GET request to the job link\n",
        "            # request web page\n",
        "            resp = requests.get(fr\"{job_link}\")\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "              # get the response text. in this case it is HTML\n",
        "              html = resp.text\n",
        "              # Parse the HTML content\n",
        "              soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "            else:\n",
        "              time.sleep(1)\n",
        "              # request again\n",
        "              resp = requests.get(job_link)\n",
        "              if resp.status_code == 200:\n",
        "                # get the response text. in this case it is HTML\n",
        "                html = resp.text\n",
        "                # Parse the HTML content\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "              else:\n",
        "                print(\"Failed to retrieve job posting from\", job_link)\n",
        "                row_num += 1\n",
        "                continue\n",
        "\n",
        "            # get position, company, location, pay\n",
        "            x = soup.get_text().split('\\n')\n",
        "            # Remove elements with only whitespace\n",
        "            filtered_list = [string for string in x if string.strip()]\n",
        "\n",
        "            for i in filtered_list:\n",
        "              if i.find('Join now') != -1 and filtered_list[filtered_list.index(i) + 1].find('Sign in') != -1:\n",
        "                position = filtered_list[filtered_list.index(i) + 2]\n",
        "                company = filtered_list[filtered_list.index(i) + 3]\n",
        "                location = filtered_list[filtered_list.index(i) + 4]\n",
        "                break\n",
        "\n",
        "            for i in filtered_list:\n",
        "              if i.find('Base pay range') != -1:\n",
        "                salary_index = filtered_list.index(i) + 1\n",
        "                salary = filtered_list[salary_index]\n",
        "                salary = salary.lstrip()\n",
        "                salary = \"N/A\" if (salary.find(\"$\") == salary.find(\"€\") == salary.find(\"£\") == -1) else salary\n",
        "                break\n",
        "              else:\n",
        "                salary = \"N/A\"\n",
        "\n",
        "            file.write(f\"position is {position}\\n\")\n",
        "            file.write(f\"company is {company}\\n\")\n",
        "            file.write(f\"location is {location}\\n\")\n",
        "            file.write(f\"salary is {salary}\\n\\n\")\n",
        "\n",
        "            #get seniority level, employment type, job function, and industries\n",
        "            spans = soup.find_all('span', {'class': \"description__job-criteria-text description__job-criteria-text--criteria\"})\n",
        "            for span in spans:\n",
        "              parent_tags = span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"})\n",
        "              for tag in parent_tags:\n",
        "                field = tag.contents[0].strip()\n",
        "                #print(span.parent.find_all(\"h3\", {'class': \"description__job-criteria-subheader\"}))\n",
        "              value = span.contents[0].strip()\n",
        "              file.write(f\"{field} is {value}\\n\")\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "            # get main body text\n",
        "            characters_per_line = []\n",
        "\n",
        "            # Extract text content from the HTML\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Split the text into lines\n",
        "            lines = text_content.splitlines()\n",
        "\n",
        "            # Calculate the number of characters in each line\n",
        "            for line in lines:\n",
        "              characters_per_line.append(len(line))\n",
        "\n",
        "            body_len = max(characters_per_line)\n",
        "            body_idx = characters_per_line.index(body_len)\n",
        "            body_text = lines[body_idx]\n",
        "            file.write(body_text)\n",
        "          row_num += 1\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "#csv_file_path = \"job_links.csv\"\n",
        "#scrape_job_postings(csv_file_path)"
      ],
      "metadata": {
        "id": "-MJwlw4-S2Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcCHirdhWds3",
        "outputId": "006ab738-b9c1-4b79-e070-d3cedb2633d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  successfulLinks.csv  text_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"successfulLinks.csv\"\n",
        "scrape_job_postings(csv_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJj1E5QgbcUg",
        "outputId": "16c31f02-8d55-4c53-e33f-23756236499a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.linkedin.com/jobs/view/3806930943/?alternateChannel=search&refId=lDlZJvG4OW0xRP7xt0RVlg%3D%3D&trackingId=qNRVFnO%2BQJyuaa2b%2FT8GiQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3810274485/?alternateChannel=search&trackingId=Pfa8QX7ZT2WW4HsSclTlwQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3688425969/?alternateChannel=search&trackingId=0obu3Q%2BRR2S0S1OgTqg3%2FA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3813623032/?alternateChannel=search&refId=Li9t%2BqLtLcGrKx9w7AU0TA%3D%3D&trackingId=gGm7qd5TyA2C%2BUZZSPfpLA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3827214551/?alternateChannel=search&refId=UhDr2lWBdoB%2FTVqatOBjmA%3D%3D&trackingId=82xHPXMSr0xmrV4QInnNVg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3834705114/?alternateChannel=search&refId=yMUzpNjEGPyqqP3Xeijg0w%3D%3D&trackingId=Y0kHs9dankN0IRhDY%2BPNFA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3827218722/?alternateChannel=search&refId=yMUzpNjEGPyqqP3Xeijg0w%3D%3D&trackingId=FBt%2BUmI7UGiu8ybYAgCJFw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3827218759/?alternateChannel=search&refId=ypGtOb2iR5KjpGGQAGfEAw%3D%3D&trackingId=mfkx5lAJpjiHebhpYrl8qg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832044163/?alternateChannel=search&refId=ypGtOb2iR5KjpGGQAGfEAw%3D%3D&trackingId=qr%2FcBo8pyfwaFyBz5fM40g%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3829104865/?alternateChannel=search&refId=ypGtOb2iR5KjpGGQAGfEAw%3D%3D&trackingId=DR69TQw9nJuwVnAA%2F8wXGA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833021103/?alternateChannel=search&refId=ypGtOb2iR5KjpGGQAGfEAw%3D%3D&trackingId=ttCQq11%2BOa8p7a%2BE24sqnQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3800542255/?alternateChannel=search&refId=%2Bncl7eRNa9%2BMS1QVluimzw%3D%3D&trackingId=eukoQd3qEPyDRkAwNcrO9A%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3829148019/?alternateChannel=search&refId=ypGtOb2iR5KjpGGQAGfEAw%3D%3D&trackingId=%2FyR%2B9J70vrKue7YH%2BB7FZg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3798383786/?alternateChannel=search&refId=bwOZQgtsTxculXdpC6O1ow%3D%3D&trackingId=1o5wbTkg32wS6r%2BHKfKbGg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835861845/?alternateChannel=search&refId=UtjC%2F%2BXx07gz0R2QDM3sQQ%3D%3D&trackingId=eJdnIEWFLgxbU1Xe3kzagg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836484328/?alternateChannel=search&refId=UtjC%2F%2BXx07gz0R2QDM3sQQ%3D%3D&trackingId=MvNbnuE3itRhq%2Flu6Svycw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836400608/?alternateChannel=search&refId=TPOQbsdwIQFUzI5LhBa1Xw%3D%3D&trackingId=0%2BQZcMPD404megytBymVmw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832170769/?alternateChannel=search&refId=TPOQbsdwIQFUzI5LhBa1Xw%3D%3D&trackingId=UIY%2FWQnnLgCkkgqVo9fl5Q%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3811198945/?alternateChannel=search&refId=FIhCBnGx7HZOR88JXte2iA%3D%3D&trackingId=UpTe%2B0J7h2XnuMSDxmWcwQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3808300009/?alternateChannel=search&refId=rSlIjH%2BfDQ86BxQTRFW8fQ%3D%3D&trackingId=q2sFdsciuMxIe4ZKG9Rddg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3689173593/?alternateChannel=search&refId=Oe%2FfkNyrHFnMLHyo%2BiLU4Q%3D%3D&trackingId=8FiTb4GszxTX46yNzGOZ9w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3441711629/?alternateChannel=search&refId=glSwFukC9bBcogczJg%2FVjA%3D%3D&trackingId=j%2FVC%2FGhD4g%2FMKmRFr0fpCg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3825906133/?alternateChannel=search&refId=iWpDPdKpLjj9jusfgwmThg%3D%3D&trackingId=t15l0y2i9sjM%2BejMOVSOjA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3784891383/?alternateChannel=search&refId=Oe%2FfkNyrHFnMLHyo%2BiLU4Q%3D%3D&trackingId=phRHpjilpRJd5GV5b2ptpQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832459492/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=P6hFsHbmDBYYMHnhu4Y6PQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3798124941/?alternateChannel=search&refId=W2Ij6%2Fty2UFxn1DQviH8ug%3D%3D&trackingId=0x%2FblRJ5m5hVG4SJNYeV0Q%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3815232124/?alternateChannel=search&refId=Bt3XEWCEFS5isGyz27sukQ%3D%3D&trackingId=fgcz2m8VwXcY8oVUNLSSnw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831519366/?alternateChannel=search&refId=l3jqmTDmyHlz3kZn3n7giA%3D%3D&trackingId=3YUYIcyWVSVDy9Sxg58rAA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3742731777/?alternateChannel=search&refId=glSwFukC9bBcogczJg%2FVjA%3D%3D&trackingId=9YTVmJzTeBMhm%2BbGOXyQVw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833446125/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=qKrs9bD9SFycD%2BJprMq5EQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3828508473/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=g5uUb%2BlWriMKkYIyWcsntg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833447054/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=SEwYkokzmk24PdDbwiFYDw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836993295/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=6ZX7cMffl5vmFknAhewm4g%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3827935451/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=%2B95qCh%2Bwao5Z6aEqWR8ayA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3821095211/?alternateChannel=search&refId=q6AFAbBnAPX%2F6mTTv4M9qg%3D%3D&trackingId=iIUQcbZ2MA6Tri%2BHg0zVtQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3807126446/?alternateChannel=search&refId=FIhCBnGx7HZOR88JXte2iA%3D%3D&trackingId=Vtth5RizDBP3qj7wxZ3DoQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836985908/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=%2BJUwoyQCVi6xjhnyrpeOZw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836430754/?alternateChannel=search&refId=MtKo46PdGalKPVbVK6VmVg%3D%3D&trackingId=xwHWVIXD%2FAbIWq50TM7fPQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3811310322/?alternateChannel=search&refId=FIhCBnGx7HZOR88JXte2iA%3D%3D&trackingId=FDJ7gbqhtCjGFhhiqyagdQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833445390/?alternateChannel=search&refId=17DWXWfwEx8HC0B5SKNZCg%3D%3D&trackingId=qynKTnlwIwYJ9Pr7z5X7zA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3823042029/?alternateChannel=search&refId=YHg8B5V0GN6BD6vR4Ef5Hg%3D%3D&trackingId=TYH5DSVbeIrg6jhVH7zSDA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3827949510/?alternateChannel=search&refId=JnqE9SL3C42npDT2WvHpXA%3D%3D&trackingId=NYnOfJ0hCxZQxpKQwWSOaw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831389135/?alternateChannel=search&refId=JnqE9SL3C42npDT2WvHpXA%3D%3D&trackingId=7NVZzbwkmIZkHmeGyfG%2F0w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831525203/?alternateChannel=search&refId=JnqE9SL3C42npDT2WvHpXA%3D%3D&trackingId=z%2BdUnZu6JW0nNH3hdhnjTw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3828510263/?alternateChannel=search&refId=JnqE9SL3C42npDT2WvHpXA%3D%3D&trackingId=vjkyJHU3gCe0Sc8KYjKMog%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831090609/?alternateChannel=search&refId=JnqE9SL3C42npDT2WvHpXA%3D%3D&trackingId=UeHaaS1ynIS9nuc2lfSo8w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3806460658/?refId=4dd9f5ee-2ac2-44c7-a51b-5e838fe62715&trackingId=cgo7m3KYTbiI4rhA1ptcqA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3814476835/?refId=4f1cb55f-f36f-418d-b6b4-f7cfb1abe044&trackingId=eNo2LGg5QLm7C7Io5CjO6g%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3830640488/?alternateChannel=search&refId=IjF%2Be5ZCr%2BFaNDuFkU9BoA%3D%3D&trackingId=gAT%2FNtNHJkGSrYdJFI27GA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3830492099/?alternateChannel=search&refId=jJaA2IfsylWZNm9cgap3OA%3D%3D&trackingId=AuZY%2BEUy7Ro1RxQqWgPm7w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3744789838/?refId=ced7878d-7756-4fde-9475-8ad86f5a4d7b&trackingId=ekRlHWQwQo2VGOLIEqw4wg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3830494081/?alternateChannel=search&refId=jJaA2IfsylWZNm9cgap3OA%3D%3D&trackingId=rqAmWEuFcRI45fq3XlBshQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833443708/?alternateChannel=search&refId=jJaA2IfsylWZNm9cgap3OA%3D%3D&trackingId=bBBeUl%2BJlvVvtGzFMNsZXw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831242514/?alternateChannel=search&refId=NcKKq4VoQ2WhHi%2BN%2BDJW%2BQ%3D%3D&trackingId=%2Fn7VHCRZg3SU1ScptuaOPw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3820786394/?alternateChannel=search&refId=mo%2BOOwBBeIVE8JpFjdYQ7w%3D%3D&trackingId=%2Bb718xpG%2BwU1FRZHKtQLpA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3799970378/?alternateChannel=search&refId=e5ytXuux9Kn6wULYPoMyGQ%3D%3D&trackingId=sVm3f6UN91RqELyLYB9sow%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831525256/?alternateChannel=search&refId=EoGodWzbksWbsAANZ7JZEQ%3D%3D&trackingId=yTpXPqlcxjK6kNa77w38Jg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836955949/?alternateChannel=search&refId=Qe59kHTUM0%2FrwVmsxSTozQ%3D%3D&trackingId=8sU9dDC5wupBlLDE4CCsYw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3811310015/?alternateChannel=search&refId=5wN2ojp0m6J79G0FrU1%2B4g%3D%3D&trackingId=MbSdl9QGcCWHAh3%2FSfbrmA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3814173635/?alternateChannel=search&refId=5wN2ojp0m6J79G0FrU1%2B4g%3D%3D&trackingId=ATbkc77tGSgyG9ecUCW%2BvA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831198780/?alternateChannel=search&refId=eTbUEijvZfahk%2B7PrNhWrA%3D%3D&trackingId=FtueZkxENBDskdXeiEJmEw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835763987/?alternateChannel=search&refId=eTbUEijvZfahk%2B7PrNhWrA%3D%3D&trackingId=YlwAmjDkLNnMWQdZw4%2BIvA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832726226/?alternateChannel=search&refId=nJY%2FMdotXUYkY3rwiAx2Vw%3D%3D&trackingId=Qk1iVjEoPahY9IE3HfhItg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831192258/?alternateChannel=search&refId=aDDLGK0ZqW78uMEVZFTYuA%3D%3D&trackingId=lFSdrp7PDQ%2BI%2FcWlVWuFWw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831206453/?alternateChannel=search&refId=aDDLGK0ZqW78uMEVZFTYuA%3D%3D&trackingId=LsVkrcCxVO5fMWATJEpJTA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836414971/?alternateChannel=search&refId=qUGCojoamuhA6eJLeMt7zQ%3D%3D&trackingId=srDS4mobRlOvmdvxP4bhQg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831539162/?alternateChannel=search&refId=leYy%2FM4d%2BY%2B4zzIjDj5nWw%3D%3D&trackingId=iqLxHi7jhh5s97HQrhRuuw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831553384/?alternateChannel=search&refId=pmUhaJDcn7JH400%2FmUYtKA%3D%3D&trackingId=zvV1h6E7nLovFTR392ESFg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3829152118/?alternateChannel=search&refId=pmUhaJDcn7JH400%2FmUYtKA%3D%3D&trackingId=CXAN5prQ7NhomrJsSNyWow%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836415815/?alternateChannel=search&refId=HV881YI00QA5W%2BfZHRT%2FIA%3D%3D&trackingId=2aaAU7vbyEh20RhagZS9%2BA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3829664308/?alternateChannel=search&refId=HV881YI00QA5W%2BfZHRT%2FIA%3D%3D&trackingId=jTBZAKcBRS%2Byh2di3VCz2A%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836413988/?alternateChannel=search&refId=xeTo82CuWku35j4GcKo5CQ%3D%3D&trackingId=znfBYyagi%2BClsA8vEAySqQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831197210/?alternateChannel=search&refId=zooQiB1RUrblp%2FUAsoIGaA%3D%3D&trackingId=xstt4gBcwAeMbZBNsvDf7w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831619734/?alternateChannel=search&refId=Ao5dyU60UNMeLIu41btRhQ%3D%3D&trackingId=JdSqh59xGvNhSwXJN4HU7Q%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3814194895/?alternateChannel=search&refId=Yfb%2BeGRf8Ar9%2F5vMrw2NEQ%3D%3D&trackingId=JAUAftHIwbMnIzfF9sIJNw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3819104029/?alternateChannel=search&refId=qdT3T6Yk9vK7P%2FTQSBka3A%3D%3D&trackingId=rFl1%2Ft7Q5iYPpgykCdkdPA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3814725030/?alternateChannel=search&refId=qdT3T6Yk9vK7P%2FTQSBka3A%3D%3D&trackingId=ngCVa4doiobGhdK3p5urKg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831332995/?alternateChannel=search&refId=qdT3T6Yk9vK7P%2FTQSBka3A%3D%3D&trackingId=X%2BqeyBpowsq4DXDQFoSM3A%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3829348018/?alternateChannel=search&refId=qdT3T6Yk9vK7P%2FTQSBka3A%3D%3D&trackingId=pJStNpgs4y0Oz3qhMgxEig%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836687379/?alternateChannel=search&refId=44643796-6680-47ff-b077-60306a8efe75&trackingId=1XIWC1hzSC6XT62hdAsFdQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3714715661/?alternateChannel=search&refId=5ZGPGMhUHRks0Sq0AarUrA%3D%3D&trackingId=5SJWmFXpcPu8UjaVVmEoBQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3795274567/?alternateChannel=search&refId=5ZGPGMhUHRks0Sq0AarUrA%3D%3D&trackingId=4oumADBYiX68X0O3tcxuRw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3837186907/?alternateChannel=search&refId=ov7QZzUJ54plcKJn%2F9SvyA%3D%3D&trackingId=wb%2Fa0BWbqpkXcQp7Xt9Uow%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3837171602/?alternateChannel=search&refId=5iCkstZUdhw3HuuONYt1Vw%3D%3D&trackingId=Oq4j%2B1ojgnViTvDRbpK%2BMw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832045841/?alternateChannel=search&refId=3sI2fJxGB8vmNidLZwvxQg%3D%3D&trackingId=2FgpEXyDlaLaMd5gRLzPJg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3837175965/?alternateChannel=search&refId=gEpk5%2BQwe78lnqcwlOXxOA%3D%3D&trackingId=3Z6zBAIEXLuPRfFPctXz5A%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836126335/?alternateChannel=search&refId=IVbACzZTMQx%2F9HSKjwBiCQ%3D%3D&trackingId=qG4yEKhLrx8nkKrnWXuaAg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3833645454/?alternateChannel=search&refId=4Ao4NLtnWbEZq1FZMNc%2BEQ%3D%3D&trackingId=QF%2BEqhl3D%2BmGerX8PyoBqg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3828541111/?alternateChannel=search&refId=5bPA%2BAtS2JDSIVt0rEUGlw%3D%3D&trackingId=9P5I1Wb1%2FDq2YvPEMJwgwg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3769581760/?alternateChannel=search&refId=5bPA%2BAtS2JDSIVt0rEUGlw%3D%3D&trackingId=V19Mffe9yoBzyAnvs42tuA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3804821979/?alternateChannel=search&refId=5bPA%2BAtS2JDSIVt0rEUGlw%3D%3D&trackingId=e0gmuqxy8fN%2F6WeUmmk6kw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835531085/?alternateChannel=search&refId=IVbACzZTMQx%2F9HSKjwBiCQ%3D%3D&trackingId=g03J3d4J9x8NUMU7nZA3Gg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836309279/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=kqFMUKi9S7WPzqFEw7o1lA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832202511/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=iLZkyMQ2ntETAr19ZcoS8Q%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836117090/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=Kpj%2BbbGLs7yeWJu8X2aZJQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832121600/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=8livLcHuELBsgYx3jYM8HQ%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835576198/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=u3kgHp%2FVs9nYgGDi0mXn6Q%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3817707205/?alternateChannel=search&refId=tdrs11xen9nQyjG9XvarZA%3D%3D&trackingId=ax%2BTbxUUcnzk7S%2BsZto9fA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3825596122/?alternateChannel=search&refId=tdrs11xen9nQyjG9XvarZA%3D%3D&trackingId=JWBNhUgrcH3siN4kQPHAjA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3815633979/?alternateChannel=search&refId=tdrs11xen9nQyjG9XvarZA%3D%3D&trackingId=AfcDwnJ6W5tXPpnC2xVjkg%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831991118/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=c%2FDNuJVJ2HalJ7k9NBA2yw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3836966129/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=gpD1jIykqVFqpYdv02zg%2BA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3831282134/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=xfAcbzq8FmoGZNCbgWQQ6w%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835714906/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=WrXS7A3arRoNOQeJgbwlhA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3832073162/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=KAkN%2FeuaneO1Sm6nIKgynw%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3835714032/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=r8XwKcFrz%2BIzoL%2FM3FMbvA%3D%3D\n",
            "https://www.linkedin.com/jobs/view/3828592919/?alternateChannel=search&refId=ouN3LKeaQN8abWEHkJAkCg%3D%3D&trackingId=7E%2FaIvm6lgYQbijApGe7Og%3D%3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent! So the text files were saved in Google Drive and they look good! They are here: https://drive.google.com/drive/folders/1xJxoBnqQKhKg45K_k_4fijBcYnRPjqX9"
      ],
      "metadata": {
        "id": "VmqG6XCPtKvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update: as far as the prompt we send to ChatGPT, here is what I've found to be the best prompt: (note that I have engineered it to have ChatGPT return a json object that contains the extracted feature data for the given job posting)\n",
        "\n",
        "\"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put \"N/A\". Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Here is the text from the job posting:\"\n",
        "\n",
        "And the output actually looks pretty good! https://drive.google.com/drive/folders/1IYOZtvYNB9D9ZxDBlU9ux-qrqeEZ3Mc1\n",
        "\n",
        "We should discuss what post-processing steps and further feature engineering we will need to perform on this json data. However, we should first test that we can get json data that will be straightforward to work with using OpenAI API calls. We will go to the other notebook where we test OpenAI API calls: https://colab.research.google.com/drive/1egzUeDTVPsBLbkhcrF1Il1OR_JwyyApu\n",
        "\n"
      ],
      "metadata": {
        "id": "rqHaD88mFm2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I may as well copy the code from api-test (which I used to prove that the process of creating a dictionary object from the message returned by ChatGPT using a JSON representation is straightforward) here."
      ],
      "metadata": {
        "id": "0PLIr0uwORT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "xwfbaBZTOQ8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44994e89-c59e-4d73-9d77-80a194253f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Here is the text from the job posting: \"\n",
        "\n",
        "with open('row106.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "3TIO2YjsPV3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)\n",
        "print(\"/n\")\n",
        "print(chat_completion.choices[0].message.content)\n",
        "print(chat_completion.choices[0].message.content.strip('json\\n'))"
      ],
      "metadata": {
        "id": "gXhPZIbaPXYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fcf2b2-4303-4512-f229-f9d180d11ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Here is the text from the job posting: \n",
            "position is Algorithm Developer\n",
            "company is Nurp\n",
            "location is United States\n",
            "salary is $100,000.00/yr - $130,000.00/yr\n",
            "\n",
            "Seniority level is Mid-Senior level\n",
            "Employment type is Full-time\n",
            "Job function is Engineering, Finance, and Information Technology\n",
            "Industries is Financial Services, Investment Banking, and Software Development\n",
            "\n",
            "Algorithmic Trading DeveloperPOSITION SUMMARYAre you passionate about the fusion of finance and technology? Nurp is actively seeking an Algorithmic Trading Developer to join our team. As a key member, you will play a pivotal role in constructing and implementing cutting-edge trading algorithms, contributing to the development and coding of sophisticated trading strategies. If you have a passion for algorithmic trading, a knack for innovation, and an experienced software developer in the trading space, we want to hear from you.ABOUT OUR COMPANYNurp pioneers the convergence of modern and future investing through emerging technologies. Our advanced algorithmic trading programs and comprehensive forex trading systems challenge traditional investment models, propelling algorithmic investing for unparalleled success.PERFORMANCE OBJECTIVESThe primary responsibility of the Algorithmic Developer will be to translate trading strategies provided by strategists and other stakeholders into functional algorithms. Design, code build and test the algorithmic strategies provided into functioning programs for MT 4 and MT 5 platforms.Write clean, concise and efficient code.Troubleshoot and debug program.Analyze complex problems and develop innovative solutions to optimize the algorithms code and address potential issues.Work closely, meeting with and collaborating with traders, quantitative strategists, involved contractors and other team members to accomplish.Analyze user feedback and make adjustments as needed or directed.Recommending and executing program improvements.KEY COMPETENCIESFinancial Acumen: Understanding of financial markets, including equities, derivatives, and forex, would be a plus. Algorithmic Trading Expertise: Strong understanding of algorithmic trading concepts and experience in developing trading algorithms.Programming Skills: Proficiency in programming languages such as Python, C++, or Java, and familiarity with trading platforms and APIs, including MetaTrader 4 and 5.Technical Proficiency: Proficiency in trading platforms, APIs, and data analysis tools, with experience in platforms like MT 4 and 5.Adaptability: Ability to quickly learn new technologies and methodologies to stay ahead of market trends and adapt to changing market conditions.Team Collaboration: Strong interpersonal skills and ability to collaborate effectively with cross-functional teams.Problem Solving: Strong analytical, Initiative, and problem-solving skills.EDUCATION AND EXPERIENCEBachelor's or Master's degree in Computer Science, Finance, Mathematics, or a related field. (Preferred) 3 years of experience developing and coding trading algorithms in the financial industry.(Required)Proficiency in programming languages such as Python and C++.Experience with MT 4 and 5.(Required)Familiarity with trading platforms, APIs, and quantitative analysis tools. (Required)BENEFITS100% Remote work.Health insurance.Dental insurance.Vision Insurance.Voluntary Life Insurance.Paid Time Off.Opportunities for professional development and training.COMMITMENT TO DIVERSITYAs an equal opportunity employer committed to meeting the needs of a multigenerational and multicultural workforce, Nurp LLC recognizes that a diverse staff, reflective of our community, is an integral and welcome part of a successful and ethical business. We hire talent at all levels regardless of race, color, religion, age, national origin, gender, gender identity, sexual orientation, or disability and actively foster inclusion in all forms both within our company and across interactions with clients, candidates, and partners.\n",
            "/n\n",
            "```json\n",
            "{\n",
            "    \"Employment Type\": [\"Full-time\"],\n",
            "    \"Job Function\": [\"Engineering\", \"Finance\", \"Information Technology\"],\n",
            "    \"Description of Product/Service\": [\"Algorithmic trading programs and forex trading systems\", \"Cutting-edge trading algorithms\"],\n",
            "    \"Industries\": [\"Financial Services\", \"Investment Banking\", \"Software Development\"],\n",
            "    \"Position Name\": [\"Algorithm Developer\"],\n",
            "    \"Broader Role Name\": [\"N/A\"],\n",
            "    \"Company\": [\"Nurp\"],\n",
            "    \"Location\": [\"United States\"],\n",
            "    \"Salary/Compensation Range\": [\"$100,000.00/yr - $130,000.00/yr\"],\n",
            "    \"Responsibilities\": [\"Translate trading strategies into functional algorithms\", \"Design, code, test algorithmic strategies\", \"Analyze problems and develop solutions\", \"Collaborate with team and stakeholders\"],\n",
            "    \"Goals/Objectives\": [\"Develop and optimize trading algorithms\", \"Maintain clean and efficient code\"],\n",
            "    \"Name of Department or Team\": [\"N/A\"],\n",
            "    \"Required Qualifications\": [\"Bachelor's or Master's degree in CS, Finance, Math\", \"3 years of experience developing trading algorithms\", \"Proficiency in Python, C++\", \"Experience with MT 4 and 5\", \"Familiarity with trading platforms and APIs\"],\n",
            "    \"Preferred Qualifications\": [\"N/A\"],\n",
            "    \"Benefits\": [\"Remote work\", \"Health/Dental/Vision Insurance\", \"Life Insurance\", \"Paid Time Off\", \"Professional Development Opportunities\"],\n",
            "    \"Work Arrangement\": [\"Remote\"]\n",
            "}\n",
            "```\n",
            "```json\n",
            "{\n",
            "    \"Employment Type\": [\"Full-time\"],\n",
            "    \"Job Function\": [\"Engineering\", \"Finance\", \"Information Technology\"],\n",
            "    \"Description of Product/Service\": [\"Algorithmic trading programs and forex trading systems\", \"Cutting-edge trading algorithms\"],\n",
            "    \"Industries\": [\"Financial Services\", \"Investment Banking\", \"Software Development\"],\n",
            "    \"Position Name\": [\"Algorithm Developer\"],\n",
            "    \"Broader Role Name\": [\"N/A\"],\n",
            "    \"Company\": [\"Nurp\"],\n",
            "    \"Location\": [\"United States\"],\n",
            "    \"Salary/Compensation Range\": [\"$100,000.00/yr - $130,000.00/yr\"],\n",
            "    \"Responsibilities\": [\"Translate trading strategies into functional algorithms\", \"Design, code, test algorithmic strategies\", \"Analyze problems and develop solutions\", \"Collaborate with team and stakeholders\"],\n",
            "    \"Goals/Objectives\": [\"Develop and optimize trading algorithms\", \"Maintain clean and efficient code\"],\n",
            "    \"Name of Department or Team\": [\"N/A\"],\n",
            "    \"Required Qualifications\": [\"Bachelor's or Master's degree in CS, Finance, Math\", \"3 years of experience developing trading algorithms\", \"Proficiency in Python, C++\", \"Experience with MT 4 and 5\", \"Familiarity with trading platforms and APIs\"],\n",
            "    \"Preferred Qualifications\": [\"N/A\"],\n",
            "    \"Benefits\": [\"Remote work\", \"Health/Dental/Vision Insurance\", \"Life Insurance\", \"Paid Time Off\", \"Professional Development Opportunities\"],\n",
            "    \"Work Arrangement\": [\"Remote\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(chat_completion.choices[0].message.content)\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "id": "VU-5PjoQPbbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'Employment type': ['Full-time'], 'Job function': ['Engineering and Information Technology'], 'Description of product/service': ['Kernel development for Boeing products'], 'Industries': ['Airlines and Aviation', 'Aviation and Aerospace Component Manufacturing', 'Defense and Space Manufacturing'], 'Position name': ['Associate Software Engineer - Kernel Developer'], 'Broader role name': ['Software Engineer'], 'Company': ['Boeing'], 'Location': ['Mesa, AZ'], 'Salary/compensation range': ['$79,050.00 - $123,050.00'], 'Responsibilities': ['Develop, document, maintain architectures, algorithms', 'Subject matter expert for kernel internals', 'Create and organize project initiatives', 'Participate in daily scrums'], 'Goals/objectives': ['Contribute to safety-critical kernel development'], 'Name of department or team': ['Kernel development team'], 'Required qualifications': ['Bachelor, Master, or Doctorate in relevant field', '1+ years of Linux OS experience', '1+ years of C++, C, C#, or Python experience'], 'Preferred qualifications': ['2+ years of C++, C, or C# experience', 'Passion for open source projects', 'Real-time software development experience', 'Experience with modern processor architectures'], 'Benefits': ['Competitive base pay', 'Variable compensation opportunities', 'Health insurance, retirement plans, life/disability insurance'], 'Work arrangement': ['Virtual'], 'N/A': ['Seniority level', 'Position description', 'Travel requirements', 'Relocation assistance', 'Drug policy', 'Shift schedule', 'Application deadline', 'Export control details', 'Equal Opportunity Policy']}"
      ],
      "metadata": {
        "id": "s5eX2BqaRqH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the output and explicitly spell out the further post-processing and feature engineering steps we will need to take. Modifying the prompt is another option that can help reduce the feature engineering work needed downstream. For example, we might want to have ChatGPT extract the name of the state for the locale if applicable instead of us having to extract the state programatically. But that has not been decided. I don't think it would be too challenging for us to do without ChatGPT.\n",
        "\n",
        "So without further ado, let's discuss what processing steps if any are needed to get our data into a more desirable format. We are not concerning ourselves yet with finding encodings or numerical values for our various features. At this stage we look at feature engineering work that will involve specialty logic to parse strings in certain fields, such as the 'Location' field as mentioned above.  \n",
        "\n",
        "Honestly, there isn't that much of that to do though. Some ideas include:\n",
        "\n",
        "- make columns for minimum and maximum of pay range and remove 'salary/compensation range' column from final dataset\n",
        "- extract state and possibly country into a new column of the dataset\n",
        "- we may try to get clever with our prompt engineering to handle this, but the following may actually be straightforward enough to do programmatically:\n",
        "  - extract specific experience qualifications into other lists that go into columns designated '1+ years required, 1+ years desired, 2+ years required, 2+ years desired, 3+ years required, 3+ years desired that will list\n",
        "- we might need logic to handle data that would correspond to new columns, or to fill out columns using data from other columns, like for those fields in the list for 'N/A', we'd need some code that runs that fills out the columns corresponding to those field names with 'N/A'. We'll probably just have a list of supported column names, which should be named as closely as possible to the field name in the JSON ChatGPT spits out. What we end up doing though depends largely on the repeatability of ChatGPT's response structure, format, and overall performance with adhering to the prompt guidelines.  "
      ],
      "metadata": {
        "id": "UTGI5aVMRspO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to see what response we get with a different prompt."
      ],
      "metadata": {
        "id": "F69hk4icnXcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Here is the text from the job posting: \\n\"\n",
        "\n",
        "with open('row105.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt  + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "yroa2J8inWSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(prompt)\n",
        "#print(\"\\n\")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbx4EEwpn-3R",
        "outputId": "223ccc4a-e11b-4ce6-abe1-afbc1b4deb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"Employment type\": [\"Full-time\"],\n",
            "\"Job function\": [\"Engineering\", \"Finance\", \"Information Technology\"],\n",
            "\"description of product/service\": [\"Algorithmic Trading Developer\", \"Forex trading systems\"],\n",
            "\"Industries\": [\"Financial Services\", \"Investment Banking\", \"Software Development\"],\n",
            "\"Position name\": [\"Algorithm Developer\"],\n",
            "\"Broader role name\": [\"N/A\"],\n",
            "\"Company\": [\"Nurp\"],\n",
            "\"Location\": [\"United States\"],\n",
            "\"Salary/compensation range\": [\"$100,000.00/yr - $130,000.00/yr\"],\n",
            "\"Responsibilities\": [\"Translate trading strategies into algorithms\", \"Design, code, build, and test algorithms\", \"Troubleshoot and debug programs\", \"Collaborate with team members\"],\n",
            "\"Goals/objectives\": [\"Develop efficient trading algorithms\", \"Address potential issues\", \"Make program improvements\"],\n",
            "\"Name of department or team\": [\"N/A\"],\n",
            "\"Required qualifications\": [\"Bachelor's or Master's degree in relevant field\", \"3 years of experience in financial industry\", \"Proficiency in Python and C++\", \"Experience with MT 4 and 5 platforms\", \"Familiarity with trading platforms and APIs\"],\n",
            "\"Preferred qualifications\": [\"N/A\"],\n",
            "\"Benefits\": [\"100% Remote work\", \"Health, dental, vision insurance\", \"Paid Time Off\", \"Professional development opportunities\"],\n",
            "\"Work arrangement\": [\"Remote\"],\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(f\"{chat_completion.choices[0].message.content}\")\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "D_Fm9QuQoEHq",
        "outputId": "b7e06268-1715-4073-aa1e-c6ea4dcc2c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting property name enclosed in double quotes: line 18 column 1 (char 1259)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6b2700755697>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{chat_completion.choices[0].message.content}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now, 'data' contains the dictionary representation of the JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 18 column 1 (char 1259)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly not the result we wanted; we definitely need to be explicit in our prompt about how we want only the JSON and nothing else."
      ],
      "metadata": {
        "id": "iJiyadLPoO26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "with open('row105.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "UifbVLvcoP6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(chat_completion.choices[0].message.content)\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "4n1EL3A6pRlF",
        "outputId": "138e9885-871b-4697-a056-b3ce3bb50472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting property name enclosed in double quotes: line 18 column 1 (char 1259)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a138472f696d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now, 'data' contains the dictionary representation of the JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 18 column 1 (char 1259)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, this isn't exactly going as hoped on test #2. Let's see if it works with a different .txt file."
      ],
      "metadata": {
        "id": "QcFw7OKWrqHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "with open('row99.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "pPqlddjHpWub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(prompt)\n",
        "#print(\"\\n\")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gITzRNL_r9hO",
        "outputId": "e7e870b6-2117-43b6-dd7e-a77467fd0c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Employment type\": [\"Internship\"],\n",
            "  \"Job function\": [\"Research\", \"Analyst\", \"Information Technology\"],\n",
            "  \"Description of product/service\": [\"Quantum computing system\", \"Quantum services\"],\n",
            "  \"Industries\": [\"Software Development\"],\n",
            "  \"Position name\": [\"Research Intern - Quantum Information and Computation\"],\n",
            "  \"Broader role name\": [\"Research Intern\"],\n",
            "  \"Company\": [\"Microsoft\"],\n",
            "  \"Location\": [\"Redmond, WA\"],\n",
            "  \"Salary/compensation range\": [\"$10,120.00/yr - $12,170.00/yr\"],\n",
            "  \"Responsibilities\": [\"Problem identification\", \"Formulation\", \"Publishing results\", \"Collaboration\"],\n",
            "  \"Goals/objectives\": [\"Contribute to research strides\"],\n",
            "  \"Name of department or team\": [\"N/A\"],\n",
            "  \"Required qualifications\": [\"PhD\",\"Physics\", \"Mathematics\", \"Computer Science\", \"Electrical and Computer Engineering\"],\n",
            "  \"Preferred qualifications\": [\"Quantum algorithms\", \"Machine learning\", \"Physics device modeling\", \"Effective communication\", \"Problem-solving skills\"],\n",
            "  \"Benefits\": [\"N/A\"],\n",
            "  \"Work arrangement\": [\"On-site\"],\n",
            "  \"Hybrid, on-site, remote?\": [\"Hybrid\"],\n",
            "  \"N/A\": [\"N/A\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(chat_completion.choices[0].message.content)\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4th0YUGsAbw",
        "outputId": "ea247697-90a1-4279-c48e-9f80096e74ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Employment type': ['Internship'], 'Job function': ['Research', 'Analyst', 'Information Technology'], 'Description of product/service': ['Quantum computing system', 'Quantum services'], 'Industries': ['Software Development'], 'Position name': ['Research Intern - Quantum Information and Computation'], 'Broader role name': ['Research Intern'], 'Company': ['Microsoft'], 'Location': ['Redmond, WA'], 'Salary/compensation range': ['$10,120.00/yr - $12,170.00/yr'], 'Responsibilities': ['Problem identification', 'Formulation', 'Publishing results', 'Collaboration'], 'Goals/objectives': ['Contribute to research strides'], 'Name of department or team': ['N/A'], 'Required qualifications': ['PhD', 'Physics', 'Mathematics', 'Computer Science', 'Electrical and Computer Engineering'], 'Preferred qualifications': ['Quantum algorithms', 'Machine learning', 'Physics device modeling', 'Effective communication', 'Problem-solving skills'], 'Benefits': ['N/A'], 'Work arrangement': ['On-site'], 'Hybrid, on-site, remote?': ['Hybrid'], 'N/A': ['N/A']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the good news is this worked on test text files 1 and 3, but not #2. Let's try more of them to estimate what our failure rate would be and see if it's worth living with/working around."
      ],
      "metadata": {
        "id": "3f3g0F8bsEFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "with open('row95.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")"
      ],
      "metadata": {
        "id": "igQeErdXsDWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(prompt)\n",
        "#print(\"\\n\")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1FRp7xso9s",
        "outputId": "4265a2bf-053c-4710-e8cf-26266661706b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Employment Type\": [\"Full-time\"],\n",
            "  \"Job Function\": [\"Engineering\", \"Finance\", \"Information Technology\"],\n",
            "  \"Product/Service Description\": [\"Algorithmic Trading Developer\"],\n",
            "  \"Industries\": [\"Financial Services\", \"Investment Banking\", \"Software Development\"],\n",
            "  \"Position Name\": [\"Algorithm Developer\"],\n",
            "  \"Broader Role Name\": [\"N/A\"],\n",
            "  \"Company\": [\"Nurp\"],\n",
            "  \"Location\": [\"United States\"],\n",
            "  \"Salary/Compensation Range\": [\"$100,000.00/yr\", \"$130,000.00/yr\"],\n",
            "  \"Responsibilities\": [\"Translate trading strategies into algorithms\", \"Design, code, build, and test algorithms\", \"Troubleshoot and debug programs\", \"Analyze problems and develop innovative solutions\", \"Meet and collaborate with team members\", \"Analyze user feedback and make adjustments\", \"Recommend and execute program improvements\"],\n",
            "  \"Goals/Objectives\": [\"Implement trading algorithms\", \"Optimize algorithms code\", \"Address potential issues\", \"Stay ahead of market trends\"],\n",
            "  \"Department/Team Name\": [\"N/A\"],\n",
            "  \"Required Qualifications\": [\"Bachelor's or Master's degree in relevant field\", \"3 years of experience in coding trading algorithms\", \"Proficiency in Python and C++\", \"Experience with MT 4 and 5\", \"Familiarity with trading platforms and APIs\"],\n",
            "  \"Preferred Qualifications\": [\"N/A\"],\n",
            "  \"Benefits\": [\"100% Remote work\", \"Health, dental, and vision insurance\", \"Life insurance\", \"Paid Time Off\", \"Professional development opportunities\"],\n",
            "  \"Work Arrangement\": [\"Remote\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(chat_completion.choices[0].message.content.strip())\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ioaaozstt5",
        "outputId": "f2b36b06-7b30-4bfc-d6d0-2d013361595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Employment Type': ['Full-time'], 'Job Function': ['Engineering', 'Finance', 'Information Technology'], 'Product/Service Description': ['Algorithmic Trading Developer'], 'Industries': ['Financial Services', 'Investment Banking', 'Software Development'], 'Position Name': ['Algorithm Developer'], 'Broader Role Name': ['N/A'], 'Company': ['Nurp'], 'Location': ['United States'], 'Salary/Compensation Range': ['$100,000.00/yr', '$130,000.00/yr'], 'Responsibilities': ['Translate trading strategies into algorithms', 'Design, code, build, and test algorithms', 'Troubleshoot and debug programs', 'Analyze problems and develop innovative solutions', 'Meet and collaborate with team members', 'Analyze user feedback and make adjustments', 'Recommend and execute program improvements'], 'Goals/Objectives': ['Implement trading algorithms', 'Optimize algorithms code', 'Address potential issues', 'Stay ahead of market trends'], 'Department/Team Name': ['N/A'], 'Required Qualifications': [\"Bachelor's or Master's degree in relevant field\", '3 years of experience in coding trading algorithms', 'Proficiency in Python and C++', 'Experience with MT 4 and 5', 'Familiarity with trading platforms and APIs'], 'Preferred Qualifications': ['N/A'], 'Benefits': ['100% Remote work', 'Health, dental, and vision insurance', 'Life insurance', 'Paid Time Off', 'Professional development opportunities'], 'Work Arrangement': ['Remote']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_columns = [\"emploment type\", \"job function\", \"description of product/service\", \"industries\",\n",
        "              \"position name\", \"broader role name\", \"company\", \"location\", \"salary/compensation range\",\n",
        "              \"responsibilities\", \"goals/objectives\", \"name of department/team\", \"required qualifications\",\n",
        "              \"preferred qualifications\", \"benefits\", \"work arrangement\"]\n",
        "df = pd.DataFrame.from_dict([data])\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-SR74VaRZ6e",
        "outputId": "1f976458-12c6-454c-e3dc-691c271009d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent, we're at 3/4. Let's try maybe 10. Instead of making new cells we will just rerun the last 3 and report back on our results.\n",
        "\n",
        "Try 5 - fail\n",
        "\n",
        "Try 6 - pass\n",
        "\n",
        "Try 7 - pass\n",
        "\n",
        "Try 8 - pass\n",
        "\n",
        "Try 9 - pass\n",
        "\n",
        "Try 10 - pass\n",
        "\n",
        "So we have 2 failures out of 10. We guess that the success rate of what we're doing is 80%. But with the two failures, we see the same root cause: the response returned by ChatGPT has the word 'json' in the text. So maybe it is straightforward to remove this label and just get the json in code so we can convert those two failures to passes. We'll go back to one of the failing situations and see if we can process the string correctly.\n"
      ],
      "metadata": {
        "id": "DDRiIM_es2m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: Employment type, Job function, description of product/service, Industries, position name, broader role name, company, location, salary/compensation range, Responsibilities, Goals/objectives, name of department or team, required qualifications, preferred qualifications, Benefits, Work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "with open('row66.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")"
      ],
      "metadata": {
        "id": "aSIFRgqqujuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, after trying this again with one of the failing text files ChatGPT's behavior no longer deviates from what we want. Let's try out the other failing text file again."
      ],
      "metadata": {
        "id": "bamc1M_buzzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "with open('row101.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")"
      ],
      "metadata": {
        "id": "7TOHCfNHvIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other failing one, 'row105.txt', still fails after trying again. So we can say with some level of confidence that our failure rate is around 20%. Maybe white space is the issue for the still failing case? The answer is no, .strip() method didn't fix the issue. We might have to just live with this sub-optimal performance to continue moving forward preparing our baseline dataset."
      ],
      "metadata": {
        "id": "NrXViAVRvgla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will modify our prompt so we can try to get Chat to make the field names all be lower-cased.\n",
        "\n",
        "\"Take the following text from a job posting and extract all info related to the following fields: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \""
      ],
      "metadata": {
        "id": "6KZl5DbkSmro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This didn't influence Chat's behavior or if it did it made it output upper-case field names! Thus we will need to post process the JSON dictionary keys and make sure they match with our dataframe column names."
      ],
      "metadata": {
        "id": "2qcmzPsFTmLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_columns = [\"emploment type\", \"job function\", \"description of product/service\", \"industries\",\n",
        "              \"position name\", \"broader role name\", \"company\", \"location\", \"salary/compensation range\",\n",
        "              \"responsibilities\", \"goals/objectives\", \"name of department/team\", \"required qualifications\",\n",
        "              \"preferred qualifications\", \"benefits\", \"work arrangement\"]\n",
        "# Initialize DataFrame with column names\n",
        "df = pd.DataFrame(columns=df_columns)\n",
        "\n",
        "df.head()\n",
        "\n",
        "try:\n",
        "  row_num = df.iloc[-1]\n",
        "except IndexError:\n",
        "  row_num = 0\n",
        "\n",
        "# Iterate over each key-value pair in the JSON dictionary\n",
        "for key, value in data.items():\n",
        "    # Check if the key exists as a column name (ignoring case)\n",
        "    column_name = key.lower()\n",
        "    if column_name in df.columns:\n",
        "        # Add the value to the corresponding column and row\n",
        "        df.at[row_num, column_name] = value\n",
        "\n",
        "\n",
        "#df = pd.DataFrame.from_dict([data])\n",
        "#print(df.shape)"
      ],
      "metadata": {
        "id": "uwKp-rx-UjjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNhzDntkVZu0",
        "outputId": "f6777c43-e852-4cea-c41a-40c841b1f361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "with open('row101.txt', 'r') as file:\n",
        "  # Read the contents of the file into a string\n",
        "  file_contents = file.read()\n",
        "\n",
        "prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")"
      ],
      "metadata": {
        "id": "NobklA54VgTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "data = json.loads(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "\n",
        "# Now, 'data' contains the dictionary representation of the JSON data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0GW9cUDVoNi",
        "outputId": "06f85bf1-a3a5-4bfa-bbdb-7a898c558610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"employment_type\": [\"Full-time\"],\n",
            "  \"job_function\": [\"Design\", \"Consulting\", \"Engineering\"],\n",
            "  \"description_of_product/service\": [\"Optical Design Software\"],\n",
            "  \"industries\": [\"Semiconductor Manufacturing\", \"Software Development\", \"Computer Hardware Manufacturing\"],\n",
            "  \"position_name\": [\"Software Architect - C++\"],\n",
            "  \"broader_role_name\": [\"Software Architect\"],\n",
            "  \"company\": [\"Synopsys Inc\"],\n",
            "  \"location\": [\"Irvine, CA\"],\n",
            "  \"salary_compensation_range\": [\"N/A\"],\n",
            "  \"responsibilities\": [\"Guide development process\", \"Assess legacy systems\", \"Design scalable components\", \"Lead modernization initiatives\", \"Collaborate with teams\", \"Provide technical guidance\", \"Oversee application of best practices\", \"Create architecture documentation\", \"Stay updated with industry trends\", \"Drive performance tuning\", \"Develop algorithms for parallel computation\"],\n",
            "  \"goals_objectives\": [\"Deliver cutting-edge optical design solutions\"],\n",
            "  \"name_of_department_team\": [\"OSG\"],\n",
            "  \"required_qualifications\": [\"Bachelor's degree in CS or related field\", \"6+ years experience as Software Architect\", \"C++ development background\", \"Understanding of software design patterns\", \"Excellent problem-solving skills\", \"Excellent communication skills\"],\n",
            "  \"preferred_qualifications\": [\"Experience in Physics, Optics, Mathematics\", \"Hands-on experience with parallel computation\", \"Experience in CAD/CAM and ray tracing\", \"Knowledge of 3D Geometry and Optimization Algorithms\"],\n",
            "  \"benefits\": [\"Competitive salary\", \"Bonus package\", \"Stock purchase plan\", \"401k plan\", \"Insurance packages\", \"Professional development support\"],\n",
            "  \"work_arrangement\": [\"Hybrid\", \"Remote\"],\n",
            "  \"N/A\": [\"position is Software Architect - C++ - 46456BR\", \"Seniority level is Mid-Senior level\"]\n",
            "}\n",
            "{'employment_type': ['Full-time'], 'job_function': ['Design', 'Consulting', 'Engineering'], 'description_of_product/service': ['Optical Design Software'], 'industries': ['Semiconductor Manufacturing', 'Software Development', 'Computer Hardware Manufacturing'], 'position_name': ['Software Architect - C++'], 'broader_role_name': ['Software Architect'], 'company': ['Synopsys Inc'], 'location': ['Irvine, CA'], 'salary_compensation_range': ['N/A'], 'responsibilities': ['Guide development process', 'Assess legacy systems', 'Design scalable components', 'Lead modernization initiatives', 'Collaborate with teams', 'Provide technical guidance', 'Oversee application of best practices', 'Create architecture documentation', 'Stay updated with industry trends', 'Drive performance tuning', 'Develop algorithms for parallel computation'], 'goals_objectives': ['Deliver cutting-edge optical design solutions'], 'name_of_department_team': ['OSG'], 'required_qualifications': [\"Bachelor's degree in CS or related field\", '6+ years experience as Software Architect', 'C++ development background', 'Understanding of software design patterns', 'Excellent problem-solving skills', 'Excellent communication skills'], 'preferred_qualifications': ['Experience in Physics, Optics, Mathematics', 'Hands-on experience with parallel computation', 'Experience in CAD/CAM and ray tracing', 'Knowledge of 3D Geometry and Optimization Algorithms'], 'benefits': ['Competitive salary', 'Bonus package', 'Stock purchase plan', '401k plan', 'Insurance packages', 'Professional development support'], 'work_arrangement': ['Hybrid', 'Remote'], 'N/A': ['position is Software Architect - C++ - 46456BR', 'Seniority level is Mid-Senior level']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have the seen ChatGPT putting '_' between words we will need to tweak our json item iterator code."
      ],
      "metadata": {
        "id": "VX9vBra8ZgfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_iter = 0"
      ],
      "metadata": {
        "id": "vTBsVjFwdncQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  row_num = df.iloc[-1]\n",
        "  row_num = df.shape[0] + 1\n",
        "except IndexError:\n",
        "  row_num = 0\n",
        "\n",
        "# Iterate over each key-value pair in the JSON dictionary\n",
        "for key, value in data.items():\n",
        "    # Check if the key exists as a column name (ignoring case)\n",
        "    column_name = key.replace(\"_\", \" \").lower()\n",
        "    if column_name in df.columns:\n",
        "        # Add the value to the corresponding column and row\n",
        "        df.at[row_num, column_name] = value\n",
        "\n",
        "\n",
        "#df = pd.DataFrame.from_dict([data])\n",
        "print(df.shape)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScrkguQCZdoX",
        "outputId": "45a12795-1c57-4ecf-c7bf-8267b45ce54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 16)\n",
            "  employment_type                       job_function  \\\n",
            "0     [Full-time]  [Design, Consulting, Engineering]   \n",
            "2             NaN                                NaN   \n",
            "\n",
            "  description_of_product/service  \\\n",
            "0      [Optical Design Software]   \n",
            "2                            NaN   \n",
            "\n",
            "                                          industries  \\\n",
            "0  [Semiconductor Manufacturing, Software Develop...   \n",
            "2  [Semiconductor Manufacturing, Software Develop...   \n",
            "\n",
            "                position_name     broader_role_name         company  \\\n",
            "0  [Software Architect - C++]  [Software Architect]  [Synopsys Inc]   \n",
            "2                         NaN                   NaN  [Synopsys Inc]   \n",
            "\n",
            "       location salary_compensation_range  \\\n",
            "0  [Irvine, CA]                     [N/A]   \n",
            "2  [Irvine, CA]                       NaN   \n",
            "\n",
            "                                    responsibilities  \\\n",
            "0  [Guide development process, Analyze existing l...   \n",
            "2  [Guide development process, Assess legacy syst...   \n",
            "\n",
            "                                    goals_objectives name_of_department/team  \\\n",
            "0  [Deliver cutting-edge optical design solutions...          [Synopsys OSG]   \n",
            "2                                                NaN                     NaN   \n",
            "\n",
            "                             required_qualifications  \\\n",
            "0  [Bachelor's degree in CS or related field, 6+ ...   \n",
            "2                                                NaN   \n",
            "\n",
            "                            preferred_qualifications  \\\n",
            "0  [Experience in Physics, Optics, Mathematics, H...   \n",
            "2                                                NaN   \n",
            "\n",
            "                                            benefits work_arrangement  \n",
            "0  [Ability to work remotely/from multiple locati...         [Hybrid]  \n",
            "2  [Competitive salary, Bonus package, Stock purc...              NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['company'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZILEISL7bujt",
        "outputId": "e657a4bf-f8e1-47c6-b472-f56361743137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [Harnham]\n",
            "1    [Harnham]\n",
            "Name: company, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global row_iter\n",
        "row_iter = 0"
      ],
      "metadata": {
        "id": "OCMByhU3eUu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_columns = [\"emploment type\", \"job function\", \"description of product/service\", \"industries\",\n",
        "              \"position name\", \"broader role name\", \"company\", \"location\", \"salary/compensation range\",\n",
        "              \"responsibilities\", \"goals/objectives\", \"name of department/team\", \"required qualifications\",\n",
        "              \"preferred qualifications\", \"benefits\", \"work arrangement\"]\n",
        "# Initialize DataFrame with column names\n",
        "df = pd.DataFrame(columns=df_columns)\n",
        "\n",
        "print(df.last_valid_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB4GnR3recVF",
        "outputId": "6a173642-3335-4866-fe2d-6f2be2ae8e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def generate_json_summary(text_file_path):\n",
        "  with open(text_file_path, 'r') as file:\n",
        "    # Read the contents of the file into a string\n",
        "    file_contents = file.read()\n",
        "\n",
        "  prompt = \"Take the following text from a job posting and extract all info related to the following fields: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "  prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        "  )\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "  )\n",
        "  json_data = data = json.loads(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "  return json_data"
      ],
      "metadata": {
        "id": "vQ7CHeLGdzBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "prompt = \"Take the following text from a job posting and extract all info related to the following fields: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \""
      ],
      "metadata": {
        "id": "5oloyy8Ff0Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_row_to_df(json_data):\n",
        "  # Iterate over each key-value pair in the JSON dictionary\n",
        "  row_num = df.last_valid_index()\n",
        "  if row_num == None:\n",
        "    row_num = 0\n",
        "  else:\n",
        "    row_num = row_num + 1\n",
        "  for key, value in json_data.items():\n",
        "      # Check if the key exists as a column name (ignoring case)\n",
        "      column_name = key.replace(\"_\", \" \").lower()\n",
        "      if column_name in df.columns:\n",
        "          # Add the value to the corresponding column and row\n",
        "          df.at[row_num, column_name] = value"
      ],
      "metadata": {
        "id": "0Z-s-LgieQpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuVjv1vKgPtY",
        "outputId": "4d240019-8965-470a-85ba-a50190e720b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files_list = os.listdir()\n",
        "\n",
        "for i in files_list:\n",
        "  if 'row' in i:\n",
        "    json_data = generate_json_summary(i)\n",
        "    add_row_to_df(json_data)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plLFN5JDe5WD",
        "outputId": "c8adbabb-89c0-414b-edde-fb3df7098072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  emploment type                                    job function  \\\n",
            "0            NaN                                         [Other]   \n",
            "1            NaN  [Engineering, Finance, Information Technology]   \n",
            "2            NaN           [Engineering, Information Technology]   \n",
            "3            NaN                        [Education and Training]   \n",
            "4            NaN           [Engineering, Information Technology]   \n",
            "\n",
            "        description of product/service  \\\n",
            "0                                [N/A]   \n",
            "1      [Algorithmic Trading Developer]   \n",
            "2      [Quantum Engineering Solutions]   \n",
            "3     [Quantum computing applications]   \n",
            "4  [Quantitative software development]   \n",
            "\n",
            "                                          industries  \\\n",
            "0                             [Software Development]   \n",
            "1  [Financial Services, Investment Banking, Softw...   \n",
            "2  [Appliances, Electrical, Electronics Manufactu...   \n",
            "3                                [Research Services]   \n",
            "4                       [IT Services, IT Consulting]   \n",
            "\n",
            "                           position name broader role name  \\\n",
            "0          [Machine Learning Consultant]             [N/A]   \n",
            "1                  [Algorithm Developer]             [N/A]   \n",
            "2            [Quantum Solution Engineer]             [N/A]   \n",
            "3  [Summer Internship - Quantum Systems]          [Intern]   \n",
            "4       [Quantitative Software Engineer]             [N/A]   \n",
            "\n",
            "                   company           location  \\\n",
            "0                   [Dice]    [United States]   \n",
            "1                   [Nurp]    [United States]   \n",
            "2  [Keysight Technologies]   [Santa Rosa, CA]   \n",
            "3   [QuEra Computing Inc.]       [Boston, MA]   \n",
            "4                   [Raft]  [San Antonio, TX]   \n",
            "\n",
            "         salary/compensation range  \\\n",
            "0                            [N/A]   \n",
            "1                              NaN   \n",
            "2                            [N/A]   \n",
            "3                              NaN   \n",
            "4  [$90,000.00/yr, $170,000.00/yr]   \n",
            "\n",
            "                                    responsibilities  \\\n",
            "0  [Requirement gathering and documentation, Ente...   \n",
            "1  [Translate trading strategies to algorithms, D...   \n",
            "2  [Customer success, Engage with customers, Defi...   \n",
            "3  [Holography/Laser Beam Optimization, Algorithm...   \n",
            "4  [Develop software for quantitative methods, Dr...   \n",
            "\n",
            "                                    goals/objectives  \\\n",
            "0                                              [N/A]   \n",
            "1                                                NaN   \n",
            "2  [Create world-leading quantum solutions, Innov...   \n",
            "3                                                NaN   \n",
            "4                                              [N/A]   \n",
            "\n",
            "           name of department/team  \\\n",
            "0                            [N/A]   \n",
            "1                            [N/A]   \n",
            "2  [Quantum Engineering Solutions]   \n",
            "3                              NaN   \n",
            "4                            [N/A]   \n",
            "\n",
            "                             required qualifications  \\\n",
            "0  [Strong understanding of SDLC, Proficiency in ...   \n",
            "1  [Bachelor's or Master's degree in relevant fie...   \n",
            "2  [MS or PhD in physics or engineering, 5+ years...   \n",
            "3  [Degree in CS/Physics/Math or related, Python ...   \n",
            "4  [3+ years software development, Python/Java/C+...   \n",
            "\n",
            "                            preferred qualifications  \\\n",
            "0                                              [N/A]   \n",
            "1                                              [N/A]   \n",
            "2  [Experimental background in quantum physics, O...   \n",
            "3                                [Optics experience]   \n",
            "4  [Big data processing experience, Machine learn...   \n",
            "\n",
            "                                            benefits work arrangement  \n",
            "0                                              [N/A]         [Remote]  \n",
            "1  [100% Remote work, Health, Dental, Vision Insu...         [Remote]  \n",
            "2  [Medical, dental, vision, 401(k) Plan, Paid Ho...        [On-site]  \n",
            "3                         [Diverse work environment]        [On-site]  \n",
            "4  [Competitive salary, Healthcare/dental/vision ...   [Local remote]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have code that effectively creates a crude form of the dataframe we want. There is still data and feature engineering to do."
      ],
      "metadata": {
        "id": "SuMjXBx30dph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once this dataframe is created from all of the text files, we"
      ],
      "metadata": {
        "id": "sr96kNhz0sdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "q4tkrhED1kHM",
        "outputId": "45bebd2c-4a11-4285-aced-14ce058de654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  emploment type                                    job function  \\\n",
              "0            NaN                                         [Other]   \n",
              "1            NaN  [Engineering, Finance, Information Technology]   \n",
              "2            NaN           [Engineering, Information Technology]   \n",
              "3            NaN                        [Education and Training]   \n",
              "4            NaN           [Engineering, Information Technology]   \n",
              "5            NaN                            [Engineering and IT]   \n",
              "6            NaN           [Engineering, Information Technology]   \n",
              "7            NaN               [Design, Consulting, Engineering]   \n",
              "\n",
              "                     description of product/service  \\\n",
              "0                                             [N/A]   \n",
              "1                   [Algorithmic Trading Developer]   \n",
              "2                   [Quantum Engineering Solutions]   \n",
              "3                  [Quantum computing applications]   \n",
              "4               [Quantitative software development]   \n",
              "5                                               NaN   \n",
              "6  [Kernel development for safety-critical systems]   \n",
              "7                         [Optical Design Software]   \n",
              "\n",
              "                                          industries  \\\n",
              "0                             [Software Development]   \n",
              "1  [Financial Services, Investment Banking, Softw...   \n",
              "2  [Appliances, Electrical, Electronics Manufactu...   \n",
              "3                                [Research Services]   \n",
              "4                       [IT Services, IT Consulting]   \n",
              "5                       [IT Services, IT Consulting]   \n",
              "6  [Airlines and Aviation, Aerospace Component Ma...   \n",
              "7  [Semiconductor Manufacturing, Software Develop...   \n",
              "\n",
              "                                      position name     broader role name  \\\n",
              "0                     [Machine Learning Consultant]                 [N/A]   \n",
              "1                             [Algorithm Developer]                 [N/A]   \n",
              "2                       [Quantum Solution Engineer]                 [N/A]   \n",
              "3             [Summer Internship - Quantum Systems]              [Intern]   \n",
              "4                  [Quantitative Software Engineer]                 [N/A]   \n",
              "5                            [AI Software Engineer]   [Software Engineer]   \n",
              "6  [Associate Software Engineer - Kernel Developer]   [Software Engineer]   \n",
              "7                        [Software Architect - C++]  [Software Architect]   \n",
              "\n",
              "                   company           location  \\\n",
              "0                   [Dice]    [United States]   \n",
              "1                   [Nurp]    [United States]   \n",
              "2  [Keysight Technologies]   [Santa Rosa, CA]   \n",
              "3   [QuEra Computing Inc.]       [Boston, MA]   \n",
              "4                   [Raft]  [San Antonio, TX]   \n",
              "5                   [Zoom]  [California, USA]   \n",
              "6                 [Boeing]         [Mesa, AZ]   \n",
              "7           [Synopsys Inc]       [Irvine, CA]   \n",
              "\n",
              "         salary/compensation range  \\\n",
              "0                            [N/A]   \n",
              "1                              NaN   \n",
              "2                            [N/A]   \n",
              "3                              NaN   \n",
              "4  [$90,000.00/yr, $170,000.00/yr]   \n",
              "5                              NaN   \n",
              "6                              NaN   \n",
              "7                            [N/A]   \n",
              "\n",
              "                                    responsibilities  \\\n",
              "0  [Requirement gathering and documentation, Ente...   \n",
              "1  [Translate trading strategies to algorithms, D...   \n",
              "2  [Customer success, Engage with customers, Defi...   \n",
              "3  [Holography/Laser Beam Optimization, Algorithm...   \n",
              "4  [Develop software for quantitative methods, Dr...   \n",
              "5  [Collaborate multidisciplinary teams, Implemen...   \n",
              "6  [Develops reusable architectures, Serves as su...   \n",
              "7  [Guide development process, Assess legacy syst...   \n",
              "\n",
              "                                    goals/objectives  \\\n",
              "0                                              [N/A]   \n",
              "1                                                NaN   \n",
              "2  [Create world-leading quantum solutions, Innov...   \n",
              "3                                                NaN   \n",
              "4                                              [N/A]   \n",
              "5                                                NaN   \n",
              "6                                                NaN   \n",
              "7  [Deliver cutting-edge optical solutions, Integ...   \n",
              "\n",
              "           name of department/team  \\\n",
              "0                            [N/A]   \n",
              "1                            [N/A]   \n",
              "2  [Quantum Engineering Solutions]   \n",
              "3                              NaN   \n",
              "4                            [N/A]   \n",
              "5                              NaN   \n",
              "6                              NaN   \n",
              "7                   [Synopsys OSG]   \n",
              "\n",
              "                             required qualifications  \\\n",
              "0  [Strong understanding of SDLC, Proficiency in ...   \n",
              "1  [Bachelor's or Master's degree in relevant fie...   \n",
              "2  [MS or PhD in physics or engineering, 5+ years...   \n",
              "3  [Degree in CS/Physics/Math or related, Python ...   \n",
              "4  [3+ years software development, Python/Java/C+...   \n",
              "5  [Advanced education in CS, AI, Deep understand...   \n",
              "6  [Bachelor/Master/Doctorate in related field, 1...   \n",
              "7  [Bachelor's degree in CS or related field, 6+ ...   \n",
              "\n",
              "                            preferred qualifications  \\\n",
              "0                                              [N/A]   \n",
              "1                                              [N/A]   \n",
              "2  [Experimental background in quantum physics, O...   \n",
              "3                                [Optics experience]   \n",
              "4  [Big data processing experience, Machine learn...   \n",
              "5                                              [N/A]   \n",
              "6  [2+ years C++, C, or C# experience, Real-time ...   \n",
              "7  [Background in Physics, Optics, Mathematics, E...   \n",
              "\n",
              "                                            benefits  work arrangement  \n",
              "0                                              [N/A]          [Remote]  \n",
              "1  [100% Remote work, Health, Dental, Vision Insu...          [Remote]  \n",
              "2  [Medical, dental, vision, 401(k) Plan, Paid Ho...         [On-site]  \n",
              "3                         [Diverse work environment]         [On-site]  \n",
              "4  [Competitive salary, Healthcare/dental/vision ...    [Local remote]  \n",
              "5  [Diverse perks, health support, work-life bala...          [Hybrid]  \n",
              "6  [Health insurance, Retirement savings plans, P...         [Virtual]  \n",
              "7  [Cutting-edge technology projects, Dynamic wor...  [Hybrid, Remote]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab759d13-3c99-4dfc-9860-76cfb2aaf4a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emploment type</th>\n",
              "      <th>job function</th>\n",
              "      <th>description of product/service</th>\n",
              "      <th>industries</th>\n",
              "      <th>position name</th>\n",
              "      <th>broader role name</th>\n",
              "      <th>company</th>\n",
              "      <th>location</th>\n",
              "      <th>salary/compensation range</th>\n",
              "      <th>responsibilities</th>\n",
              "      <th>goals/objectives</th>\n",
              "      <th>name of department/team</th>\n",
              "      <th>required qualifications</th>\n",
              "      <th>preferred qualifications</th>\n",
              "      <th>benefits</th>\n",
              "      <th>work arrangement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Other]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Software Development]</td>\n",
              "      <td>[Machine Learning Consultant]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Dice]</td>\n",
              "      <td>[United States]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Requirement gathering and documentation, Ente...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Strong understanding of SDLC, Proficiency in ...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Remote]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Finance, Information Technology]</td>\n",
              "      <td>[Algorithmic Trading Developer]</td>\n",
              "      <td>[Financial Services, Investment Banking, Softw...</td>\n",
              "      <td>[Algorithm Developer]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Nurp]</td>\n",
              "      <td>[United States]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Translate trading strategies to algorithms, D...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Bachelor's or Master's degree in relevant fie...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[100% Remote work, Health, Dental, Vision Insu...</td>\n",
              "      <td>[Remote]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[Quantum Engineering Solutions]</td>\n",
              "      <td>[Appliances, Electrical, Electronics Manufactu...</td>\n",
              "      <td>[Quantum Solution Engineer]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Keysight Technologies]</td>\n",
              "      <td>[Santa Rosa, CA]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Customer success, Engage with customers, Defi...</td>\n",
              "      <td>[Create world-leading quantum solutions, Innov...</td>\n",
              "      <td>[Quantum Engineering Solutions]</td>\n",
              "      <td>[MS or PhD in physics or engineering, 5+ years...</td>\n",
              "      <td>[Experimental background in quantum physics, O...</td>\n",
              "      <td>[Medical, dental, vision, 401(k) Plan, Paid Ho...</td>\n",
              "      <td>[On-site]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Education and Training]</td>\n",
              "      <td>[Quantum computing applications]</td>\n",
              "      <td>[Research Services]</td>\n",
              "      <td>[Summer Internship - Quantum Systems]</td>\n",
              "      <td>[Intern]</td>\n",
              "      <td>[QuEra Computing Inc.]</td>\n",
              "      <td>[Boston, MA]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Holography/Laser Beam Optimization, Algorithm...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Degree in CS/Physics/Math or related, Python ...</td>\n",
              "      <td>[Optics experience]</td>\n",
              "      <td>[Diverse work environment]</td>\n",
              "      <td>[On-site]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[Quantitative software development]</td>\n",
              "      <td>[IT Services, IT Consulting]</td>\n",
              "      <td>[Quantitative Software Engineer]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Raft]</td>\n",
              "      <td>[San Antonio, TX]</td>\n",
              "      <td>[$90,000.00/yr, $170,000.00/yr]</td>\n",
              "      <td>[Develop software for quantitative methods, Dr...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[3+ years software development, Python/Java/C+...</td>\n",
              "      <td>[Big data processing experience, Machine learn...</td>\n",
              "      <td>[Competitive salary, Healthcare/dental/vision ...</td>\n",
              "      <td>[Local remote]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering and IT]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[IT Services, IT Consulting]</td>\n",
              "      <td>[AI Software Engineer]</td>\n",
              "      <td>[Software Engineer]</td>\n",
              "      <td>[Zoom]</td>\n",
              "      <td>[California, USA]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Collaborate multidisciplinary teams, Implemen...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Advanced education in CS, AI, Deep understand...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Diverse perks, health support, work-life bala...</td>\n",
              "      <td>[Hybrid]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[Kernel development for safety-critical systems]</td>\n",
              "      <td>[Airlines and Aviation, Aerospace Component Ma...</td>\n",
              "      <td>[Associate Software Engineer - Kernel Developer]</td>\n",
              "      <td>[Software Engineer]</td>\n",
              "      <td>[Boeing]</td>\n",
              "      <td>[Mesa, AZ]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Develops reusable architectures, Serves as su...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Bachelor/Master/Doctorate in related field, 1...</td>\n",
              "      <td>[2+ years C++, C, or C# experience, Real-time ...</td>\n",
              "      <td>[Health insurance, Retirement savings plans, P...</td>\n",
              "      <td>[Virtual]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Design, Consulting, Engineering]</td>\n",
              "      <td>[Optical Design Software]</td>\n",
              "      <td>[Semiconductor Manufacturing, Software Develop...</td>\n",
              "      <td>[Software Architect - C++]</td>\n",
              "      <td>[Software Architect]</td>\n",
              "      <td>[Synopsys Inc]</td>\n",
              "      <td>[Irvine, CA]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Guide development process, Assess legacy syst...</td>\n",
              "      <td>[Deliver cutting-edge optical solutions, Integ...</td>\n",
              "      <td>[Synopsys OSG]</td>\n",
              "      <td>[Bachelor's degree in CS or related field, 6+ ...</td>\n",
              "      <td>[Background in Physics, Optics, Mathematics, E...</td>\n",
              "      <td>[Cutting-edge technology projects, Dynamic wor...</td>\n",
              "      <td>[Hybrid, Remote]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab759d13-3c99-4dfc-9860-76cfb2aaf4a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab759d13-3c99-4dfc-9860-76cfb2aaf4a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab759d13-3c99-4dfc-9860-76cfb2aaf4a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b36e375a-46e0-4ec2-87db-56494bd811c6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b36e375a-46e0-4ec2-87db-56494bd811c6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b36e375a-46e0-4ec2-87db-56494bd811c6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4e9433c5-ccbf-4775-aacb-980e39b4159a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4e9433c5-ccbf-4775-aacb-980e39b4159a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "'str' object has no attribute 'empty'"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first thought might be to fill all missing values (or fields with value NaN) to 0. When we tokenize words and then phrases in the vocabulary, we will make sure nothing is assigned value 0. Or, we will make one-hot encodings of phrase occurence for each column, but that is not ideal. Hopefully the algorithm we use can learn well enough from label encodings rather than having to use one-hot encodings which would make the dataframe have many many columns. We know that this can negatively impact performance but the difference might be small enough to justify abandoning the one-hot approach."
      ],
      "metadata": {
        "id": "brs1-B4x2YH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df.fillna(0)\n",
        "\n",
        "print(df_filled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkxfjJd838Nm",
        "outputId": "70b29250-8d7b-4aa2-8460-61a18583f08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emploment type                                    job function  \\\n",
            "0               0                                         [Other]   \n",
            "1               0  [Engineering, Finance, Information Technology]   \n",
            "2               0           [Engineering, Information Technology]   \n",
            "3               0                        [Education and Training]   \n",
            "4               0           [Engineering, Information Technology]   \n",
            "5               0                            [Engineering and IT]   \n",
            "6               0           [Engineering, Information Technology]   \n",
            "7               0               [Design, Consulting, Engineering]   \n",
            "\n",
            "                     description of product/service  \\\n",
            "0                                             [N/A]   \n",
            "1                   [Algorithmic Trading Developer]   \n",
            "2                   [Quantum Engineering Solutions]   \n",
            "3                  [Quantum computing applications]   \n",
            "4               [Quantitative software development]   \n",
            "5                                                 0   \n",
            "6  [Kernel development for safety-critical systems]   \n",
            "7                         [Optical Design Software]   \n",
            "\n",
            "                                          industries  \\\n",
            "0                             [Software Development]   \n",
            "1  [Financial Services, Investment Banking, Softw...   \n",
            "2  [Appliances, Electrical, Electronics Manufactu...   \n",
            "3                                [Research Services]   \n",
            "4                       [IT Services, IT Consulting]   \n",
            "5                       [IT Services, IT Consulting]   \n",
            "6  [Airlines and Aviation, Aerospace Component Ma...   \n",
            "7  [Semiconductor Manufacturing, Software Develop...   \n",
            "\n",
            "                                      position name     broader role name  \\\n",
            "0                     [Machine Learning Consultant]                 [N/A]   \n",
            "1                             [Algorithm Developer]                 [N/A]   \n",
            "2                       [Quantum Solution Engineer]                 [N/A]   \n",
            "3             [Summer Internship - Quantum Systems]              [Intern]   \n",
            "4                  [Quantitative Software Engineer]                 [N/A]   \n",
            "5                            [AI Software Engineer]   [Software Engineer]   \n",
            "6  [Associate Software Engineer - Kernel Developer]   [Software Engineer]   \n",
            "7                        [Software Architect - C++]  [Software Architect]   \n",
            "\n",
            "                   company           location  \\\n",
            "0                   [Dice]    [United States]   \n",
            "1                   [Nurp]    [United States]   \n",
            "2  [Keysight Technologies]   [Santa Rosa, CA]   \n",
            "3   [QuEra Computing Inc.]       [Boston, MA]   \n",
            "4                   [Raft]  [San Antonio, TX]   \n",
            "5                   [Zoom]  [California, USA]   \n",
            "6                 [Boeing]         [Mesa, AZ]   \n",
            "7           [Synopsys Inc]       [Irvine, CA]   \n",
            "\n",
            "         salary/compensation range  \\\n",
            "0                            [N/A]   \n",
            "1                                0   \n",
            "2                            [N/A]   \n",
            "3                                0   \n",
            "4  [$90,000.00/yr, $170,000.00/yr]   \n",
            "5                                0   \n",
            "6                                0   \n",
            "7                            [N/A]   \n",
            "\n",
            "                                    responsibilities  \\\n",
            "0  [Requirement gathering and documentation, Ente...   \n",
            "1  [Translate trading strategies to algorithms, D...   \n",
            "2  [Customer success, Engage with customers, Defi...   \n",
            "3  [Holography/Laser Beam Optimization, Algorithm...   \n",
            "4  [Develop software for quantitative methods, Dr...   \n",
            "5  [Collaborate multidisciplinary teams, Implemen...   \n",
            "6  [Develops reusable architectures, Serves as su...   \n",
            "7  [Guide development process, Assess legacy syst...   \n",
            "\n",
            "                                    goals/objectives  \\\n",
            "0                                              [N/A]   \n",
            "1                                                  0   \n",
            "2  [Create world-leading quantum solutions, Innov...   \n",
            "3                                                  0   \n",
            "4                                              [N/A]   \n",
            "5                                                  0   \n",
            "6                                                  0   \n",
            "7  [Deliver cutting-edge optical solutions, Integ...   \n",
            "\n",
            "           name of department/team  \\\n",
            "0                            [N/A]   \n",
            "1                            [N/A]   \n",
            "2  [Quantum Engineering Solutions]   \n",
            "3                                0   \n",
            "4                            [N/A]   \n",
            "5                                0   \n",
            "6                                0   \n",
            "7                   [Synopsys OSG]   \n",
            "\n",
            "                             required qualifications  \\\n",
            "0  [Strong understanding of SDLC, Proficiency in ...   \n",
            "1  [Bachelor's or Master's degree in relevant fie...   \n",
            "2  [MS or PhD in physics or engineering, 5+ years...   \n",
            "3  [Degree in CS/Physics/Math or related, Python ...   \n",
            "4  [3+ years software development, Python/Java/C+...   \n",
            "5  [Advanced education in CS, AI, Deep understand...   \n",
            "6  [Bachelor/Master/Doctorate in related field, 1...   \n",
            "7  [Bachelor's degree in CS or related field, 6+ ...   \n",
            "\n",
            "                            preferred qualifications  \\\n",
            "0                                              [N/A]   \n",
            "1                                              [N/A]   \n",
            "2  [Experimental background in quantum physics, O...   \n",
            "3                                [Optics experience]   \n",
            "4  [Big data processing experience, Machine learn...   \n",
            "5                                              [N/A]   \n",
            "6  [2+ years C++, C, or C# experience, Real-time ...   \n",
            "7  [Background in Physics, Optics, Mathematics, E...   \n",
            "\n",
            "                                            benefits  work arrangement  \n",
            "0                                              [N/A]          [Remote]  \n",
            "1  [100% Remote work, Health, Dental, Vision Insu...          [Remote]  \n",
            "2  [Medical, dental, vision, 401(k) Plan, Paid Ho...         [On-site]  \n",
            "3                         [Diverse work environment]         [On-site]  \n",
            "4  [Competitive salary, Healthcare/dental/vision ...    [Local remote]  \n",
            "5  [Diverse perks, health support, work-life bala...          [Hybrid]  \n",
            "6  [Health insurance, Retirement savings plans, P...         [Virtual]  \n",
            "7  [Cutting-edge technology projects, Dynamic wor...  [Hybrid, Remote]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step will be to accumulate all the words in the vocabulary. We need to first make a list of all the words that appear in our columns, except for the 'location' and 'salary' column which we will handle differently.\n",
        "\n"
      ],
      "metadata": {
        "id": "0w1HAiLj6wXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the following does not contain 'location' or 'salary/compensation range'\n",
        "columns_to_check = [\"emploment type\", \"job function\", \"description of product/service\", \"industries\",\n",
        "              \"position name\", \"broader role name\", \"company\", \"responsibilities\", \"goals/objectives\",\n",
        "              \"name of department/team\", \"required qualifications\", \"preferred qualifications\",\n",
        "              \"benefits\", \"work arrangement\"]\n",
        "\n",
        "# Get the set of all unique values\n",
        "unique_values = set(df[columns_to_check].stack().unique())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "wNJcQxy88H4A",
        "outputId": "0446b5ca-2de4-4249-cf9d-52c50fb47818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-66ce1d9514fa>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the set of all unique values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0mCategories\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m'b'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m         \"\"\"\n\u001b[0;32m-> 2242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0munique_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here exposes a major problem: most of our values are lists which this df.stack().unique() call cannot unhash. We may have to evaluate the vocabulary distribution a different way which is not ideal. We will have to generate all of our .json files and save them off. Then, we will need special logic that goes through our chosen fields in each JSON file that we have chosen for our training set (80% of files) and adds every word that occurs to a registry which is a big list or dictionary. Then after doing this each word in the vocab registry will have an associated integer. We will make 'N/A' be 0 as the NaN's will also be filled with 0. By having a lookup registry for each word in the vocab set we can represent each unique phrase as a unique number based on applying Cantor's rule to the set of words in the phrase. When we set up the registry we will not include standard stopping words (e.g. a, the, and, etc) and thus we will lose track of any stopping words but assign numbers to those that are not. This list of phrases separated by commas should then become a list of lists of numbers. For each list of numbers, we will use the Cantor function to calculate its numerical representation. Thus a phrase of 'N/A' will get a value of 1 since 2^0 = 1. Then every field of the dataframe will have numbers, or a list.\n",
        "\n",
        "From talking about my plans though, I am starting to get quite worried that a label encoding will be bad. Maybe we should at least do a one-hot encoding for phrase occurences in each column. There will probably be at least several hundred different phrases but maybe more. My first instinct is to see how well we will do if we then encode our groups of phrases into unique numbers for each group by again applying Cantor's rule."
      ],
      "metadata": {
        "id": "R6acLuZa8x1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now, we backtrack to generating AND saving the JSON files. Then we will shuffle our rows, and pick an 80% training subset on which to tokenize our words."
      ],
      "metadata": {
        "id": "zSaGTiZkB6jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we modify the generate_json_data function to be a generate_json_file function."
      ],
      "metadata": {
        "id": "hEbVJGIUC0MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "def generate_json_file(text_file_path):\n",
        "  with open(text_file_path, 'r') as file:\n",
        "    # Read the contents of the file into a string\n",
        "    file_contents = file.read()\n",
        "\n",
        "  prompt = \"Take the following text from a job posting and extract all info related to the following fields which will end up being the keys in the JSON data you will return: emploment type, job function, description of product/service, industries, position name, broader role name, company, location, salary/compensation range, responsibilities, goals/objectives, name of department/team, required qualifications, preferred qualifications, benefits, work arrangement (hybrid, on-site, remote?), organizing this info into a table where the first column contains the fields and the second column contains the info relevant to these fields. Try to extract as much of the info as possible while being as succinct as possible. Remove redundancy wherever possible. If there is no info related to a particular field, put 'N/A'. Standardize entries in the info column by separating individual considerations with a hyphen. Do not exceed 5 words for any individual consideration in the info column. Use acronyms and abbreviations where necessary to do so. Return the table as a json data structure that is loadable in python, and make sure the keys are the fields listed earlier, not 'fields' and 'info'. Store the dictionary values as python lists. Return only the JSON and no title. Here is the text from the job posting: \"\n",
        "\n",
        "  prompt = prompt + \"/n\" + file_contents\n",
        "\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        "  )\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "  )\n",
        "  json_files_dir = r\"drive/MyDrive/LI-Jobs-JSON/\"\n",
        "  json_data = json.loads(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "  json_file_path = json_files_dir + text_file_path.strip(r\"drive/MyDrive/text_files2/\").strip('.txt') + '.json'\n",
        "  # Write JSON data to the file\n",
        "  with open(json_file_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file)\n",
        "\n",
        "  return json_file_path\n"
      ],
      "metadata": {
        "id": "Utu79EtCB5zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n612Df8rpaIY",
        "outputId": "60748205-968c-4ed0-e8d8-aa6a7c180188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we should have our files generated we need not worry about making the dataframe at this stage."
      ],
      "metadata": {
        "id": "ftoTLWt0DCBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir_name = \"drive/MyDrive/text_files2/\"\n",
        "\n",
        "files_list = os.listdir(dir_name)\n",
        "\n",
        "for i in files_list:\n",
        "  if 'row' in i:\n",
        "    file_path = dir_name + i\n",
        "    generate_json_file(file_path)\n",
        "    # ignore adding row to dataframe for now\n",
        "    #add_row_to_df(json_data)"
      ],
      "metadata": {
        "id": "kK50G5_MDRdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zsh54MTgnnq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent, so it took 57 seconds for the above code to generate 12 .json files. So the stage of preparing the json files for 100 text files would take around 10 mins, which is not bad. Also, all my OpenAI API calls so far have cost me 5 cents. I estimate that I've made up to 50 or so. So if I need to make 500 API calls, it will cost me about 50 cents! I can live with that."
      ],
      "metadata": {
        "id": "8OqiaBSzDvv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJrDW67iKqKn",
        "outputId": "cc6a1a42-0a21-46ec-e718-a043535e7e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plan is to now attempt to generate a json file for every one of the 107 text files we have saved in Drive. We will save the JSON files to this folder in Drive: https://drive.google.com/drive/u/0/folders/1XZV0nbGcIuT0Kim3IoNP0sdrKuvi1uza"
      ],
      "metadata": {
        "id": "1jW9OdnV3YPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"drive/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwsAP5u_4F4w",
        "outputId": "7c05636d-047c-4ad4-d187-ce91d260f588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How to get started with Drive.pdf',\n",
              " 'Stewart Calculus Early Transcendentals 7th.pdf',\n",
              " 'Stewart Calculus 7 Edition [Solutions Manual Chapters 1-11].pdf',\n",
              " 'writ2 article 21st century atlantis.pdf',\n",
              " 'Sublease AgreementLiam.doc',\n",
              " 'Untitled document.gdoc',\n",
              " 'Liam resume.pages',\n",
              " 'Liam resume.pdf',\n",
              " 'Liam resume.doc',\n",
              " 'Psych Final.docx',\n",
              " 'Psych Midterm.docx',\n",
              " 'Psych Practice Questions.zip',\n",
              " '1d stats reports peak measurements ss.pages',\n",
              " '2016_09_13_19_12_55.pdf',\n",
              " '2016_09_13_19_19_55.pdf',\n",
              " '2016_12_05_14_03_42.pdf',\n",
              " 'Griffiths (1).pdf',\n",
              " 'Hartle,Gravity.pdf',\n",
              " 'Sublease AgreementLiam.doc.gdoc',\n",
              " 'Quantum-Mechanics-griffiths-2ed-Solutions.pdf',\n",
              " 'Liam resume.TLdoc.gdoc',\n",
              " 'Hartle Gravity Solutions.pdf',\n",
              " '[griffiths_d.]_introduction_to_elementary_particle.pdf',\n",
              " 'Liamresume.TLdoc 2.docx',\n",
              " 'Liamresume.TLdoc 2.docx.gdoc',\n",
              " 'Liam Resume Current.docx',\n",
              " 'Resume  05-04-17.docx',\n",
              " 'The BEST Resume.pdf',\n",
              " 'FT Supp.gdoc',\n",
              " 'Fitness Transform CL.gdoc',\n",
              " 'Productive Robotics CovL.gdoc',\n",
              " 'taylor classical mechanics.pdf',\n",
              " 'classical mechanics taylor.pdf',\n",
              " 'Complex Analysis Bak & Newman.pdf',\n",
              " 'Microwave Optics.gslides',\n",
              " 'The BEST Resume.pdf.gdoc',\n",
              " 'Resume Current (1).docx',\n",
              " 'Resume Current.pdf',\n",
              " 'Interferometry.gslides',\n",
              " 'Laser Properties.gslides',\n",
              " 'Abstract algebra chapter 1 solutions.pdf',\n",
              " 'Principles of Mathematical Analysis Rudin.pdf',\n",
              " 'history-of-modern-latin-america-a-meade-teresa-a.pdf',\n",
              " 'Griffiths-Electrodynamics-4ed.pdf',\n",
              " 'Ross-Elementary-Analysis.pdf',\n",
              " 'Brown-Churchill-Complex Variables and Application 8th edition.pdf',\n",
              " 'Abstract algebra dummit.pdf',\n",
              " 'aimless-for you_ 7.mp3',\n",
              " 'a lowfi christmas mix_ 17.mp3',\n",
              " 'close your eyes_ 6.mp3',\n",
              " 'axiom-skadderbrain_ 4.mp3',\n",
              " 'backwhen cherry_ 6.mp3',\n",
              " 'atcafe_ 6.mp3',\n",
              " 'dozing off_ 19.mp3',\n",
              " 'young vibes_ 9.mp3',\n",
              " 'dozing off_ 5.mp3',\n",
              " 'atcafe_ 4.mp3',\n",
              " 'a lowfi christmas mix_ 3 1.mp3',\n",
              " 'axiom-skadderbrain_ 8.mp3',\n",
              " 'atcafe_ 15.mp3',\n",
              " 'dozing off_ 13.mp3',\n",
              " 'dozing off_ 16.mp3',\n",
              " 'delusion_ 3.mp3',\n",
              " 'CALM_ 2.mp3',\n",
              " 'dozing off_ 17.mp3',\n",
              " 'sunday beats_ 2.mp3',\n",
              " 'backwhen cherry_ 9.mp3',\n",
              " 'dozing off_ 10.mp3',\n",
              " 'dozing off_ 14.mp3',\n",
              " 'selcouth_ 5.mp3',\n",
              " 'CALM_ 4.mp3',\n",
              " 'nightlife inst_ 2.mp3',\n",
              " 'nightlife inst_ 9.mp3',\n",
              " 'dozing off_ 1.mp3',\n",
              " 'a lowfi chirstmas mix_ 4 1.mp3',\n",
              " 'sunday beats_ 1.mp3',\n",
              " 'a lowfi christmas mix_ 19.mp3',\n",
              " 'nightlife inst_ 11.mp3',\n",
              " 'childhood_ 3.mp3',\n",
              " 'close your eyes_ 17.mp3',\n",
              " 'boom bap_ 8.mp3',\n",
              " 'backwhen cherry_ 7.mp3',\n",
              " 'close your eyes_ 16.mp3',\n",
              " 'childhood_ 4.mp3',\n",
              " 'nightlife inst_ 19.mp3',\n",
              " 'axiom- skadderbrain_ 2.mp3',\n",
              " 'dozing off_ 11.mp3',\n",
              " 'nightlife inst_ 6.mp3',\n",
              " 'boom bap_ 4.mp3',\n",
              " 'CALM_ 5.mp3',\n",
              " 'selcouth_ 4.mp3',\n",
              " 'delusion_ 1.mp3',\n",
              " 'zimmer 26_ 9.mp3',\n",
              " 'dreamin_ 5.mp3',\n",
              " 'sunday beats_ 4.mp3',\n",
              " 'backwhen cherry_ 1.mp3',\n",
              " 'CALM_ 1.mp3',\n",
              " 'a lowfi christmas mix_ 14.mp3',\n",
              " 'a lowfi christmas mix_ 2.mp3',\n",
              " 'axiom-skadderbrain_ 12.mp3',\n",
              " 'atcafe_ 8.mp3',\n",
              " 'nightlife inst_ 1.mp3',\n",
              " 'delusion_ 2.mp3',\n",
              " 'young vibes_ 6.mp3',\n",
              " 'delusion_ 5.mp3',\n",
              " 'close your eyes_ 10.mp3',\n",
              " 'close your eyes_ 31.mp3',\n",
              " 'close your eyes_ 22.mp3',\n",
              " 'dozing off_ 6.mp3',\n",
              " 'axiom-skadderbrain_ 9.mp3',\n",
              " 'childhood_ 1.mp3',\n",
              " 'sunday beats_ 5.mp3',\n",
              " 'selcouth_ 3.mp3',\n",
              " 'a lowfi christmas mix_ 11.mp3',\n",
              " 'dozing off_ 2.mp3',\n",
              " 'close your eyes_ 3.mp3',\n",
              " 'boom bap_ 1.mp3',\n",
              " 'close your eyes_ 20.mp3',\n",
              " 'close your eyes_ 13.mp3',\n",
              " 'atcafe_ 10.mp3',\n",
              " 'zimmer 26_ 2.mp3',\n",
              " 'close your eyes_ 33.mp3',\n",
              " 'close your eyes_ 4.mp3',\n",
              " 'a lowfi christmas mix_ 5.mp3',\n",
              " 'a lowfi christmas mix_ 1 1.mp3',\n",
              " 'close your eyes_ 19.mp3',\n",
              " 'selcouth_ 7.mp3',\n",
              " 'nightlife inst_ 7.mp3',\n",
              " 'close your eyes_ 29.mp3',\n",
              " 'close your eyes_ 5.mp3',\n",
              " 'atcafe_ 14.mp3',\n",
              " 'a lowfi christmas mix_ 6.mp3',\n",
              " 'axiom-skadderbrain_ 5.mp3',\n",
              " 'dreamin_ 3.mp3',\n",
              " 'young vibes_ 14.mp3',\n",
              " 'atcafe_ 5.mp3',\n",
              " 'boom bap_ 11.mp3',\n",
              " 'nightlife inst_ 13.mp3',\n",
              " 'dreamin_ 6.mp3',\n",
              " 'delusion_ 7.mp3',\n",
              " 'backwhen cherry_ 3.mp3',\n",
              " 'close your eyes_ 15.mp3',\n",
              " 'axiom-skadderbrain_ 10.mp3',\n",
              " 'atcafe_ 7.mp3',\n",
              " 'dozing off_ 25.mp3',\n",
              " 'young vibes_ 13.mp3',\n",
              " 'atcafe_ 1.mp3',\n",
              " 'dreamin_ 4.mp3',\n",
              " 'zimmer 26_ 10.mp3',\n",
              " 'CALM_ 6.mp3',\n",
              " 'dozing off_ 22.mp3',\n",
              " 'a lowfi christmas mix_ 10.mp3',\n",
              " 'aimless-for you_ 4.mp3',\n",
              " 'nightlife inst_ 8.mp3',\n",
              " 'boom bap_ 13.mp3',\n",
              " 'axiom-skadderbrain_ 11.mp3',\n",
              " 'close your eyes_ 8.mp3',\n",
              " 'axiom-skadderbrain_ 6.mp3',\n",
              " 'a lowfi christmas mix_ 9.mp3',\n",
              " 'backwhen cherry_ 10.mp3',\n",
              " 'close your eyes_ 26.mp3',\n",
              " 'CALM_ 3.mp3',\n",
              " 'atcafe_ 9.mp3',\n",
              " 'young vibes_ 12.mp3',\n",
              " 'zimmer 26_ 6.mp3',\n",
              " 'close your eyes_ 28.mp3',\n",
              " 'delusion_ 4.mp3',\n",
              " 'aimless-for you_ 8.mp3',\n",
              " 'atcafe_ 13.mp3',\n",
              " 'delusion_ 6.mp3',\n",
              " 'atcafe_ 12.mp3',\n",
              " 'nightlife inst_ 4.mp3',\n",
              " 'young vibes_ 5.mp3',\n",
              " 'nightlife inst_ 16.mp3',\n",
              " 'backwhen cherry_ 8.mp3',\n",
              " 'backwhen cherry_ 2.mp3',\n",
              " 'close your eyes_ 2.mp3',\n",
              " 'sunday beats_ 3.mp3',\n",
              " 'young vibes_ 11.mp3',\n",
              " 'zimmer 26_ 4.mp3',\n",
              " 'dozing off_ 21.mp3',\n",
              " 'close your eyes_ 27.mp3',\n",
              " 'selcouth_ 1.mp3',\n",
              " 'close your eyes_ 30.mp3',\n",
              " 'nightlife inst_ 10.mp3',\n",
              " 'atcafe_ 2.mp3',\n",
              " 'selcouth_ 6.mp3',\n",
              " 'boom bap_ 5.mp3',\n",
              " 'zimmer 26_ 7.mp3',\n",
              " 'young vibes_ 4.mp3',\n",
              " 'childhood_ 5.mp3',\n",
              " 'backwhen cherry_ 4.mp3',\n",
              " 'young vibes_ 2.mp3',\n",
              " 'a lowfi christmas mix_ 18.mp3',\n",
              " 'axiom- skadderbrain_ 3.mp3',\n",
              " 'boom bap_ 10.mp3',\n",
              " 'dozing off_ 23.mp3',\n",
              " 'a lowfi christmas mix_ 2 1.mp3',\n",
              " 'young vibes_ 10.mp3',\n",
              " 'young vibes_ 7.mp3',\n",
              " 'dozing off_ 24.mp3',\n",
              " 'a lowfi christmas mix_ 12.mp3',\n",
              " 'a lowfi christmas mix_ 7.mp3',\n",
              " 'nightlife inst_ 17.mp3',\n",
              " 'dreamin_ 1.mp3',\n",
              " 'dozing off_ 26.mp3',\n",
              " 'dozing off_ 12.mp3',\n",
              " 'atcafe_ 3.mp3',\n",
              " 'axiom- skadderbrain_ 1.mp3',\n",
              " 'boom bap_ 6.mp3',\n",
              " 'zimmer 26_ 3.mp3',\n",
              " 'atcafe_ 11.mp3',\n",
              " 'close your eyes_ 32.mp3',\n",
              " 'a lowfi christmas mix_ 3.mp3',\n",
              " 'zimmer 26_ 1.mp3',\n",
              " 'aimless-for you_ 1.mp3',\n",
              " 'boom bap_ 7.mp3',\n",
              " 'young vibes_ 3.mp3',\n",
              " 'dozing off_ 7.mp3',\n",
              " 'close your eyes_ 24.mp3',\n",
              " 'close your eyes_ 18.mp3',\n",
              " 'a lowfi christmas mix_ 1.mp3',\n",
              " 'a lowfi christmas mix_ 13.mp3',\n",
              " 'aimless-for you_ 6.mp3',\n",
              " 'selcouth_ 2.mp3',\n",
              " 'nightlife inst_ 12.mp3',\n",
              " 'close your eyes_ 7.mp3',\n",
              " 'atcafe_ 17.mp3',\n",
              " 'close your eyes_ 23.mp3',\n",
              " 'backwhen cherry_ 5.mp3',\n",
              " 'dozing off_ 3.mp3',\n",
              " 'young vibes_ 8.mp3',\n",
              " 'CALM_ 7.mp3',\n",
              " 'sunday beats_ 6.mp3',\n",
              " 'boom bap_ 9.mp3',\n",
              " 'nightlife inst_ 3.mp3',\n",
              " 'a lowfi christmas mix_ 15.mp3',\n",
              " 'close your eyes_ 21.mp3',\n",
              " 'young vibes_ 1.mp3',\n",
              " 'atcafe_ 16.mp3',\n",
              " 'boom bap_ 2.mp3',\n",
              " 'childhood_ 6.mp3',\n",
              " 'dozing off_ 20.mp3',\n",
              " 'aimless-for you_ 5.mp3',\n",
              " 'childhood_ 2.mp3',\n",
              " 'nightlife inst_ 5.mp3',\n",
              " 'dozing off_ 9.mp3',\n",
              " 'zimmer 26_ 11.mp3',\n",
              " 'close your eyes_ 25.mp3',\n",
              " 'aimless-for you_ 2.mp3',\n",
              " 'zimmer 26_ 5.mp3',\n",
              " 'boom bap_ 12.mp3',\n",
              " 'zimmer 26_ 8.mp3',\n",
              " 'boom bap_ 14.mp3',\n",
              " 'a lowfi chirstmas mix_ 4.mp3',\n",
              " 'nightlife inst_ 15.mp3',\n",
              " 'boom bap_ 3.mp3',\n",
              " 'young vibes_ 15.mp3',\n",
              " 'axiom-skadderbrain_ 7.mp3',\n",
              " 'nightlife inst_ 14.mp3',\n",
              " 'dozing off_ 15.mp3',\n",
              " 'a lowfi christmas mix_ 16.mp3',\n",
              " 'dozing off_ 8.mp3',\n",
              " 'nightlife inst_ 18.mp3',\n",
              " 'close your eyes_ 1.mp3',\n",
              " 'close your eyes_ 14.mp3',\n",
              " 'atcafe_ 19.mp3',\n",
              " 'aimless-for you_ 3.mp3',\n",
              " 'close your eyes_ 34.mp3',\n",
              " 'close your eyes_ 11.mp3',\n",
              " 'close your eyes_ 9.mp3',\n",
              " 'dreamin_ 2.mp3',\n",
              " 'close your eyes_ 12.mp3',\n",
              " 'atcafe_ 18.mp3',\n",
              " 'dozing off_ 4.mp3',\n",
              " 'dozing off_ 18.mp3',\n",
              " 'AS Background Consent Form - Rev 3.19.13 [Fillable Form] (1).pdf',\n",
              " 'FLIR ITAR form.pdf',\n",
              " 'FLIR ITAR form.gdoc',\n",
              " 'winmail.dat',\n",
              " 'Untitled folder',\n",
              " 'Resume Current.docx',\n",
              " 'Resume 7-11-21.gdoc',\n",
              " 'RESUME 7-21-2021.gdoc',\n",
              " 'Resume 09_20_21.gdoc',\n",
              " 'Griffiths.pdf',\n",
              " 'Genentech CV.gdoc',\n",
              " 'job apps snippets.gdoc',\n",
              " 'Places bucket list.gsheet',\n",
              " 'RF Wireless Design Validation Test (DVT) Engineering Apple Cover Letter.gdoc',\n",
              " 'resume.gdoc',\n",
              " 'LinkedIn Project Ingredients.gdoc',\n",
              " 'Colab Notebooks',\n",
              " 'Career Strategy Plan - MEC - Liam Abrams.gdoc',\n",
              " 'AI in College Admissions Decisions - A Discussion of Ethical Implications.gdoc',\n",
              " 'Notes for First Meeting with Semih.gdoc',\n",
              " 'Questions for Peter.gdoc',\n",
              " 'Kauai Trip Outline.gdoc',\n",
              " 'Liam Abrams Resume CWC.gdoc',\n",
              " 'Daily improvement grade.gsheet',\n",
              " 'API.gdoc',\n",
              " 'new office roi.docx',\n",
              " 'Desired jobs.gsheet',\n",
              " 'Questions for next meeting (2 14 24).gdoc',\n",
              " 'LI-jobs-features-list.gsheet',\n",
              " 'LI-jobs-ChatGPT-screenshots',\n",
              " 'Liam Abrams Resume CWC - for valet job.gdoc',\n",
              " 'Liam Abrams Resume CWC - dental office.gdoc',\n",
              " 'successfulLinks.gsheet',\n",
              " 'text_files2',\n",
              " 'LI-Jobs-JSON']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will modify our generate_json_files function to save each JSON file to the correct folder in drive. We will also need to modify our for loop so that the files_list has the correct paths to the text files saved in Drive. Then we will rerun both code blocks: the generate_json_files function definition, as well as the for loop code that calls the function. We'll report back on how long it took and what the damage is cost-wise from the OpenAI API calls.\n",
        "\n",
        "It took us about a minute to do 23 before we encountered an issue on row24.\n",
        "\n",
        "We will rerun this against rows 24-107. We made a folder for the already done ones which is a subfolder of text_files2.\n",
        "\n",
        "It turns out we need to unmount and remount drive after moving data files around because the directory structure in this workspace doesn't get updated when I make changes directly to Drive. It took the for loop around 6 mins to do 64, including repeating the first 23. Since we have already generated JSON files for the first 64, we will move the original text files for those 64 over to alreadyDone and rerun the for loop. We have to unmount and remount drive as well after making the changes to text_files2. We got done with another 8 before the for loop ran into the usual error with generating the JSON file. For this scenario, it would probably be wise to do some exception handling to ask ChatGPT again to return the JSON and hopefully on the 2nd try it is valid. For now though we will continue to generate JSON files the rough and dirty way, moving text files that have already been successfully summarized in JSON to alreadyDone, and rerunning everything.\n",
        "\n",
        "We finally finished up the last 35 JSON files in about 3 mins. So we generated JSON files for all 107 original text files. In total, we have made no more than 200 OpenAI API calls, which have incurred a total of 14 cents worth of costs. Not bad! So to process a fairly large dataset of maybe 1000 jobs would cost no more than a dollar!"
      ],
      "metadata": {
        "id": "mr8vjhfK4-s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that all of the JSON file names are missing 'r' and came at as 'ow54.json', probably because we tried concatenating the file name with the directory rather than using os.path.join. We will fix this in the future. We will quickly fix the file names that were generated with a for loop. We will also move all text files that we've processed to \"alreadyDone\", so that when the time comes to add more data to the dataset, it will be clear which ones will require JSON summaries."
      ],
      "metadata": {
        "id": "MC7xBvoIERLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = \"drive/MyDrive/LI-Jobs-JSON/\"\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "files = os.listdir(directory_path)\n",
        "\n",
        "# Iterate over each file and rename it\n",
        "for old_name in files:\n",
        "    if old_name[0] == 'o':\n",
        "      # Construct the new name however you like\n",
        "      new_name = \"r\" + old_name  # Example: Add a prefix to the old name\n",
        "\n",
        "      # Join the directory path with the old file name\n",
        "      old_path = os.path.join(directory_path, old_name)\n",
        "\n",
        "      # Join the directory path with the new file name\n",
        "      new_path = os.path.join(directory_path, new_name)\n",
        "\n",
        "      # Rename the file\n",
        "      os.rename(old_path, new_path)\n",
        "\n",
        "      #print(f\"Renamed '{old_name}' to '{new_name}'\")\n",
        "\n",
        "print(\"All files have been renamed.\")"
      ],
      "metadata": {
        "id": "tWCRFtsOEph6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874e2c8b-be83-4833-812d-88cba8106c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files have been renamed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am thinking that if we do use Cantor's function to make a 1-1 mapping from possible phrase space to number space, we might consider using CatBoostAlgorithm rather than a regular decision tree-based algorithm to make sure those numbers are treated as categorical rather than ordinal. Another thing to think about is would it be straightforward or easy enough to leverage a word-embedding model like Word2Vec to generate effective encodings for us that may actually be treated as ordinal or that are vectors that don't require too many columns to express in our dataframe, which we could then train a regular decision-tree based model on?"
      ],
      "metadata": {
        "id": "V8ejj_gjUgtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I believe we have enough raw data now to use to play around with different ML algorithms. We need to create a dataframe that contains the field-value pairings of each job posting which will be a row in the dataframe. The final dataframe we pass to the algorithm will depend on the algorithm we choose to use. Thus we have some more important decisions to make as well as a fair amount of processing that we still will need to do on the data side. The first thing to note is I have observed two variants of the JSON files we generated. One is the one we anticipated/desired where ChatGPT properly organizes the information into dictionary key-value pairs where the keys are the different fields we specified in our prompt, such as 'employment_type', 'work_arrangement', 'benefits', etc. The other is a dictionary with only two keys: 'fields' and 'info' whose values are lists, and the value for a given index of the 'info' list will be the info value corresponding to the field value for that same index in the 'field' list. So just loading our data from our JSON files into a crude version of our desired final dataframe will require some custom logic since the JSON files aren't all in the same format, unfortunately. In addition, the convention for the field names (whether they are capitalized or not and whether they are snake-cased) varies across JSON files. So we will need to be quite diligent with how we load our data at this point into the dataframe. Not to mention that all the work that will be required to encode our text under each column prior to feeding our data into the ML algorithm will be significant, but we need to focus on one thing at a time so that our objective isn't too daunting. So we will focus on generating a crude version of our final dataframe, where all of the dataframe elements will contain text. In addition, we will need to cross-reference the original successfulLinks spreadsheet in order to correctly add the rating value into our target variable column which we will aptly name 'rating'."
      ],
      "metadata": {
        "id": "Y6Rs2HX-HmXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now embark on our next step, generating a dataframe from the 107 JSON files and the ratings column in the successfulLinks.csv file. In our code we will start by declaring the needed file paths."
      ],
      "metadata": {
        "id": "wJGcDBmuOeRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCmkPKlBPBou",
        "outputId": "df993b12-a873-4e93-a719-0818cfeb3ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_files_path = \"drive/MyDrive/LI-Jobs-JSON/\"\n",
        "links_csv_path = \"drive/MyDrive/successfulLinksGDRIVE.csv\""
      ],
      "metadata": {
        "id": "wXOwNn_bO6hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the plan should be to make the dataframe row-by-row, including the rating field. This may be subject to change. I think it will be helpful to read the successfulLinks csv file into a dataframe so that the indexing will automatically match up between it and the dataframe we are generating."
      ],
      "metadata": {
        "id": "pZy8PYmnSJ3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "links_dataframe = pd.read_csv(links_csv_path, header=None, names=['url', 'rating'])\n",
        "#links_dataframe.head()\n",
        "#print(links_dataframe.loc[0, 'rating'])"
      ],
      "metadata": {
        "id": "H6BdYRHgQuoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent, so now we can hopefully create our dataframe row by row, remembering that we will need some special logic to deal with the different cases of JSON files. We have some code we wrote earlier where we instantiated an empty dataframe with specified column names and made a function to add a row to that dataframe with a JSON dictionary. Let us revisit and modify that code to accomplish what we need to."
      ],
      "metadata": {
        "id": "lFnzG1NKVi1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# let's snakecase our column names to avoid having spaces in them\n",
        "# also we add a rating column at the end of the list to store the target variable\n",
        "df_columns = [\"emploment_type\", \"job_function\", \"description_of_product/service\", \"industries\",\n",
        "              \"position_name\", \"broader_role_name\", \"company\", \"location\", \"salary/compensation_range\",\n",
        "              \"responsibilities\", \"goals/objectives\", \"name_of_department/team\", \"required_qualifications\",\n",
        "              \"preferred_qualifications\", \"benefits\", \"work_arrangement\", \"rating\"]\n",
        "# Initialize DataFrame with column names\n",
        "df = pd.DataFrame(columns=df_columns)"
      ],
      "metadata": {
        "id": "dQ71rzHPMySp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "locale_json_files_path = \"drive/MyDrive/jobLocations/\"\n",
        "salary_json_files_path = \"drive/MyDrive/jobSalaries/\"\n",
        "\n",
        "for i in range(1, 108):\n",
        "  row_num = df.last_valid_index()\n",
        "  if row_num == None:\n",
        "    row_num = 0\n",
        "  else:\n",
        "    row_num = row_num + 1\n",
        "  assert i == row_num + 1\n",
        "  json_file_path = os.path.join(json_files_path, f'row{i}.json')\n",
        "  # Read JSON file\n",
        "  with open(json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  if 'fields' in json_data.keys():\n",
        "    #print(json_data)\n",
        "    field_names = json_data['fields']\n",
        "    for index, value in enumerate(field_names):\n",
        "      info = json_data['info'][index]\n",
        "      # Check if the value exists as a column name (ignoring case)\n",
        "      column_name = value.replace(\" \", \"_\").lower()\n",
        "      if column_name in df.columns:\n",
        "        # Add the info to the corresponding column and row\n",
        "        df.at[row_num, column_name] = info\n",
        "  else:\n",
        "    for key, value in json_data.items():\n",
        "      # Check if the key exists as a column name (ignoring case)\n",
        "      column_name = key.replace(\" \", \"_\").lower()\n",
        "      if column_name == 'emploment_type':\n",
        "        column_name = 'employment_type'\n",
        "      if column_name in df.columns:\n",
        "        # Add the value to the corresponding column and row\n",
        "        df.at[row_num, column_name] = value\n",
        "\n",
        "  locale_json_file_path = os.path.join(locale_json_files_path, f'row{i}.json')\n",
        "  salary_json_file_path = os.path.join(salary_json_files_path, f'row{i}.json')\n",
        "  # Read locale JSON file\n",
        "  with open(locale_json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  city = json_data['city']\n",
        "  state = json_data['state']\n",
        "  country = json_data['country']\n",
        "  df.at[row_num, 'city'] = city\n",
        "  df.at[row_num, 'state'] = state\n",
        "  df.at[row_num, 'country'] = country\n",
        "  # Read salary JSON file\n",
        "  with open(salary_json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  min_salary = json_data['min_salary']\n",
        "  max_salary = json_data['min_salary']\n",
        "  df.at[row_num, 'min_salary'] = min_salary\n",
        "  df.at[row_num, 'max_salary'] = max_salary\n",
        "  rating = links_dataframe.loc[row_num, 'rating']\n",
        "  df.at[row_num, 'rating'] = rating\n"
      ],
      "metadata": {
        "id": "uqM39z9XNy6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to work around the case where the summary is broken down into JSON with 'fields' and 'info' keys. From the looks of it, the associations between the items in 'info' and those in 'fields' aren't always to be correct, as the attempt on the third one of that kind ran into an error. I think we will make the decision to print JSON dictionaries that have 'fields' as a key, and not actually individual fields as keys, and pass on making the JSON dictionary for now until we require CHATGPT to redo the ones we have identified in the more desirable and supported format. It may be prudent to write some code to automatically delete the JSON files with the undesired formats before we generate the new ones in the desired format. In addition we will make a list of those indices so we will know the names of which files to delete."
      ],
      "metadata": {
        "id": "YOpMU1zreWYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "need_fix_indices_list = []\n",
        "for i in range(1, 108):\n",
        "  #row_num = df.last_valid_index()\n",
        "  #if row_num == None:\n",
        "    #row_num = 0\n",
        "  #else:\n",
        "    #row_num = row_num + 1\n",
        "  #assert i == row_num + 1\n",
        "  json_file_path = os.path.join(json_files_path, f'row{i}.json')\n",
        "  # Read JSON file\n",
        "  with open(json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  if 'fields' in json_data.keys():\n",
        "    need_fix_indices_list.append(i)\n",
        "\n",
        "\n",
        "    '''field_names = json_data['fields']\n",
        "    for index, value in enumerate(field_names):\n",
        "      info = json_data['info'][index]\n",
        "      # Check if the value exists as a column name (ignoring case)\n",
        "      column_name = value.replace(\" \", \"_\").lower()\n",
        "      if column_name in df.columns:\n",
        "        # Add the info to the corresponding column and row\n",
        "        df.at[row_num, column_name] = info'''\n",
        "  else:\n",
        "    '''for key, value in json_data.items():\n",
        "      # Check if the key exists as a column name (ignoring case)\n",
        "      column_name = key.replace(\" \", \"_\").lower()\n",
        "      if column_name in df.columns:\n",
        "        # Add the value to the corresponding column and row\n",
        "        df.at[row_num, column_name] = value'''\n",
        "\n",
        "  #rating = links_dataframe.loc[row_num, 'rating']\n",
        "  #df.at[row_num, 'rating'] = rating"
      ],
      "metadata": {
        "id": "clldmJvFgNNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(need_fix_indices_list)\n",
        "print(len(need_fix_indices_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u71L7nkl6Xv",
        "outputId": "b27de4ff-88fc-45de-a3e8-f1dcd8d1a08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 15, 19, 20, 23, 24, 25, 26, 32, 33, 34, 38, 40, 41, 44, 55, 60, 61, 65, 68, 74, 79, 80, 81, 82, 83, 85, 89, 94, 96, 99, 102]\n",
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to get 33 redone responses from ChtGPT. We should probably reengineer our prompt to be as explicit as possible about the JSON file structure."
      ],
      "metadata": {
        "id": "XwY5zwgAnGjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Source directory\n",
        "source_dir = \"drive/MyDrive/text_files2/alreadyDone\"\n",
        "\n",
        "# Destination directory (parent directory)\n",
        "destination_dir = \"drive/MyDrive/text_files2\"\n",
        "text_files_dir_name = destination_dir\n",
        "# List all files in the source directory\n",
        "files = os.listdir(source_dir)\n",
        "\n",
        "# Move each file to the destination directory\n",
        "'''for file in files:\n",
        "    # Get the full path of the source file\n",
        "    source_file = os.path.join(source_dir, file)\n",
        "    # Move the file to the destination directory\n",
        "    shutil.move(source_file, destination_dir)'''\n",
        "\n",
        "#files_list = os.listdir(dir_name)\n",
        "\n",
        "'''for ind in need_fix_indices_list:\n",
        "  json_files_dir = \"drive/MyDrive/LI-Jobs-JSON\"\n",
        "  json_file_name = f'row{ind}.json'\n",
        "  json_file_path = os.path.join(json_files_dir, json_file_name)\n",
        "  os.remove(json_file_path)'''\n",
        "\n",
        "\n",
        "for ind in need_fix_indices_list:\n",
        "  if ind > 83:\n",
        "    file_path = f'row{ind}.txt'\n",
        "    file_path = os.path.join(text_files_dir_name, file_path)\n",
        "    generate_json_file(file_path)\n",
        "    # ignore adding row to dataframe for now\n",
        "    #add_row_to_df(json_data)"
      ],
      "metadata": {
        "id": "AZ0qYeKjnyUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_document(document_path):\n",
        "    \"\"\"Tokenize a document into words.\"\"\"\n",
        "    with open(document_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        words = text.split()\n",
        "        return words"
      ],
      "metadata": {
        "id": "Qt98_LhNqQWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_document(\"drive/MyDrive/LI-Jobs-JSON/row1.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHMSUsNFrCVr",
        "outputId": "842e43ba-ecc6-4d39-bf64-f9cebe13b41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\"employment',\n",
              " 'type\":',\n",
              " '[\"Full-time\"],',\n",
              " '\"job',\n",
              " 'function\":',\n",
              " '[\"Engineering\",',\n",
              " '\"Information',\n",
              " 'Technology\"],',\n",
              " '\"description',\n",
              " 'of',\n",
              " 'product/service\":',\n",
              " '[\"design-led',\n",
              " 'software',\n",
              " 'development\",',\n",
              " '\"end-to-end',\n",
              " 'digital',\n",
              " 'services\"],',\n",
              " '\"industries\":',\n",
              " '[\"Business',\n",
              " 'Consulting\",',\n",
              " '\"Services\"],',\n",
              " '\"position',\n",
              " 'name\":',\n",
              " '[\"Python',\n",
              " 'Software',\n",
              " 'Engineer',\n",
              " '(Robotics/Mechatronics)\"],',\n",
              " '\"broader',\n",
              " 'role',\n",
              " 'name\":',\n",
              " '[\"N/A\"],',\n",
              " '\"company\":',\n",
              " '[\"Fresh',\n",
              " 'Consulting\"],',\n",
              " '\"location\":',\n",
              " '[\"Redmond,',\n",
              " 'WA\"],',\n",
              " '\"salary/compensation',\n",
              " 'range\":',\n",
              " '[\"$30.00/hr',\n",
              " '-',\n",
              " '$45.00/hr\"],',\n",
              " '\"responsibilities\":',\n",
              " '[\"integrate',\n",
              " 'software/hardware',\n",
              " 'components\",',\n",
              " '\"develop',\n",
              " 'UI',\n",
              " 'front-end\",',\n",
              " '\"investigate',\n",
              " 'defects\",',\n",
              " '\"be',\n",
              " 'active',\n",
              " 'team',\n",
              " 'player\",',\n",
              " '\"learn',\n",
              " 'new',\n",
              " 'systems/tools\",',\n",
              " '\"document',\n",
              " 'code',\n",
              " 'quality\",',\n",
              " '\"work',\n",
              " 'on',\n",
              " 'python',\n",
              " 'applications\"],',\n",
              " '\"goals/objectives\":',\n",
              " '[\"manage',\n",
              " 'delivery',\n",
              " 'of',\n",
              " 'high-quality',\n",
              " 'work\"],',\n",
              " '\"name',\n",
              " 'of',\n",
              " 'department/team\":',\n",
              " '[\"N/A\"],',\n",
              " '\"required',\n",
              " 'qualifications\":',\n",
              " '[\"0-1+',\n",
              " 'years',\n",
              " 'experience\",',\n",
              " '\"Python',\n",
              " 'skills\",',\n",
              " '\"programming',\n",
              " 'skills\",',\n",
              " '\"problem',\n",
              " 'solving',\n",
              " 'skills\",',\n",
              " '\"understanding',\n",
              " 'of',\n",
              " 'HW/Embedded',\n",
              " 'Systems/Mechatronics/Robotics\",',\n",
              " '\"troubleshooting',\n",
              " 'skills\",',\n",
              " '\"BSCSE/MSCSE',\n",
              " 'preferred\"],',\n",
              " '\"preferred',\n",
              " 'qualifications\":',\n",
              " '[\"clear',\n",
              " 'communication\",',\n",
              " '\"outside',\n",
              " 'the',\n",
              " 'box',\n",
              " 'thinking\",',\n",
              " '\"work',\n",
              " 'well',\n",
              " 'with',\n",
              " 'others\"],',\n",
              " '\"benefits\":',\n",
              " '[\"100%',\n",
              " 'Medical\",',\n",
              " '\"PTO\",',\n",
              " '\"Holiday',\n",
              " 'Pay\",',\n",
              " '\"401K',\n",
              " 'Plan\"],',\n",
              " '\"work',\n",
              " 'arrangement\":',\n",
              " '[\"N/A\"]}']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So tokenizing our JSON files works pretty well but not perfectly. Moreover, we only want to tokenize the values, not the keys, of the JSON dictionary."
      ],
      "metadata": {
        "id": "z8TBsWcOrcwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One idea I have is to start with a pre-trained Word2Vec model and represent each word in the dataframe as its embedding in that model. One problem however is that there is no guarantee that a given word in our dataset was present in the original training data for that model and we will not be able to get an embedding for that word unless we fit a Word2Vec model to our specific corpus which contains that word. Another issue is that we will need to be very diligent about post-processing our text so that words stuck together with a hyphen (-) or slash (/) are treated as separate words."
      ],
      "metadata": {
        "id": "0xFrHUcncrUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a pre-trained vocabulary model through transfer learning can save us a lot of work on the feature engineering side including having to concatenate all of our text together and tokenize each unique word in it, and it might be able to save us from having to include TF-IDF features or worse still write custom code to encode the units of our corpus (in this case words). And instead of encoding unique bags or groupings of words that make the phrases, we might choose to use a clustering algorithm on this embedding model to shrink down the words feature space in order to then bin the possible phrases into a much smaller set of classes, or we could use some logic involving cosine similarity. There is a lot of thinking as well as implementation work we still need to do."
      ],
      "metadata": {
        "id": "LxX5TBUgd4XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now we will do some EDA on each column to see what special work we will have to do for that particular column."
      ],
      "metadata": {
        "id": "l3Oc4q4Khk97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "iNM6W5WaDVeQ",
        "outputId": "cbe4f541-581f-4d00-b32b-472721ee4c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  emploment_type                                       job_function  \\\n",
              "0            NaN              [Engineering, Information Technology]   \n",
              "1            NaN     [Design, Art/Creative, Information Technology]   \n",
              "2            NaN  [Information Technology, Consulting, Engineering]   \n",
              "3            NaN              [Engineering, Information Technology]   \n",
              "4            NaN  [Engineering, Quality Assurance, Information T...   \n",
              "\n",
              "                      description_of_product/service  \\\n",
              "0  [design-led software development, end-to-end d...   \n",
              "1                        [Surgical Robotics Systems]   \n",
              "2  [Amazon Robotics builds high-performance, real...   \n",
              "3                            [Credit scoring models]   \n",
              "4  [Large-scale distributed software applications...   \n",
              "\n",
              "                                          industries  \\\n",
              "0                    [Business Consulting, Services]   \n",
              "1  [Biotechnology Research, Pharmaceutical Manufa...   \n",
              "2  [Software Development, IT Services, IT Consult...   \n",
              "3  [Financial Services, Capital Markets, IT Servi...   \n",
              "4                       [IT Services, IT Consulting]   \n",
              "\n",
              "                                       position_name    broader_role_name  \\\n",
              "0  [Python Software Engineer (Robotics/Mechatroni...                [N/A]   \n",
              "1                                [Robotics Engineer]                [N/A]   \n",
              "2                            [SDE - Amazon Robotics]                [N/A]   \n",
              "3                         [Senior Software Engineer]  [Software Engineer]   \n",
              "4                        [Software Engineer in Test]                [N/A]   \n",
              "\n",
              "              company                       location  \\\n",
              "0  [Fresh Consulting]                  [Redmond, WA]   \n",
              "1  [Barrington James]      [New York, United States]   \n",
              "2            [Amazon]            [North Reading, MA]   \n",
              "3     [VantageScore®]       [San Francisco Bay Area]   \n",
              "4            [Optomi]  [Dallas-Fort Worth Metroplex]   \n",
              "\n",
              "  salary/compensation_range  \\\n",
              "0   [$30.00/hr - $45.00/hr]   \n",
              "1                       NaN   \n",
              "2                     [N/A]   \n",
              "3           [$150K - $200K]   \n",
              "4                     [N/A]   \n",
              "\n",
              "                                    responsibilities  \\\n",
              "0  [integrate software/hardware components, devel...   \n",
              "1  [Contribute to cutting-edge robotic systems, C...   \n",
              "2  [Help with initial robotic deployments, Plan r...   \n",
              "3  [Application Development, Collaboration, Mento...   \n",
              "4  [Build and maintain automated test infrastruct...   \n",
              "\n",
              "                                    goals/objectives name_of_department/team  \\\n",
              "0             [manage delivery of high-quality work]                   [N/A]   \n",
              "1                                                NaN                     NaN   \n",
              "2  [Build high-performance robotic systems, Inven...                   [N/A]   \n",
              "3                             [Building public APIs]                   [N/A]   \n",
              "4                                              [N/A]                   [N/A]   \n",
              "\n",
              "                             required_qualifications  \\\n",
              "0  [0-1+ years experience, Python skills, program...   \n",
              "1  [Bachelor's, Master's, or Ph.D. in Robotics, M...   \n",
              "2  [3+ years professional software development ex...   \n",
              "3  [Bachelor's degree, Master's degree, Computer ...   \n",
              "4  [5+ years of test automation experience, Profi...   \n",
              "\n",
              "                            preferred_qualifications  \\\n",
              "0  [clear communication, outside the box thinking...   \n",
              "1                                              [N/A]   \n",
              "2  [3+ years full software development life cycle...   \n",
              "3  [Quantitative applications, Fintech experience...   \n",
              "4                                              [N/A]   \n",
              "\n",
              "                                            benefits work_arrangement rating  \n",
              "0        [100% Medical, PTO, Holiday Pay, 401K Plan]            [N/A]      1  \n",
              "1                                              [N/A]            [N/A]      2  \n",
              "2                                              [N/A]        [On-site]      2  \n",
              "3  [401(K) match, Flexible Time Off, 12 Paid Holi...         [Hybrid]      3  \n",
              "4                                              [N/A]        [On-site]      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c0afded-9b4e-4da5-8e76-70d4b1104997\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emploment_type</th>\n",
              "      <th>job_function</th>\n",
              "      <th>description_of_product/service</th>\n",
              "      <th>industries</th>\n",
              "      <th>position_name</th>\n",
              "      <th>broader_role_name</th>\n",
              "      <th>company</th>\n",
              "      <th>location</th>\n",
              "      <th>salary/compensation_range</th>\n",
              "      <th>responsibilities</th>\n",
              "      <th>goals/objectives</th>\n",
              "      <th>name_of_department/team</th>\n",
              "      <th>required_qualifications</th>\n",
              "      <th>preferred_qualifications</th>\n",
              "      <th>benefits</th>\n",
              "      <th>work_arrangement</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[design-led software development, end-to-end d...</td>\n",
              "      <td>[Business Consulting, Services]</td>\n",
              "      <td>[Python Software Engineer (Robotics/Mechatroni...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Fresh Consulting]</td>\n",
              "      <td>[Redmond, WA]</td>\n",
              "      <td>[$30.00/hr - $45.00/hr]</td>\n",
              "      <td>[integrate software/hardware components, devel...</td>\n",
              "      <td>[manage delivery of high-quality work]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[0-1+ years experience, Python skills, program...</td>\n",
              "      <td>[clear communication, outside the box thinking...</td>\n",
              "      <td>[100% Medical, PTO, Holiday Pay, 401K Plan]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Design, Art/Creative, Information Technology]</td>\n",
              "      <td>[Surgical Robotics Systems]</td>\n",
              "      <td>[Biotechnology Research, Pharmaceutical Manufa...</td>\n",
              "      <td>[Robotics Engineer]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Barrington James]</td>\n",
              "      <td>[New York, United States]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Contribute to cutting-edge robotic systems, C...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Bachelor's, Master's, or Ph.D. in Robotics, M...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Information Technology, Consulting, Engineering]</td>\n",
              "      <td>[Amazon Robotics builds high-performance, real...</td>\n",
              "      <td>[Software Development, IT Services, IT Consult...</td>\n",
              "      <td>[SDE - Amazon Robotics]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Amazon]</td>\n",
              "      <td>[North Reading, MA]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Help with initial robotic deployments, Plan r...</td>\n",
              "      <td>[Build high-performance robotic systems, Inven...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[3+ years professional software development ex...</td>\n",
              "      <td>[3+ years full software development life cycle...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[On-site]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[Credit scoring models]</td>\n",
              "      <td>[Financial Services, Capital Markets, IT Servi...</td>\n",
              "      <td>[Senior Software Engineer]</td>\n",
              "      <td>[Software Engineer]</td>\n",
              "      <td>[VantageScore®]</td>\n",
              "      <td>[San Francisco Bay Area]</td>\n",
              "      <td>[$150K - $200K]</td>\n",
              "      <td>[Application Development, Collaboration, Mento...</td>\n",
              "      <td>[Building public APIs]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Bachelor's degree, Master's degree, Computer ...</td>\n",
              "      <td>[Quantitative applications, Fintech experience...</td>\n",
              "      <td>[401(K) match, Flexible Time Off, 12 Paid Holi...</td>\n",
              "      <td>[Hybrid]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[Engineering, Quality Assurance, Information T...</td>\n",
              "      <td>[Large-scale distributed software applications...</td>\n",
              "      <td>[IT Services, IT Consulting]</td>\n",
              "      <td>[Software Engineer in Test]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Optomi]</td>\n",
              "      <td>[Dallas-Fort Worth Metroplex]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Build and maintain automated test infrastruct...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[5+ years of test automation experience, Profi...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[On-site]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c0afded-9b4e-4da5-8e76-70d4b1104997')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c0afded-9b4e-4da5-8e76-70d4b1104997 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c0afded-9b4e-4da5-8e76-70d4b1104997');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2fa9b3b1-5972-4686-a6d1-1c2cdfc781d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fa9b3b1-5972-4686-a6d1-1c2cdfc781d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2fa9b3b1-5972-4686-a6d1-1c2cdfc781d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 107,\n  \"fields\": [\n    {\n      \"column\": \"emploment_type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job_function\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description_of_product/service\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"industries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"position_name\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"broader_role_name\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"company\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salary/compensation_range\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"responsibilities\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"goals/objectives\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name_of_department/team\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"required_qualifications\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preferred_qualifications\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"benefits\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"work_arrangement\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"0\",\n        \"max\": \"3\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2\",\n          \"0\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['emploment_type']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf836B3Wg_lF",
        "outputId": "5967715e-c385-498c-bf0d-87ed9f6efc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['Full-time']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['Full-time']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['Full-time']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that first off, we are missing a 'y' 'emploment_type'. Also, there are only three rows that are Full-time and the rest are nans. So our first option is to just drop this column entirely, as it is just taking up space and yet barely providing any useful information at all.  "
      ],
      "metadata": {
        "id": "UL9iEnifhskt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['job_function']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hY0KvQZiW4B",
        "outputId": "38d4c353-230b-4911-cc15-712b66335e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Engineering', 'Information Technology']\n",
            "['Design', 'Art/Creative', 'Information Technology']\n",
            "['Information Technology', 'Consulting', 'Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Quality Assurance', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology', 'Research']\n",
            "['Information Technology']\n",
            "['Finance', 'Sales']\n",
            "['Engineering']\n",
            "['Information Technology']\n",
            "['Design', 'Art/Creative', 'Information Technology']\n",
            "['Information Technology - Other - Consulting']\n",
            "['Analyst']\n",
            "['Engineering', 'Finance', 'Information Technology']\n",
            "['Engineering']\n",
            "['Accounting/Auditing', 'Finance']\n",
            "['Finance', 'Information Technology']\n",
            "['Engineering']\n",
            "['Research and Education']\n",
            "['Accounting/Auditing', 'Finance']\n",
            "['Engineering', 'Information Technology']\n",
            "['Other']\n",
            "['Administrative', 'Sales', 'Customer Service']\n",
            "['Sales and Business Development']\n",
            "['Accounting/Auditing', 'Finance']\n",
            "['Other']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Manufacturing']\n",
            "['Information Technology']\n",
            "['Education']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Information Technology']\n",
            "['Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Sales', 'General Business', 'Customer Service']\n",
            "['Health Care Provider']\n",
            "['Engineering', 'Information Technology']\n",
            "['Information Technology', 'Consulting', 'Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Other', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Information Technology', 'Business Development', 'Engineering']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Training']\n",
            "['Information Technology', 'Business Development']\n",
            "['Marketing', 'Sales']\n",
            "['Assistant Manager']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Sales and Business Development']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Sales']\n",
            "['Engineering', 'Design', 'Information Technology']\n",
            "['Other']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Information Technology', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Marketing', 'Sales']\n",
            "['Sales and Business Development']\n",
            "['Sales and Business Development']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology', 'Analyst']\n",
            "['Engineering']\n",
            "['Consulting']\n",
            "['Information Technology', 'Engineering']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['Management', 'Manufacturing']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['AI Applications Engineer']\n",
            "['Engineering', 'Information Technology', 'Research']\n",
            "['Engineering', 'Information Technology']\n",
            "['Quality Assurance']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Education and Training']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Science']\n",
            "['Design', 'Consulting', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Finance', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Information Technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The job_function column we should definitely keep, at least for now. We see there are some words grouped together with '/' like Art/Creative. So the first thing we should probably do is .replace('/', ' '), and we PROBABLY want to do this for the whole dataframe but we will only do it for this column right now."
      ],
      "metadata": {
        "id": "BGdoQi7jiecc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '/' with ' ' in each list\n",
        "df['job_function'] = df['job_function'].apply(lambda x: [category.replace('/', ' ') for category in x])"
      ],
      "metadata": {
        "id": "OAiakA7ai7SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['job_function']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Ha9Qc8jlMh",
        "outputId": "1ed13bd4-a434-488a-a99f-dcdf1286697d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Engineering', 'Information Technology']\n",
            "['Design', 'Art Creative', 'Information Technology']\n",
            "['Information Technology', 'Consulting', 'Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Quality Assurance', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology', 'Research']\n",
            "['Information Technology']\n",
            "['Finance', 'Sales']\n",
            "['Engineering']\n",
            "['Information Technology']\n",
            "['Design', 'Art Creative', 'Information Technology']\n",
            "['Information Technology - Other - Consulting']\n",
            "['Analyst']\n",
            "['Engineering', 'Finance', 'Information Technology']\n",
            "['Engineering']\n",
            "['Accounting Auditing', 'Finance']\n",
            "['Finance', 'Information Technology']\n",
            "['Engineering']\n",
            "['Research and Education']\n",
            "['Accounting Auditing', 'Finance']\n",
            "['Engineering', 'Information Technology']\n",
            "['Other']\n",
            "['Administrative', 'Sales', 'Customer Service']\n",
            "['Sales and Business Development']\n",
            "['Accounting Auditing', 'Finance']\n",
            "['Other']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Manufacturing']\n",
            "['Information Technology']\n",
            "['Education']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Information Technology']\n",
            "['Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Sales', 'General Business', 'Customer Service']\n",
            "['Health Care Provider']\n",
            "['Engineering', 'Information Technology']\n",
            "['Information Technology', 'Consulting', 'Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Other', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Information Technology', 'Business Development', 'Engineering']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Training']\n",
            "['Information Technology', 'Business Development']\n",
            "['Marketing', 'Sales']\n",
            "['Assistant Manager']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Sales and Business Development']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Sales']\n",
            "['Engineering', 'Design', 'Information Technology']\n",
            "['Other']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Information Technology', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering', 'Information Technology']\n",
            "['Marketing', 'Sales']\n",
            "['Sales and Business Development']\n",
            "['Sales and Business Development']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Information Technology', 'Analyst']\n",
            "['Engineering']\n",
            "['Consulting']\n",
            "['Information Technology', 'Engineering']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['Management', 'Manufacturing']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering']\n",
            "['Engineering and Information Technology']\n",
            "['AI Applications Engineer']\n",
            "['Engineering', 'Information Technology', 'Research']\n",
            "['Engineering', 'Information Technology']\n",
            "['Quality Assurance']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Education and Training']\n",
            "['Research', 'Analyst', 'Information Technology']\n",
            "['Science']\n",
            "['Design', 'Consulting', 'Engineering']\n",
            "['Information Technology']\n",
            "['Engineering and Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Engineering', 'Finance', 'Information Technology']\n",
            "['Engineering', 'Information Technology']\n",
            "['Information Technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will attempt to find some pre-trained vocabulary model like a pre-trained Word2Vec model and see if we can get word embeddings for each word in each item of each list."
      ],
      "metadata": {
        "id": "rGCBooF5jxHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we are downloading a 500 dimensional pre-trained Word2Vec model from 2018 from Github and it is over 17 GB. Hopefully I will be able to store it on disk space and then instantiate the embedding model within Colab using the gensim API. While we are waiting for that to get downloaded we will do more EDA on the other columns.  "
      ],
      "metadata": {
        "id": "p4bumDkDmqR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['description_of_product/service']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQuWCtmmnkTO",
        "outputId": "009665bd-b65f-418f-d3a3-c305ef340e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['design-led software development', 'end-to-end digital services']\n",
            "['Surgical Robotics Systems']\n",
            "['Amazon Robotics builds high-performance, real-time robotic systems', 'Invent and scale AI systems for robotics in fulfillment', 'Building computer vision systems, ML and AI models, robotic control and motion planning, process management', 'End-to-end ownership of decision explanation, fault detection, monitoring, A/B testing, large scale model training, simulation, hardware integration']\n",
            "['Credit scoring models']\n",
            "['Large-scale distributed software applications, systems, services']\n",
            "['AI-driven education platform']\n",
            "['Financial Services']\n",
            "['Financial Services']\n",
            "['Quantitative trading team', 'ML based data pipelines', 'Analytics for research', 'Systematic trading strategies']\n",
            "nan\n",
            "['Global markets proprietary trading firm']\n",
            "['Remotely-operated endovascular surgical robot']\n",
            "['Bioinformatics pipelines using Nextflow', 'Python/R scripts for data processing', 'Reporting system for clinic-ready reports', 'Documentation for pipeline management', 'Web-based user interfaces']\n",
            "['Financial Planning solutions']\n",
            "['Generative AI and NLP']\n",
            "['ETL Automation', 'Front-End Dashboarding', 'Documentation']\n",
            "['Low Latency Trading Systems']\n",
            "['Advanced medical device']\n",
            "['Augmented Reality Systems Platform']\n",
            "['SAP Analytics Cloud (SAC)', 'Financial Planning processes']\n",
            "['CPUs', 'System on Chips', 'machine learning', 'data centers', 'high-performance computing applications']\n",
            "['Interdisciplinary Artificial Intelligence Research']\n",
            "['N/A']\n",
            "['Security risk assessment']\n",
            "['Qualcomm Neural Processing SDK']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Material handling equipment and systems']\n",
            "['N/A']\n",
            "['Machine Learning', 'X-ray Imaging']\n",
            "['Firmware development for hardware media pipeline', 'Digital Rights Management', 'Trusted Execution Environment', 'Audio/Video formats and containers']\n",
            "['Financial Services']\n",
            "['N/A']\n",
            "['Advancing understanding of radio spectrum and wireless signals']\n",
            "['Automation test frameworks - Selenium, Appium, Playwright', 'iOS XCTest, XCUITest', 'Android Espresso, UI Automator', 'Java, JavaScript']\n",
            "['Teaching students to read and comprehend']\n",
            "['CAD/EDA Tools Software Engineer for Test Chip Design']\n",
            "['Embedded software for Bobcat equipment']\n",
            "['Quantitative Developer for systematic corporate bond and credit derivatives strategies']\n",
            "['Machine Learning ASICs for Data Center servers']\n",
            "['N/A']\n",
            "['Google Cloud Platform', 'cutting-edge technology', 'cleanest cloud in the industry']\n",
            "nan\n",
            "['Marine Engineering Networking', 'C/Networking Software']\n",
            "['consumer electronics']\n",
            "['Financial Services']\n",
            "['Mission capability integrator']\n",
            "['AI Video Platform']\n",
            "['User-owned talent network', 'Connects professionals with enterprises', 'Eliminates middlemen and markups', 'Efficient and quality matching']\n",
            "['Triage Software Engineer - Initial defect analysis - Dashboard creation - Problem-solving - Communication skills - Automotive Infotainment - Embedded software - JIRA/Confluence - Agile/Scrum - Requirement Analysis']\n",
            "['Small satellite industry solutions']\n",
            "nan\n",
            "['Scientific Software Engineer - Space', 'Join a company at the heart of space research and operations', 'Create solutions for complex problems', 'Ensure data and products are of highest quality for aerospace community']\n",
            "['Cybersecurity']\n",
            "['AI/Client models']\n",
            "['Academic support through tutoring']\n",
            "nan\n",
            "['K9 gear']\n",
            "['Boston Public Market']\n",
            "['AI Research']\n",
            "['Artificial Intelligence']\n",
            "['AI research']\n",
            "['AI solutions for educational experience enhancement']\n",
            "['Training devices for military aircraft']\n",
            "['Data & AI solutions']\n",
            "['seismic-acoustic signal processing']\n",
            "['N/A']\n",
            "['Jury consulting services']\n",
            "['Cyber Security services', 'cutting-edge technologies']\n",
            "['Data Engineering for Wholesale Building Materials']\n",
            "['Software Development']\n",
            "['AR/VR development']\n",
            "['AI Microsoft Chatbot Developer']\n",
            "['Creative studio', 'Lifestyle consultancy', 'Branding', 'Marketing', 'Design', 'Web & app development', 'Business leadership', 'Revenue & scale strategies']\n",
            "['Transformer products and solutions']\n",
            "['Cybersecurity Data Services']\n",
            "['Data Science Platform empowers data scientists to build machine learning systems']\n",
            "['Machine Learning Platform']\n",
            "['High-performance data analytics platform handling petabytes of data']\n",
            "['Real Time ML Service', 'RTML Model Serving Framework', 'RTML Framework', 'AI Center', 'AI-driven solutions']\n",
            "['Pre-owned vehicles', 'Financing', 'Warranties', 'Vehicle appraisals']\n",
            "['Autonomous driving software']\n",
            "['Quantitative Software Engineer']\n",
            "['Transportation services']\n",
            "['Clinical Trial Research']\n",
            "['AI for Autonomy Lab researches applying AI-related technologies for autonomy systems']\n",
            "['Embedded Cybersecurity solutions for Caterpillar machines & engines']\n",
            "['AI research and deployment']\n",
            "['Microservices development']\n",
            "['Electric vehicles']\n",
            "['AI platform at Together AI', 'Research-driven artificial intelligence company', 'Contribute to leading open-source research, models, and datasets']\n",
            "['Building open source digital systems and solutions to battle environmental threats', 'Developing open innovative technology to increase planetary resilience', 'Creating software to track progress towards Paris Agreement goals', 'Improving data infrastructure for climate analysis using AI', 'Maximizing impact through Open Source projects']\n",
            "['Software development for finance industry']\n",
            "['Support workflow development for NOAA SFS', 'NWP modeling system', 'Forecast guidance', 'Earth system models']\n",
            "['Software solutions for critical infrastructure']\n",
            "['AI algorithms', 'software applications']\n",
            "['enterprise B2B SaaS solutions']\n",
            "['Quantum control solution (QCS)', 'Quantum computing', 'Quantum communications', 'Quantum sensing']\n",
            "['Quantum Systems']\n",
            "['Quantum computing system', 'Azure Quantum', 'Revolutionize computing']\n",
            "['Quantum Computing and Devices']\n",
            "['Cutting-edge optical design solutions']\n",
            "['Introduction of automation to construction sector']\n",
            "['Software engineering opportunity at JPMorgan Chase']\n",
            "['Ads Creative Management']\n",
            "['Algorithmic trading programs', 'Forex trading systems']\n",
            "[\"Kernel software development for Boeing's product portfolio\"]\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are reminded by the above that we should NOT apply the Lamba function to replace '/' with ' ' to the whole dataframe just yet, as there nan's as well as 'N/A's within lists under at least some of the columns. First we should fill the free nan's with 0's."
      ],
      "metadata": {
        "id": "bKeoBwRMoE8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['description_of_product/service'] = df['description_of_product/service'].fillna(0)\n",
        "\n",
        "for i in df['description_of_product/service']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBZQRhphopDp",
        "outputId": "9157f3a2-fcae-4a23-b150-5e4edc59429d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['design-led software development', 'end-to-end digital services']\n",
            "['Surgical Robotics Systems']\n",
            "['Amazon Robotics builds high-performance, real-time robotic systems', 'Invent and scale AI systems for robotics in fulfillment', 'Building computer vision systems, ML and AI models, robotic control and motion planning, process management', 'End-to-end ownership of decision explanation, fault detection, monitoring, A/B testing, large scale model training, simulation, hardware integration']\n",
            "['Credit scoring models']\n",
            "['Large-scale distributed software applications, systems, services']\n",
            "['AI-driven education platform']\n",
            "['Financial Services']\n",
            "['Financial Services']\n",
            "['Quantitative trading team', 'ML based data pipelines', 'Analytics for research', 'Systematic trading strategies']\n",
            "0\n",
            "['Global markets proprietary trading firm']\n",
            "['Remotely-operated endovascular surgical robot']\n",
            "['Bioinformatics pipelines using Nextflow', 'Python/R scripts for data processing', 'Reporting system for clinic-ready reports', 'Documentation for pipeline management', 'Web-based user interfaces']\n",
            "['Financial Planning solutions']\n",
            "['Generative AI and NLP']\n",
            "['ETL Automation', 'Front-End Dashboarding', 'Documentation']\n",
            "['Low Latency Trading Systems']\n",
            "['Advanced medical device']\n",
            "['Augmented Reality Systems Platform']\n",
            "['SAP Analytics Cloud (SAC)', 'Financial Planning processes']\n",
            "['CPUs', 'System on Chips', 'machine learning', 'data centers', 'high-performance computing applications']\n",
            "['Interdisciplinary Artificial Intelligence Research']\n",
            "['N/A']\n",
            "['Security risk assessment']\n",
            "['Qualcomm Neural Processing SDK']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Material handling equipment and systems']\n",
            "['N/A']\n",
            "['Machine Learning', 'X-ray Imaging']\n",
            "['Firmware development for hardware media pipeline', 'Digital Rights Management', 'Trusted Execution Environment', 'Audio/Video formats and containers']\n",
            "['Financial Services']\n",
            "['N/A']\n",
            "['Advancing understanding of radio spectrum and wireless signals']\n",
            "['Automation test frameworks - Selenium, Appium, Playwright', 'iOS XCTest, XCUITest', 'Android Espresso, UI Automator', 'Java, JavaScript']\n",
            "['Teaching students to read and comprehend']\n",
            "['CAD/EDA Tools Software Engineer for Test Chip Design']\n",
            "['Embedded software for Bobcat equipment']\n",
            "['Quantitative Developer for systematic corporate bond and credit derivatives strategies']\n",
            "['Machine Learning ASICs for Data Center servers']\n",
            "['N/A']\n",
            "['Google Cloud Platform', 'cutting-edge technology', 'cleanest cloud in the industry']\n",
            "0\n",
            "['Marine Engineering Networking', 'C/Networking Software']\n",
            "['consumer electronics']\n",
            "['Financial Services']\n",
            "['Mission capability integrator']\n",
            "['AI Video Platform']\n",
            "['User-owned talent network', 'Connects professionals with enterprises', 'Eliminates middlemen and markups', 'Efficient and quality matching']\n",
            "['Triage Software Engineer - Initial defect analysis - Dashboard creation - Problem-solving - Communication skills - Automotive Infotainment - Embedded software - JIRA/Confluence - Agile/Scrum - Requirement Analysis']\n",
            "['Small satellite industry solutions']\n",
            "0\n",
            "['Scientific Software Engineer - Space', 'Join a company at the heart of space research and operations', 'Create solutions for complex problems', 'Ensure data and products are of highest quality for aerospace community']\n",
            "['Cybersecurity']\n",
            "['AI/Client models']\n",
            "['Academic support through tutoring']\n",
            "0\n",
            "['K9 gear']\n",
            "['Boston Public Market']\n",
            "['AI Research']\n",
            "['Artificial Intelligence']\n",
            "['AI research']\n",
            "['AI solutions for educational experience enhancement']\n",
            "['Training devices for military aircraft']\n",
            "['Data & AI solutions']\n",
            "['seismic-acoustic signal processing']\n",
            "['N/A']\n",
            "['Jury consulting services']\n",
            "['Cyber Security services', 'cutting-edge technologies']\n",
            "['Data Engineering for Wholesale Building Materials']\n",
            "['Software Development']\n",
            "['AR/VR development']\n",
            "['AI Microsoft Chatbot Developer']\n",
            "['Creative studio', 'Lifestyle consultancy', 'Branding', 'Marketing', 'Design', 'Web & app development', 'Business leadership', 'Revenue & scale strategies']\n",
            "['Transformer products and solutions']\n",
            "['Cybersecurity Data Services']\n",
            "['Data Science Platform empowers data scientists to build machine learning systems']\n",
            "['Machine Learning Platform']\n",
            "['High-performance data analytics platform handling petabytes of data']\n",
            "['Real Time ML Service', 'RTML Model Serving Framework', 'RTML Framework', 'AI Center', 'AI-driven solutions']\n",
            "['Pre-owned vehicles', 'Financing', 'Warranties', 'Vehicle appraisals']\n",
            "['Autonomous driving software']\n",
            "['Quantitative Software Engineer']\n",
            "['Transportation services']\n",
            "['Clinical Trial Research']\n",
            "['AI for Autonomy Lab researches applying AI-related technologies for autonomy systems']\n",
            "['Embedded Cybersecurity solutions for Caterpillar machines & engines']\n",
            "['AI research and deployment']\n",
            "['Microservices development']\n",
            "['Electric vehicles']\n",
            "['AI platform at Together AI', 'Research-driven artificial intelligence company', 'Contribute to leading open-source research, models, and datasets']\n",
            "['Building open source digital systems and solutions to battle environmental threats', 'Developing open innovative technology to increase planetary resilience', 'Creating software to track progress towards Paris Agreement goals', 'Improving data infrastructure for climate analysis using AI', 'Maximizing impact through Open Source projects']\n",
            "['Software development for finance industry']\n",
            "['Support workflow development for NOAA SFS', 'NWP modeling system', 'Forecast guidance', 'Earth system models']\n",
            "['Software solutions for critical infrastructure']\n",
            "['AI algorithms', 'software applications']\n",
            "['enterprise B2B SaaS solutions']\n",
            "['Quantum control solution (QCS)', 'Quantum computing', 'Quantum communications', 'Quantum sensing']\n",
            "['Quantum Systems']\n",
            "['Quantum computing system', 'Azure Quantum', 'Revolutionize computing']\n",
            "['Quantum Computing and Devices']\n",
            "['Cutting-edge optical design solutions']\n",
            "['Introduction of automation to construction sector']\n",
            "['Software engineering opportunity at JPMorgan Chase']\n",
            "['Ads Creative Management']\n",
            "['Algorithmic trading programs', 'Forex trading systems']\n",
            "[\"Kernel software development for Boeing's product portfolio\"]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, instances of ['N/A'] should also be replaced with 0."
      ],
      "metadata": {
        "id": "rSfNo5AwpErv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'N/A' with 0\n",
        "import numpy as np\n",
        "df['description_of_product/service'] = df['description_of_product/service'].apply(lambda x: np.nan if x == ['N/A'] else x)"
      ],
      "metadata": {
        "id": "q0fqQwjwpa5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['description_of_product/service']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubaNRQa0p44l",
        "outputId": "995844f3-aae5-4747-d5d4-b5be978a2012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['design-led software development', 'end-to-end digital services']\n",
            "['Surgical Robotics Systems']\n",
            "['Amazon Robotics builds high-performance, real-time robotic systems', 'Invent and scale AI systems for robotics in fulfillment', 'Building computer vision systems, ML and AI models, robotic control and motion planning, process management', 'End-to-end ownership of decision explanation, fault detection, monitoring, A/B testing, large scale model training, simulation, hardware integration']\n",
            "['Credit scoring models']\n",
            "['Large-scale distributed software applications, systems, services']\n",
            "['AI-driven education platform']\n",
            "['Financial Services']\n",
            "['Financial Services']\n",
            "['Quantitative trading team', 'ML based data pipelines', 'Analytics for research', 'Systematic trading strategies']\n",
            "0\n",
            "['Global markets proprietary trading firm']\n",
            "['Remotely-operated endovascular surgical robot']\n",
            "['Bioinformatics pipelines using Nextflow', 'Python/R scripts for data processing', 'Reporting system for clinic-ready reports', 'Documentation for pipeline management', 'Web-based user interfaces']\n",
            "['Financial Planning solutions']\n",
            "['Generative AI and NLP']\n",
            "['ETL Automation', 'Front-End Dashboarding', 'Documentation']\n",
            "['Low Latency Trading Systems']\n",
            "['Advanced medical device']\n",
            "['Augmented Reality Systems Platform']\n",
            "['SAP Analytics Cloud (SAC)', 'Financial Planning processes']\n",
            "['CPUs', 'System on Chips', 'machine learning', 'data centers', 'high-performance computing applications']\n",
            "['Interdisciplinary Artificial Intelligence Research']\n",
            "0\n",
            "['Security risk assessment']\n",
            "['Qualcomm Neural Processing SDK']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Material handling equipment and systems']\n",
            "0\n",
            "['Machine Learning', 'X-ray Imaging']\n",
            "['Firmware development for hardware media pipeline', 'Digital Rights Management', 'Trusted Execution Environment', 'Audio/Video formats and containers']\n",
            "['Financial Services']\n",
            "0\n",
            "['Advancing understanding of radio spectrum and wireless signals']\n",
            "['Automation test frameworks - Selenium, Appium, Playwright', 'iOS XCTest, XCUITest', 'Android Espresso, UI Automator', 'Java, JavaScript']\n",
            "['Teaching students to read and comprehend']\n",
            "['CAD/EDA Tools Software Engineer for Test Chip Design']\n",
            "['Embedded software for Bobcat equipment']\n",
            "['Quantitative Developer for systematic corporate bond and credit derivatives strategies']\n",
            "['Machine Learning ASICs for Data Center servers']\n",
            "0\n",
            "['Google Cloud Platform', 'cutting-edge technology', 'cleanest cloud in the industry']\n",
            "0\n",
            "['Marine Engineering Networking', 'C/Networking Software']\n",
            "['consumer electronics']\n",
            "['Financial Services']\n",
            "['Mission capability integrator']\n",
            "['AI Video Platform']\n",
            "['User-owned talent network', 'Connects professionals with enterprises', 'Eliminates middlemen and markups', 'Efficient and quality matching']\n",
            "['Triage Software Engineer - Initial defect analysis - Dashboard creation - Problem-solving - Communication skills - Automotive Infotainment - Embedded software - JIRA/Confluence - Agile/Scrum - Requirement Analysis']\n",
            "['Small satellite industry solutions']\n",
            "0\n",
            "['Scientific Software Engineer - Space', 'Join a company at the heart of space research and operations', 'Create solutions for complex problems', 'Ensure data and products are of highest quality for aerospace community']\n",
            "['Cybersecurity']\n",
            "['AI/Client models']\n",
            "['Academic support through tutoring']\n",
            "0\n",
            "['K9 gear']\n",
            "['Boston Public Market']\n",
            "['AI Research']\n",
            "['Artificial Intelligence']\n",
            "['AI research']\n",
            "['AI solutions for educational experience enhancement']\n",
            "['Training devices for military aircraft']\n",
            "['Data & AI solutions']\n",
            "['seismic-acoustic signal processing']\n",
            "0\n",
            "['Jury consulting services']\n",
            "['Cyber Security services', 'cutting-edge technologies']\n",
            "['Data Engineering for Wholesale Building Materials']\n",
            "['Software Development']\n",
            "['AR/VR development']\n",
            "['AI Microsoft Chatbot Developer']\n",
            "['Creative studio', 'Lifestyle consultancy', 'Branding', 'Marketing', 'Design', 'Web & app development', 'Business leadership', 'Revenue & scale strategies']\n",
            "['Transformer products and solutions']\n",
            "['Cybersecurity Data Services']\n",
            "['Data Science Platform empowers data scientists to build machine learning systems']\n",
            "['Machine Learning Platform']\n",
            "['High-performance data analytics platform handling petabytes of data']\n",
            "['Real Time ML Service', 'RTML Model Serving Framework', 'RTML Framework', 'AI Center', 'AI-driven solutions']\n",
            "['Pre-owned vehicles', 'Financing', 'Warranties', 'Vehicle appraisals']\n",
            "['Autonomous driving software']\n",
            "['Quantitative Software Engineer']\n",
            "['Transportation services']\n",
            "['Clinical Trial Research']\n",
            "['AI for Autonomy Lab researches applying AI-related technologies for autonomy systems']\n",
            "['Embedded Cybersecurity solutions for Caterpillar machines & engines']\n",
            "['AI research and deployment']\n",
            "['Microservices development']\n",
            "['Electric vehicles']\n",
            "['AI platform at Together AI', 'Research-driven artificial intelligence company', 'Contribute to leading open-source research, models, and datasets']\n",
            "['Building open source digital systems and solutions to battle environmental threats', 'Developing open innovative technology to increase planetary resilience', 'Creating software to track progress towards Paris Agreement goals', 'Improving data infrastructure for climate analysis using AI', 'Maximizing impact through Open Source projects']\n",
            "['Software development for finance industry']\n",
            "['Support workflow development for NOAA SFS', 'NWP modeling system', 'Forecast guidance', 'Earth system models']\n",
            "['Software solutions for critical infrastructure']\n",
            "['AI algorithms', 'software applications']\n",
            "['enterprise B2B SaaS solutions']\n",
            "['Quantum control solution (QCS)', 'Quantum computing', 'Quantum communications', 'Quantum sensing']\n",
            "['Quantum Systems']\n",
            "['Quantum computing system', 'Azure Quantum', 'Revolutionize computing']\n",
            "['Quantum Computing and Devices']\n",
            "['Cutting-edge optical design solutions']\n",
            "['Introduction of automation to construction sector']\n",
            "['Software engineering opportunity at JPMorgan Chase']\n",
            "['Ads Creative Management']\n",
            "['Algorithmic trading programs', 'Forex trading systems']\n",
            "[\"Kernel software development for Boeing's product portfolio\"]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that for hyphenated word groupings, at least some of them are legitimate individual words, for example, 'X-ray'. Thus we will hold off on applying a lambda function anywhere in the dataframe that replaces '-' with ' ' until we can verify whether or not the hyphenated word grouping is considered a word in our word embedding model."
      ],
      "metadata": {
        "id": "L2UDMaibqKhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['industries']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La9gzQO_rAnY",
        "outputId": "e8dab94f-0680-4983-8344-08c7be6ace13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Business Consulting', 'Services']\n",
            "['Biotechnology Research', 'Pharmaceutical Manufacturing']\n",
            "['Software Development', 'IT Services', 'IT Consulting', 'Technology', 'Information', 'Internet']\n",
            "['Financial Services', 'Capital Markets', 'IT Services', 'IT Consulting']\n",
            "['IT Services', 'IT Consulting']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Financial Services']\n",
            "['Financial Services']\n",
            "['Financial Services', 'Technology', 'Information and Internet', 'Software Development']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Staffing', 'Recruiting']\n",
            "['Robotics Engineering', 'Software Development', 'Medical Equipment Manufacturing']\n",
            "['Hospitals and Health Care']\n",
            "['Food and Beverage Services']\n",
            "['Outsourcing and Offshoring Consulting', 'Business Consulting and Services', 'IT Services and IT Consulting']\n",
            "['Advertising Services', 'OEM/Automotive']\n",
            "['Banking', 'Software Development', 'Financial Services']\n",
            "['Software Development']\n",
            "['Technology', 'Information', 'Internet']\n",
            "['Manufacturing', 'Food and Beverage Manufacturing', 'Food and Beverage Services']\n",
            "['Computer Hardware Manufacturing', 'Software Development', 'Computers and Electronics Manufacturing']\n",
            "['Higher Education and Research Services']\n",
            "['Hospitals and Health Care', 'Non-profit Organizations', 'Education Administration Programs']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Telecommunications']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Industrial Automation']\n",
            "['Manufacturing']\n",
            "['Research Services', 'Higher Education']\n",
            "['Software Development']\n",
            "['Financial Services']\n",
            "['Software Development']\n",
            "['Broadcast Media Production and Distribution']\n",
            "['Financial Services']\n",
            "['Primary and Secondary Education', 'E-Learning Providers']\n",
            "['Semiconductor Manufacturing']\n",
            "['Machinery Manufacturing', 'Manufacturing', 'Construction']\n",
            "['Investment Management']\n",
            "['Technology', 'Information', 'Internet']\n",
            "['Software Development']\n",
            "['Information Services and Technology', 'Information and Internet']\n",
            "['Hospitals and Health Care']\n",
            "['Consumer Electronics']\n",
            "['Software Development', 'IT Services', 'IT Consulting', 'Technology', 'Information', 'Internet']\n",
            "['Financial Services']\n",
            "['Civil Engineering']\n",
            "['Software Development']\n",
            "['Technology', 'Information', 'Internet']\n",
            "['Technology', 'Information and Media', 'Information Services', 'Software Development']\n",
            "['Defense', 'Space Manufacturing']\n",
            "['Technology', 'Information', 'Internet']\n",
            "['Aviation and Aerospace Component Manufacturing', 'Research Services', 'Defense and Space Manufacturing']\n",
            "['Software Development']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Hospitals and Health Care']\n",
            "['Business Consulting', 'Services', 'Automation Machinery Manufacturing']\n",
            "['Retail']\n",
            "['Non-profit Organizations']\n",
            "['Technology', 'Information', 'Internet']\n",
            "['Semiconductor Manufacturing']\n",
            "['Research Services']\n",
            "['Higher Education']\n",
            "['Airlines and Aviation', 'Aviation and Aerospace Component Manufacturing', 'Defense and Space Manufacturing']\n",
            "['Computer Hardware Manufacturing']\n",
            "['Defense and Space Manufacturing', 'Engineering Services', 'Software Development']\n",
            "['Software Development']\n",
            "['Research Services']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Wholesale Building Materials']\n",
            "['Software Development']\n",
            "['IT Services', 'IT Consulting', 'Technology', 'Information', 'Media']\n",
            "['Software Development']\n",
            "['Marketing Services', 'Media', 'Technology', 'Hospitality', 'Food & beverage', 'Retail', 'Real estate']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Computer and Network Security']\n",
            "['Advertising Services']\n",
            "['IT Services', 'IT Consulting', 'Financial Services', 'Banking']\n",
            "['Technology', 'Information and Media']\n",
            "['Telecommunications']\n",
            "['Software Development']\n",
            "['Software Development', 'Motor Vehicle Manufacturing']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Transportation/Trucking/Railroad']\n",
            "['Research']\n",
            "['Defense and Space Manufacturing', 'Higher Education', 'Software Development']\n",
            "['Construction, Machinery Manufacturing']\n",
            "['Research Services']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Motor Vehicle Manufacturing']\n",
            "['Software Development']\n",
            "['Environmental Services']\n",
            "['Technology', 'Information', 'Internet', 'Financial Services', 'Software Development']\n",
            "['Computer Hardware Manufacturing', 'Defense and Space Manufacturing', 'IT Services and IT Consulting']\n",
            "['Computer and Network Security']\n",
            "['IT Services', 'IT Consulting']\n",
            "['Financial Services']\n",
            "['Appliances', 'Electrical', 'Electronics Manufacturing']\n",
            "['Research Services']\n",
            "['Software Development']\n",
            "['Computer Hardware Manufacturing']\n",
            "['Semiconductor Manufacturing', 'Software Development', 'Computer Hardware Manufacturing']\n",
            "['Technology', 'Information and Media']\n",
            "['Financial Services']\n",
            "['Software Development']\n",
            "['Financial Services', 'Investment Banking', 'Software Development']\n",
            "['Airlines and Aviation', 'Aviation and Aerospace Component Manufacturing', 'Defense and Space Manufacturing']\n",
            "['Government Administration']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['position_name']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBKPGXy0rIeJ",
        "outputId": "497f7f5d-6fe6-41aa-bb39-6b2af0041e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python Software Engineer (Robotics/Mechatronics)']\n",
            "['Robotics Engineer']\n",
            "['SDE - Amazon Robotics']\n",
            "['Senior Software Engineer']\n",
            "['Software Engineer in Test']\n",
            "['AI Prompt Engineer']\n",
            "['Lead Software Engineer - Python']\n",
            "['Software Engineer III (Python/ML)']\n",
            "['Machine Learning Engineer']\n",
            "['Python Developer']\n",
            "['C++ Quantitative Developer']\n",
            "['Senior Machine Learning Engineer']\n",
            "['Bioinformatics Software Engineer']\n",
            "['Tech Functional Architect, FP&A']\n",
            "['Generative AI Engineer']\n",
            "['Analytics Engineer']\n",
            "['C++ Low Latency Trading Systems Developer']\n",
            "['Embedded Software Engineer']\n",
            "['AR Systems Integration Lead']\n",
            "['Tech Functional Architect, FP&A']\n",
            "['Principal System Software Architect']\n",
            "['Assistant/Associate/Full Professor']\n",
            "['Accountant']\n",
            "['Cyber and RMF Specialist, Mid']\n",
            "['Staff AI Software Build and Release Engineer']\n",
            "['Sales Coordinator']\n",
            "['Sales Application Engineer - Automation']\n",
            "['Staff Accountant']\n",
            "['Research Associate']\n",
            "['Firmware Engineer']\n",
            "['Software Engineering']\n",
            "['Embedded Engineer']\n",
            "['Machine Learning Engineer']\n",
            "['Software Engineer in Test']\n",
            "['Tutor']\n",
            "['Software Engineer']\n",
            "['Embedded Software Engineer']\n",
            "['Quantitative Developer']\n",
            "['Machine Learning SoC Architect']\n",
            "['Generative AI Engineer']\n",
            "['Database Acceleration Specialist']\n",
            "['GCP Data Engineer']\n",
            "['C/Networking Software Engineer']\n",
            "['Software Development Engineer - Test']\n",
            "['Lead Software Engineer']\n",
            "['Senior Level Resiliency Systems Engineer']\n",
            "['Senior Software Engineer']\n",
            "['Go Expert - AI Training']\n",
            "['Triage Software Engineer']\n",
            "['Aerospace Software Engineer']\n",
            "['Machine Learning Engineer']\n",
            "['Scientific Software Engineer - Space']\n",
            "['Rust Engineer', 'C++ Engineer']\n",
            "['AI Engineer']\n",
            "['Peer Tutor']\n",
            "['Innovation Manager (Marketing Technologist) - Cookie Management']\n",
            "['SEO & Content Marketing Specialist']\n",
            "['Assistant Manager']\n",
            "['Postdoctoral Research Scientist, Artificial Intelligence (PhD)']\n",
            "['Technical Sales Specialist']\n",
            "['AI Research Scientist', 'AI Research Engineer']\n",
            "['Software Engineer']\n",
            "['Rotorcraft Vehicle Simulation Software Engineer']\n",
            "['Technology Sales Engineer']\n",
            "['Algorithm Engineer']\n",
            "['Machine Learning Consultant']\n",
            "['Research Assistant']\n",
            "['System Engineer']\n",
            "['Data Engineer']\n",
            "['Gen AI Engineer']\n",
            "['AR/VR Developer']\n",
            "['AI Microsoft Chatbot Developer']\n",
            "['Search Engine Marketing Specialist']\n",
            "['Technical Sales Manager']\n",
            "['Sales Engineer']\n",
            "['Lead ML Ops Engineer']\n",
            "['ML Ops Engineer']\n",
            "['ML/ML Ops Engineer']\n",
            "['RTML Engineer', 'ML Ops Engineer']\n",
            "['Python Developer']\n",
            "['Machine Learning Systems Engineer']\n",
            "['Quantitative Software Engineer']\n",
            "['Driver', 'Bus Driver', 'CDL Driver', 'Non CDL Driver', 'Dispatcher']\n",
            "['Research Assistant']\n",
            "['Machine Learning Engineer']\n",
            "['Embedded Cybersecurity Software Engineer']\n",
            "['Senior Software Engineer, Front End']\n",
            "['Python Developer']\n",
            "['Embedded Software Engineer II']\n",
            "['Systems Research Engineer, Machine Learning Systems']\n",
            "['AI Application Engineer']\n",
            "['Software Engineer']\n",
            "['Scientific Application Programmer']\n",
            "['Product Security Test Engineer']\n",
            "['AI Software Engineer']\n",
            "['Distinguished Engineer']\n",
            "['Quantum Solution Engineer']\n",
            "['Summer Internship - Quantum Systems']\n",
            "['Research Intern - Quantum Information and Computation']\n",
            "['Research Scientist']\n",
            "['Software Architect - C++']\n",
            "['Machine Learning Engineer']\n",
            "['Software Engineer II (Quant/Python)']\n",
            "['Staff Software Engineer']\n",
            "['Algorithm Developer']\n",
            "['Associate Software Engineer - Kernel Developer']\n",
            "['Machine Learning Engineer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see parentheses around 'PhD'. We will most likely want to remove parentheses throughout the whole dataframe."
      ],
      "metadata": {
        "id": "yZVYJjENrbOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove '(' and ')' from each item in lists\n",
        "df['position_name'] = df['position_name'].apply(lambda x: [item.replace('(', '').replace(')', '') for item in x])"
      ],
      "metadata": {
        "id": "ko9cP3Vfrwqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['position_name']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQfZdnZjsDN_",
        "outputId": "7ff39710-5d0f-44ed-ae30-9f932d42c8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python Software Engineer Robotics/Mechatronics']\n",
            "['Robotics Engineer']\n",
            "['SDE - Amazon Robotics']\n",
            "['Senior Software Engineer']\n",
            "['Software Engineer in Test']\n",
            "['AI Prompt Engineer']\n",
            "['Lead Software Engineer - Python']\n",
            "['Software Engineer III Python/ML']\n",
            "['Machine Learning Engineer']\n",
            "['Python Developer']\n",
            "['C++ Quantitative Developer']\n",
            "['Senior Machine Learning Engineer']\n",
            "['Bioinformatics Software Engineer']\n",
            "['Tech Functional Architect, FP&A']\n",
            "['Generative AI Engineer']\n",
            "['Analytics Engineer']\n",
            "['C++ Low Latency Trading Systems Developer']\n",
            "['Embedded Software Engineer']\n",
            "['AR Systems Integration Lead']\n",
            "['Tech Functional Architect, FP&A']\n",
            "['Principal System Software Architect']\n",
            "['Assistant/Associate/Full Professor']\n",
            "['Accountant']\n",
            "['Cyber and RMF Specialist, Mid']\n",
            "['Staff AI Software Build and Release Engineer']\n",
            "['Sales Coordinator']\n",
            "['Sales Application Engineer - Automation']\n",
            "['Staff Accountant']\n",
            "['Research Associate']\n",
            "['Firmware Engineer']\n",
            "['Software Engineering']\n",
            "['Embedded Engineer']\n",
            "['Machine Learning Engineer']\n",
            "['Software Engineer in Test']\n",
            "['Tutor']\n",
            "['Software Engineer']\n",
            "['Embedded Software Engineer']\n",
            "['Quantitative Developer']\n",
            "['Machine Learning SoC Architect']\n",
            "['Generative AI Engineer']\n",
            "['Database Acceleration Specialist']\n",
            "['GCP Data Engineer']\n",
            "['C/Networking Software Engineer']\n",
            "['Software Development Engineer - Test']\n",
            "['Lead Software Engineer']\n",
            "['Senior Level Resiliency Systems Engineer']\n",
            "['Senior Software Engineer']\n",
            "['Go Expert - AI Training']\n",
            "['Triage Software Engineer']\n",
            "['Aerospace Software Engineer']\n",
            "['Machine Learning Engineer']\n",
            "['Scientific Software Engineer - Space']\n",
            "['Rust Engineer', 'C++ Engineer']\n",
            "['AI Engineer']\n",
            "['Peer Tutor']\n",
            "['Innovation Manager Marketing Technologist - Cookie Management']\n",
            "['SEO & Content Marketing Specialist']\n",
            "['Assistant Manager']\n",
            "['Postdoctoral Research Scientist, Artificial Intelligence PhD']\n",
            "['Technical Sales Specialist']\n",
            "['AI Research Scientist', 'AI Research Engineer']\n",
            "['Software Engineer']\n",
            "['Rotorcraft Vehicle Simulation Software Engineer']\n",
            "['Technology Sales Engineer']\n",
            "['Algorithm Engineer']\n",
            "['Machine Learning Consultant']\n",
            "['Research Assistant']\n",
            "['System Engineer']\n",
            "['Data Engineer']\n",
            "['Gen AI Engineer']\n",
            "['AR/VR Developer']\n",
            "['AI Microsoft Chatbot Developer']\n",
            "['Search Engine Marketing Specialist']\n",
            "['Technical Sales Manager']\n",
            "['Sales Engineer']\n",
            "['Lead ML Ops Engineer']\n",
            "['ML Ops Engineer']\n",
            "['ML/ML Ops Engineer']\n",
            "['RTML Engineer', 'ML Ops Engineer']\n",
            "['Python Developer']\n",
            "['Machine Learning Systems Engineer']\n",
            "['Quantitative Software Engineer']\n",
            "['Driver', 'Bus Driver', 'CDL Driver', 'Non CDL Driver', 'Dispatcher']\n",
            "['Research Assistant']\n",
            "['Machine Learning Engineer']\n",
            "['Embedded Cybersecurity Software Engineer']\n",
            "['Senior Software Engineer, Front End']\n",
            "['Python Developer']\n",
            "['Embedded Software Engineer II']\n",
            "['Systems Research Engineer, Machine Learning Systems']\n",
            "['AI Application Engineer']\n",
            "['Software Engineer']\n",
            "['Scientific Application Programmer']\n",
            "['Product Security Test Engineer']\n",
            "['AI Software Engineer']\n",
            "['Distinguished Engineer']\n",
            "['Quantum Solution Engineer']\n",
            "['Summer Internship - Quantum Systems']\n",
            "['Research Intern - Quantum Information and Computation']\n",
            "['Research Scientist']\n",
            "['Software Architect - C++']\n",
            "['Machine Learning Engineer']\n",
            "['Software Engineer II Quant/Python']\n",
            "['Staff Software Engineer']\n",
            "['Algorithm Developer']\n",
            "['Associate Software Engineer - Kernel Developer']\n",
            "['Machine Learning Engineer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['broader_role_name']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFQusuvpsNpk",
        "outputId": "db5e486a-d030-4ffa-8971-27e632758b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['N/A']\n",
            "['AI Engineer']\n",
            "['Software Engineer']\n",
            "['Software Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Tech Product Manager']\n",
            "['N/A']\n",
            "['Analytics & Quality Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['FP&A Tech Workstream Lead']\n",
            "['Director']\n",
            "['MizzouForward']\n",
            "['N/A']\n",
            "['Information security risk specialist']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Sales Application Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Clinician']\n",
            "['CAD/EDA Tools Software Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Data Scientist', 'AI-ML Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['Systems Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['Engineering']\n",
            "['N/A']\n",
            "['Engineering']\n",
            "['Engineer']\n",
            "['Engineer']\n",
            "['N/A']\n",
            "['Innovation Manager']\n",
            "['N/A']\n",
            "['Manager On Duty']\n",
            "['Research Scientist']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['AI Engineer']\n",
            "['Software Engineer']\n",
            "['Brand Technical Specialist']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Operational Support Engineer']\n",
            "['N/A']\n",
            "['Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Marketing Analyst']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Data Science Platform Team']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Principle Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['Engineering']\n",
            "['N/A']\n",
            "['Software Engineer II - Field Support']\n",
            "['Systems Research Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Engineering Solutions']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Architect']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['Software Engineer']\n",
            "['N/A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['company']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V36aurRsWii",
        "outputId": "dc60bad5-7676-4453-ed23-281f0cd65679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fresh Consulting']\n",
            "['Barrington James']\n",
            "['Amazon']\n",
            "['VantageScore®']\n",
            "['Optomi']\n",
            "['Crossover']\n",
            "['JPMorgan Chase & Co.']\n",
            "['JPMorgan Chase & Co.']\n",
            "['maven']\n",
            "['REI Systems']\n",
            "['Jobot']\n",
            "['Remedy Robotics']\n",
            "['Insight Global']\n",
            "['Ocean Spray Cranberries']\n",
            "['Capgemini']\n",
            "['Harnham']\n",
            "['Huxley']\n",
            "['Oxenham Group']\n",
            "['Meta']\n",
            "['MSH']\n",
            "['NVIDIA']\n",
            "['University of Missouri-Columbia']\n",
            "['Altarum']\n",
            "['Booz Allen Hamilton']\n",
            "['Qualcomm']\n",
            "['Generac Power Systems']\n",
            "['Conveyor Solutions, Inc.']\n",
            "['Bespoke Beauty Brands']\n",
            "['SLAC National Accelerator Laboratory']\n",
            "['Dice']\n",
            "['JPMorgan Chase & Co.']\n",
            "['Dice']\n",
            "['Harnham']\n",
            "['Photon']\n",
            "['Lindamood-Bell Learning Processes']\n",
            "['Intel Corporation']\n",
            "['Bobcat Company']\n",
            "['ASB Resources']\n",
            "['Meta']\n",
            "['Dice']\n",
            "['Google']\n",
            "['Fractal']\n",
            "['Garmin']\n",
            "['Amazon Lab126']\n",
            "['JPMorgan Chase & Co.']\n",
            "['Peraton']\n",
            "['Oho Group Ltd']\n",
            "['Braintrust']\n",
            "['CoreTek Labs']\n",
            "['EVONA']\n",
            "['HireIO, Inc.']\n",
            "['EVONA']\n",
            "['Dice']\n",
            "['Diverse Lynx']\n",
            "['Oregon Health & Science University']\n",
            "['MindSource']\n",
            "['Ray Allen Manufacturing']\n",
            "['Boston Public Market Association']\n",
            "['Meta']\n",
            "['Intel Corporation']\n",
            "['LG AI Research']\n",
            "['Catholic Institute of Technology']\n",
            "['Boeing']\n",
            "['IBM']\n",
            "['Insight Global']\n",
            "['Dice']\n",
            "['Dana Meeks Consulting']\n",
            "['Sealing Technologies, a Parsons Company']\n",
            "['Pella Corporation']\n",
            "['Dice']\n",
            "['The Mice Groups, Inc.']\n",
            "['Dice']\n",
            "['The Madison Melle Agency']\n",
            "['HICO America']\n",
            "['alphaMountain.ai']\n",
            "['Klaviyo']\n",
            "['Apexon']\n",
            "['Grid Dynamics']\n",
            "['InfoVision Inc.']\n",
            "['hackajob']\n",
            "['Ghost Autonomy']\n",
            "['Raft']\n",
            "['MASS TRANSPORTATION SERVICES, INC']\n",
            "['The US Oncology Network']\n",
            "['Software Engineering Institute | Carnegie Mellon University']\n",
            "['Caterpillar Inc.']\n",
            "['OpenAI']\n",
            "['TEKsystems']\n",
            "['Motiv Power Systems']\n",
            "['Together AI']\n",
            "['Open Earth Foundation']\n",
            "['maven']\n",
            "['SAIC']\n",
            "['Verve Industrial, A Rockwell Automation Company']\n",
            "['Zoom']\n",
            "['Capital One']\n",
            "['Keysight Technologies']\n",
            "['QuEra Computing Inc.']\n",
            "['Microsoft']\n",
            "['IBM']\n",
            "['Synopsys Inc']\n",
            "['Harnham']\n",
            "['JPMorgan Chase & Co.']\n",
            "['Reddit, Inc.']\n",
            "['Nurp']\n",
            "['Boeing']\n",
            "['Blu Omega']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['location']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Hx0goRsgGQ",
        "outputId": "0cab396e-22f8-4050-c8a5-4c3e9c6f9ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Redmond, WA']\n",
            "['New York, United States']\n",
            "['North Reading, MA']\n",
            "['San Francisco Bay Area']\n",
            "['Dallas-Fort Worth Metroplex']\n",
            "['Evansville, IN']\n",
            "['Houston, TX']\n",
            "['Jersey City, NJ']\n",
            "['Greater Chicago Area']\n",
            "['Sterling, VA']\n",
            "['Indianapolis, IN']\n",
            "['San Francisco Bay Area']\n",
            "['Memphis Metropolitan Area']\n",
            "['Lakeville, MA']\n",
            "['San Francisco Bay Area']\n",
            "['United States']\n",
            "['Chicago, IL']\n",
            "['Boston, MA']\n",
            "['Redmond, WA']\n",
            "['Greater Boston']\n",
            "['Boulder, CO']\n",
            "['Columbia, MO']\n",
            "['Ann Arbor, MI']\n",
            "['Lexington Park, MD']\n",
            "['San Diego, CA']\n",
            "['Pewaukee, WI']\n",
            "['Dallas-Fort Worth Metroplex']\n",
            "['Irving, TX']\n",
            "['Menlo Park, CA']\n",
            "['Dallas, TX']\n",
            "['Jersey City, NJ']\n",
            "['Bellevue, WA']\n",
            "['New York, NY']\n",
            "['Alpharetta, GA']\n",
            "['Palm Beach County, FL']\n",
            "['Hillsboro, OR']\n",
            "['Bismarck, ND']\n",
            "['Manhattan, NY']\n",
            "['Austin, TX']\n",
            "['Plano, TX']\n",
            "['Los Angeles, CA']\n",
            "['Bentonville, AR']\n",
            "['Cary, NC']\n",
            "['Sunnyvale, CA']\n",
            "['Brooklyn, NY']\n",
            "['Chantilly, VA']\n",
            "['Sunnyvale, CA']\n",
            "['Texas, United States']\n",
            "['Dallas, TX']\n",
            "['Colorado, United States']\n",
            "['San Francisco, CA']\n",
            "['Colorado, United States']\n",
            "['King of Prussia, PA']\n",
            "['San Jose, CA']\n",
            "['Portland, OR']\n",
            "['California, United States']\n",
            "['Colorado Springs, CO']\n",
            "['Boston, MA']\n",
            "['Pittsburgh, PA']\n",
            "['Hillsboro, OR']\n",
            "['Ann Arbor, MI']\n",
            "['United States']\n",
            "['Hazelwood, MO']\n",
            "['Indianapolis, IN']\n",
            "['United States']\n",
            "['United States']\n",
            "['Oakland, CA']\n",
            "['Columbia, MD']\n",
            "['Pella, IA']\n",
            "['Atlanta, GA']\n",
            "['United States']\n",
            "['United States']\n",
            "['Los Angeles, CA']\n",
            "['Pittsburgh, PA']\n",
            "['Lehi, UT']\n",
            "['Boston, MA']\n",
            "['Berkeley Heights, NJ']\n",
            "['United States']\n",
            "['Irving, TX', 'Dallas, TX', 'NJ']\n",
            "['United States']\n",
            "['Mountain View, CA']\n",
            "['San Antonio, TX']\n",
            "['East Orange, NJ']\n",
            "['Country Club, CA']\n",
            "['Pittsburgh, PA']\n",
            "['Chillicothe, IL']\n",
            "['San Francisco, CA']\n",
            "['Milwaukee, WI']\n",
            "['Foster City, CA']\n",
            "['San Francisco Bay Area (Hybrid)']\n",
            "['United States']\n",
            "['New York, NY']\n",
            "['College Park, MD']\n",
            "['United States']\n",
            "['California, United States']\n",
            "['Cambridge, MA']\n",
            "['Santa Rosa, CA']\n",
            "['Boston, MA']\n",
            "['Redmond, WA']\n",
            "['Yorktown Heights, NY']\n",
            "['Irvine, CA']\n",
            "['Chicago, IL']\n",
            "['New York, United States']\n",
            "['United States']\n",
            "['United States']\n",
            "['Mesa, AZ']\n",
            "['United States']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of something we might want to ask ChatGPT to do for us, at least in future posting summarization; it'd be nice if it could identify the city, state, and country for us so we don't need to write tricky code to do this feature extraction. We could try doing a for loop where we pass each list value to ChatGPT and have it spit out JSON that we can then use to create new columns for city, state, and country in our dataframe and fill them row by row."
      ],
      "metadata": {
        "id": "GtdM12E-tIn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this first we will design the prompt."
      ],
      "metadata": {
        "id": "aafVJuC_vMxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Take the following python list and create a JSON dictionary containing 'city', 'state', and 'country' as keys and fill out the values. If the list is ['Colorado Springs, CO'] you would return {\"city\": \"Colorado Springs\", \"state\": \"CO\", \"country\": \"USA\"}. Return only the JSON dictionary. I want you to do it, not to tell me how to code it. I want you to do it for:\n",
        "{list}\""
      ],
      "metadata": {
        "id": "bGuUdwiKwfMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, value in enumerate(df['location']):\n",
        "  generate_location_json_file(index, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxklLpdybqa",
        "outputId": "bf564f08-2230-493b-dea5-5dc28f6d1671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"city\": \"Redmond\", \"state\": \"WA\", \"country\": \"USA\"}\n",
            "{\"city\": \"New York\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"North Reading\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Dallas-Fort Worth\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"Evansville\", \"state\": \"IN\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"Houston\",\n",
            "  \"state\": \"TX\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Jersey City\", \"state\": \"NJ\", \"country\": \"USA\"}\n",
            "{\"city\": \"Chicago\", \"state\": \"IL\", \"country\": \"USA\"}\n",
            "{\"city\": \"Sterling\", \"state\": \"VA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Indianapolis\", \"state\": \"IN\", \"country\": \"USA\"}\n",
            "{\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Memphis Metropolitan Area\", \"state\": \"\", \"country\": \"\"}\n",
            "{\"city\": \"Lakeville\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"Chicago\", \"state\": \"IL\", \"country\": \"USA\"}\n",
            "{\"city\": \"Boston\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Redmond\", \"state\": \"WA\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"Greater Boston\",\n",
            "  \"state\": null,\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Boulder\", \"state\": \"CO\", \"country\": \"USA\"}\n",
            "{\"city\": \"Columbia\", \"state\": \"MO\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"Ann Arbor\",\n",
            "  \"state\": \"MI\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\n",
            "  \"city\": \"Lexington Park\",\n",
            "  \"state\": \"MD\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"San Diego\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Pewaukee\", \"state\": \"WI\", \"country\": \"USA\"}\n",
            "{\"city\": \"Dallas-Fort Worth\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\n",
            "\"city\": \"Irving\",\n",
            "\"state\": \"TX\",\n",
            "\"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Menlo Park\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Dallas\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"Jersey City\", \"state\": \"NJ\", \"country\": \"USA\"}\n",
            "{\"city\": \"Bellevue\", \"state\": \"WA\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"New York\",\n",
            "  \"state\": \"NY\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Alpharetta\", \"state\": \"GA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Palm Beach\", \"state\": \"FL\", \"country\": \"USA\"}\n",
            "{\"city\": \"Hillsboro\", \"state\": \"OR\", \"country\": \"USA\"}\n",
            "{\"city\": \"Bismarck\", \"state\": \"ND\", \"country\": \"USA\"}\n",
            "{\"city\": \"Manhattan\", \"state\": \"NY\", \"country\": \"USA\"}\n",
            "{\"city\": \"Austin\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"Plano\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"Los Angeles\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Bentonville\", \"state\": \"AR\", \"country\": \"USA\"}\n",
            "{\"city\": \"Cary\", \"state\": \"NC\", \"country\": \"USA\"}\n",
            "{\"city\": \"Sunnyvale\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Brooklyn\", \"state\": \"NY\", \"country\": \"USA\"}\n",
            "{\"city\": \"Chantilly\", \"state\": \"VA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Sunnyvale\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Texas\", \"state\": \"TX\", \"country\": \"United States\"}\n",
            "{\"city\": \"Dallas\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"Colorado\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Colorado\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\n",
            "  \"city\": \"King of Prussia\",\n",
            "  \"state\": \"PA\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"San Jose\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Portland\", \"state\": \"OR\", \"country\": \"USA\"}\n",
            "{\"city\": \"California\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"Colorado Springs\", \"state\": \"CO\", \"country\": \"USA\"}\n",
            "{\n",
            "\"city\": \"Boston\",\n",
            "\"state\": \"MA\",\n",
            "\"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Pittsburgh\", \"state\": \"PA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Hillsboro\", \"state\": \"OR\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"Ann Arbor\",\n",
            "  \"state\": \"MI\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"Hazelwood\", \"state\": \"MO\", \"country\": \"USA\"}\n",
            "{\"city\": \"Indianapolis\", \"state\": \"IN\", \"country\": \"USA\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\n",
            "  \"city\": \"Oakland\",\n",
            "  \"state\": \"CA\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Columbia\", \"state\": \"MD\", \"country\": \"USA\"}\n",
            "{\"city\": \"Pella\", \"state\": \"IA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Atlanta\", \"state\": \"GA\", \"country\": \"USA\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"USA\"}\n",
            "{\"city\": \"Los Angeles\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Pittsburgh\", \"state\": \"PA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Lehi\", \"state\": \"UT\", \"country\": \"USA\"}\n",
            "{\"city\": \"Boston\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\n",
            "  \"city\": \"Berkeley Heights\",\n",
            "  \"state\": \"NJ\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\n",
            "  \"1\": {\n",
            "    \"city\": \"Irving\",\n",
            "    \"state\": \"TX\",\n",
            "    \"country\": \"USA\"\n",
            "  },\n",
            "  \"2\": {\n",
            "    \"city\": \"Dallas\",\n",
            "    \"state\": \"TX\",\n",
            "    \"country\": \"USA\"\n",
            "  },\n",
            "  \"3\": {\n",
            "    \"city\": \"NJ\",\n",
            "    \"state\": \"\",\n",
            "    \"country\": \"USA\"\n",
            "  }\n",
            "}\n",
            "{\"city\": \"N/A\", \"state\": \"N/A\", \"country\": \"United States\"}\n",
            "{\n",
            "    \"city\": \"Mountain View\",\n",
            "    \"state\": \"CA\",\n",
            "    \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"San Antonio\", \"state\": \"TX\", \"country\": \"USA\"}\n",
            "{\"city\": \"East Orange\", \"state\": \"NJ\", \"country\": \"USA\"}\n",
            "{\n",
            "    \"city\": \"Country Club\",\n",
            "    \"state\": \"CA\",\n",
            "    \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"Pittsburgh\", \"state\": \"PA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Chillicothe\", \"state\": \"IL\", \"country\": \"USA\"}\n",
            "{\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Milwaukee\", \"state\": \"WI\", \"country\": \"USA\"}\n",
            "{\"city\": \"Foster City\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\n",
            "    \"city\": \"San Francisco\",\n",
            "    \"state\": \"CA\",\n",
            "    \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\n",
            "    \"city\": \"New York\",\n",
            "    \"state\": \"NY\",\n",
            "    \"country\": \"USA\"\n",
            "}\n",
            "{\n",
            "  \"city\": \"College Park\",\n",
            "  \"state\": \"MD\",\n",
            "  \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"California\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Cambridge\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Santa Rosa\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Boston\", \"state\": \"MA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Redmond\", \"state\": \"WA\", \"country\": \"USA\"}\n",
            "{\"city\": \"Yorktown Heights\", \"state\": \"NY\", \"country\": \"USA\"}\n",
            "{\"city\": \"Irvine\", \"state\": \"CA\", \"country\": \"USA\"}\n",
            "{\n",
            "   \"city\": \"Chicago\",\n",
            "   \"state\": \"IL\",\n",
            "   \"country\": \"USA\"\n",
            "}\n",
            "{\"city\": \"New York\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\"city\": \"\", \"state\": \"\", \"country\": \"United States\"}\n",
            "{\n",
            "  \"city\": \"\",\n",
            "  \"state\": \"\",\n",
            "  \"country\": \"United States\"\n",
            "}\n",
            "{\"city\": \"Mesa\", \"state\": \"AZ\", \"country\": \"USA\"}\n",
            "{\n",
            "\"city\": \"\",\n",
            "\"state\": \"\",\n",
            "\"country\": \"United States\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "def generate_location_json_file(ind, location_val):\n",
        "\n",
        "  prompt = \"Take the following python list and create a JSON dictionary containing 'city', 'state', and 'country' as keys and fill out the values. If the list is ['Colorado Springs, CO'] you would return {'city': 'Colorado Springs', 'state': 'CO', 'country': 'USA'}. If the list is instead Return only the JSON dictionary, and make sure to use double quotes even though I used single in this prompt. I want you to do it, not to tell me how to code it. I want you to do it for: \"\n",
        "\n",
        "  prompt = prompt + \"/n\" + str(location_val)\n",
        "\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        "  )\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "  )\n",
        "  json_files_dir = r\"drive/MyDrive/jobLocations/\"\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "  json_data = json.loads(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "  json_file_path = json_files_dir + f'row{ind + 1}.json'\n",
        "  # Write JSON data to the file\n",
        "  with open(json_file_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file)\n",
        "\n",
        "  return json_file_path\n"
      ],
      "metadata": {
        "id": "HoHTLrTrwn-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent, so we have generated the JSON files for the location column data. This will prove helpful when we make new feature columns for city, state, and country. So far we have two columns which we may choose to treat differently from the rest: the employment_type column which we might drop and the location columns which we will treat as having categorical data."
      ],
      "metadata": {
        "id": "bSrkJpTUQRL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az1ALhuwyRkb",
        "outputId": "05118456-98e5-4b1b-8e24-ffb55c83f160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m204.8/227.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['salary/compensation_range']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocb7CBhvRfGe",
        "outputId": "8aa7f2b5-88b3-4f29-b14c-0f19f68e1761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['$30.00/hr - $45.00/hr']\n",
            "nan\n",
            "['N/A']\n",
            "['$150K - $200K']\n",
            "['N/A']\n",
            "nan\n",
            "['N/A']\n",
            "['N/A']\n",
            "nan\n",
            "nan\n",
            "['$100,000.00/yr', '$175,000.00/yr']\n",
            "nan\n",
            "['$50.00/hr - $60.00/hr', 'Compensation: $50/hr to $65/hr']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "nan\n",
            "['N/A']\n",
            "['$129,000.00/yr', '$200,000.00/yr']\n",
            "['N/A']\n",
            "['$216,000.00/yr - $414,000.00/yr']\n",
            "nan\n",
            "['N/A']\n",
            "['$58,300.00/yr - $133,000.00/yr']\n",
            "['$126,000.00/yr - $189,000.00/yr']\n",
            "['N/A']\n",
            "['N/A']\n",
            "nan\n",
            "['$70,000.00/yr - $100,000.00/yr']\n",
            "['N/A']\n",
            "nan\n",
            "['N/A']\n",
            "['$170,000.00/yr - $200,000.00/yr']\n",
            "['N/A']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['$150,000.00/yr - $200,000.00/yr']\n",
            "nan\n",
            "['N/A']\n",
            "['$154,000.00/yr - $236,000.00/yr']\n",
            "nan\n",
            "nan\n",
            "['$115,000.00/yr - $223,600.00/yr']\n",
            "nan\n",
            "nan\n",
            "['$150,000.00/yr - $220,000.00/yr']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "nan\n",
            "nan\n",
            "['N/A']\n",
            "['$117,000.00/yr - $173,000.00/yr']\n",
            "['N/A', 'Annual Salary Range: $181,145.00-$289,863.00']\n",
            "['N/A']\n",
            "nan\n",
            "nan\n",
            "['$107,000.00/yr - $162,000.00/yr']\n",
            "['$100,000.00/yr - $140,000.00/yr']\n",
            "['N/A']\n",
            "['N/A']\n",
            "['N/A']\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "['N/A']\n",
            "nan\n",
            "['$192,000.00/yr', '$288,000.00/yr']\n",
            "nan\n",
            "['N/A']\n",
            "['N/A']\n",
            "['$120,000.00/yr - $170,000.00/yr']\n",
            "['$175,000.00/yr - $275,000.00/yr']\n",
            "['$90,000.00/yr - $170,000.00/yr']\n",
            "['N/A']\n",
            "nan\n",
            "['N/A']\n",
            "['N/A']\n",
            "['$245,000.00/yr - $385,000.00/yr']\n",
            "['N/A']\n",
            "['$122,000.00/yr - $132,000.00/yr']\n",
            "nan\n",
            "['$60,000.00/yr - $90,000.00/yr']\n",
            "nan\n",
            "nan\n",
            "['N/A']\n",
            "['$137,500.00/yr', '$220,000.00/yr']\n",
            "['N/A']\n",
            "nan\n",
            "['N/A']\n",
            "['$10,120.00/yr - $12,170.00/yr', 'USD $5,090 - $13,240 per month']\n",
            "['$98,144.00/yr - $216,646.00/yr']\n",
            "nan\n",
            "['$130,000.00/yr - $150,000.00/yr']\n",
            "['N/A']\n",
            "['$198,200.00/yr - $297,300.00/yr']\n",
            "['$100,000.00/yr', '$130,000.00/yr']\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might want to do what we did for the 'location' data and have ChatGPT summarize our data into JSON files with two keys. salary_min and salary_max."
      ],
      "metadata": {
        "id": "nplgnsb-Sn_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting started with OpenAI API!\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "def generate_salary_json_file(ind, location_val):\n",
        "\n",
        "  prompt = \"Take the following python data and infer the minimum and maximuim of the salary range, and fill out their values as floating point numbers with three decimal places in units of thousands in a JSON dictionary, with 'salary_min' and 'salary_max' being the keys. If the data given is ['$150,000.00/yr - $220,000.00/yr'] then you should return {'salary_min': 150, 'salary_max': 220}, but make sure to use double quotes to enclose the key names. If you infer that info is in dollars per hour, convert the numbers to annual salary in thousands so output is same regardless of given units. Note that $48/hr is equal to $100,000/yr. Put 'N/A' under a fields if the required information is not given. If only one number is given put it under 'salary_max'. Return only the JSON dictionary. I want you to do it, not to tell me how to code it. I want you to do it for: \"\n",
        "\n",
        "  prompt = prompt + \"/n\" + str(location_val)\n",
        "\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=\"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\",\n",
        "  )\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "  )\n",
        "  json_files_dir = r\"drive/MyDrive/jobSalaries/\"\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "  json_data = json.loads(chat_completion.choices[0].message.content.strip(\"`\").strip('json').strip())\n",
        "  json_file_path = json_files_dir + f'row{ind + 1}.json'\n",
        "  # Write JSON data to the file\n",
        "  with open(json_file_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file)\n",
        "\n",
        "  return json_file_path\n"
      ],
      "metadata": {
        "id": "zVbnyj15TMyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, value in enumerate(df['salary/compensation_range']):\n",
        "  generate_salary_json_file(index, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mYJcOB8WoKJ",
        "outputId": "afaf990b-b50b-4689-cd6c-feff32e9c7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"salary_min\": 62.4,\n",
            "    \"salary_max\": 93.6\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\"salary_min\": 150, \"salary_max\": 200}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 100,\n",
            "    \"salary_max\": 175\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 100,\n",
            "    \"salary_max\": 130\n",
            "}\n",
            "{\n",
            "\"salary_min\": \"N/A\",\n",
            "\"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 129,\n",
            "    \"salary_max\": 200\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 216,\n",
            "    \"salary_max\": 414\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 58.3,\n",
            "    \"salary_max\": 133\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 126,\n",
            "    \"salary_max\": 189\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 70,\n",
            "    \"salary_max\": 100\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 170,\n",
            "    \"salary_max\": 200\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 150,\n",
            "    \"salary_max\": 200\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 154,\n",
            "    \"salary_max\": 236\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 115,\n",
            "    \"salary_max\": 223.6\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 150,\n",
            "    \"salary_max\": 220\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": 117,\n",
            "  \"salary_max\": 173\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 181.145,\n",
            "    \"salary_max\": 289.863\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\"salary_min\": 107, \"salary_max\": 162}\n",
            "{\"salary_min\": 100, \"salary_max\": 140}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\"salary_min\": 192, \"salary_max\": 288}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 120,\n",
            "    \"salary_max\": 170\n",
            "}\n",
            "{\n",
            "  \"salary_min\": 175,\n",
            "  \"salary_max\": 275\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 90,\n",
            "    \"salary_max\": 170\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 245,\n",
            "    \"salary_max\": 385\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "\t\"salary_min\": 122,\n",
            "\t\"salary_max\": 132\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\"salary_min\": 60, \"salary_max\": 90}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 137.5,\n",
            "    \"salary_max\": 220\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": \"N/A\",\n",
            "  \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 101,\n",
            "    \"salary_max\": 122\n",
            "}\n",
            "{\"salary_min\": 98.144, \"salary_max\": 216.646}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": 130,\n",
            "    \"salary_max\": 150\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "  \"salary_min\": 198.2,\n",
            "  \"salary_max\": 297.3\n",
            "}\n",
            "{\"salary_min\": 100, \"salary_max\": 130}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n",
            "{\n",
            "    \"salary_min\": \"N/A\",\n",
            "    \"salary_max\": \"N/A\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKyU9qijq9fv",
        "outputId": "a75af583-7b1c-4340-d7fa-138c522c6bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['emploment_type', 'job_function', 'description_of_product/service',\n",
            "       'industries', 'position_name', 'broader_role_name', 'company',\n",
            "       'location', 'salary/compensation_range', 'responsibilities',\n",
            "       'goals/objectives', 'name_of_department/team',\n",
            "       'required_qualifications', 'preferred_qualifications', 'benefits',\n",
            "       'work_arrangement', 'rating'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['responsibilities']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojk90KjlYMxg",
        "outputId": "f5bd94aa-5553-4a55-dca1-9417a14bf830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['integrate software/hardware components', 'develop UI front-end', 'investigate defects', 'be active team player', 'learn new systems/tools', 'document code quality', 'work on python applications']\n",
            "['Contribute to cutting-edge robotic systems', 'Collaborate with cross-functional teams', 'Implement advanced control algorithms', 'Optimize system performance', 'Stay current with emerging technologies']\n",
            "['Help with initial robotic deployments', 'Plan roadmap, design ML systems, implement, test, monitor services']\n",
            "['Application Development', 'Collaboration', 'Mentorship', 'Establish Best Practices']\n",
            "['Build and maintain automated test infrastructure', 'Write and execute end-to-end integration scenarios', 'Collaborate with business groups and external teams', 'Partner with developers', 'Collaborate with DevOps', 'Set up test pipelines', 'Test complex data pipelines', 'Test serverless-based architecture on AWS', 'Deploy tests in AWS', 'Use Jira or qTest for tracking']\n",
            "['Design, develop, and refine prompts', 'Build scalable, automated workflows', 'Engage in thought leadership', 'Collaborate with teams to ensure alignment']\n",
            "['Execute creative software solutions', 'Develop secure high-quality code', 'Lead evaluation sessions', 'Drive awareness of new technologies']\n",
            "['Design, Develop, Troubleshoot, Architect, Analyze']\n",
            "['Build data pipelines', 'Research trading strategies', 'Enhance existing strategies', 'Create new strategies']\n",
            "['Write well designed code with automated testing', 'Operations and maintenance of Data.gov websites', 'Triage security and compliance issues', 'Migration to cloud infrastructure', 'Operate within agile scrum framework']\n",
            "['Design and implement network connectivity', 'Enhance existing components for performance', 'Collaborate with strategy teams', 'Develop components for data collection', 'Maintain applications and connections']\n",
            "['Train deep neural networks', 'Understand training data distributions', 'Develop metrics for model performance', 'Deploy trained networks on robotic system']\n",
            "['Build bioinformatics pipelines', 'Write Python/R scripts', 'Ensure pipeline flexibility', 'Implement reporting system', 'Provide documentation', 'Create web-based interfaces']\n",
            "['Workstream planning', 'Design collaboration', 'User story documentation', 'End-to-end process integration', 'Training support', 'Data cleansing direction']\n",
            "['Design, develop, and implement ML models', 'Utilize Python programming language', 'Collaborate with cross-functional teams', 'Evaluate and fine-tune models']\n",
            "['ETL Automation (~60%)', 'Front-End Dashboarding (~20%)', 'Documentation and Miscellaneous (20%)']\n",
            "['Developing low latency trading systems']\n",
            "['Work with Embedded Open-Source tools']\n",
            "['Identify gaps in AR hardware roadmap', 'Drive development of clear requirements', 'Lead prototyping vehicles engineering process', 'Communicate technology, plans effectively']\n",
            "['Functional expert for SAP Analytics Cloud (SAC)', 'Lead product team', 'Perform cost-benefit analyses', 'Collaborate with stakeholders', 'Identify cost savings initiatives', 'Automate processes', 'Drive innovative thinking', 'Lead ERP transformation project team']\n",
            "['Craft next gen SoC, CPUs', 'Provide direction for software design', 'Collaborate with hardware/software engineers', 'Research industry directions']\n",
            "['External research funding', 'Research portfolio accomplishments', 'National awards honors']\n",
            "['Prepares day-to-day and month-end close entries', 'Prepares complex reconciliations', 'Assists with month-end and year-end closing process', 'Assists in preparing working papers for external audit and tax preparation', 'Makes recommendations for improving accounting processes', 'Interacts with internal clients and responds to requests']\n",
            "['Analyze system details', 'Identify security requirements', 'Guide client through a plan of action']\n",
            "['Contribute to DevOps methodology', 'Ensure product quality and best practices', 'Drive packaging and deployment strategies', 'Interact with cross-functional teams', 'Troubleshoot applications and make enhancements']\n",
            "['Run reports to support customer meetings', \"Create PowerPoint presentations for ISR's and Field Sales\", 'Process manual orders for dealers', 'Partner with internal business partners', 'Maintain and monitor customer portals on Generac websites', 'Create and maintain process documents for team', 'Process name changes and business changes for dealers', 'Proactively monitor sales levels for dealers', 'Act as liaison between sales teams and customers', 'Enhance processes by utilizing Continuous Improvement mindset', 'Manage inbound and outbound calls and email correspondence', 'Support non-revenue generated work for sales teams']\n",
            "['Maintain annual sales plan', 'Manage customer relationships', 'Develop new customer relationships', 'Anticipate customer needs', 'Develop project proposals', 'Follow-up on proposals', 'Process orders', 'Provide customer service', 'Utilize CRM tool', 'Prepare estimates', 'Travel to job sites', 'Participate in meetings', 'Develop supplier relationships', 'Perform market research']\n",
            "['Financial Record Keeping', 'Reconciliation', 'Month-end Closing', 'Manage Quickbooks', 'Process credit applications', 'Reconcile chargebacks']\n",
            "['Developing machine learning algorithms', 'Testing algorithms on data system', 'Collaborating with research staff', 'Communicating research results']\n",
            "['Firmware development', 'DRM implementation', 'Audio/Video analysis']\n",
            "['Manage performance testing', 'Identify bottlenecks', 'Automate testing process', 'Assist cloud enablement', 'Create telemetry', 'Determine capacity testing', 'Generate dashboards']\n",
            "['C++ Developer', 'Core Windows development', 'bug fix', 'feature level work', 'debugging in C++', 'feature validations', 'issues triage', 'driving bug fixes', 'code reviews', 'handling end-to-end delivery', 'debug issues with windbg/similar debuggers', 'incident triage', 'investigation and analysis', 'scenario validation', 'building automation', 'drive regular scenario validation', 'building effective documentation', 'reporting feature development status']\n",
            "['Design experimental frameworks', 'Develop and optimize machine learning models for real-time, edge applications', 'Assess effectiveness of models', 'Stay updated on advancements in machine learning and signal processing', 'Translate research findings into actionable insights', 'Integrate machine learning models into products', 'Provide technical guidance and mentorship']\n",
            "['Create automation scripts', 'Analyze Business Requirements', 'Participate in development and testing sessions', 'Prepare test execution plan', 'Coordinate with QA team', 'Debug and fix issues', 'Automate customer scenarios']\n",
            "['One-to-one and small group instruction', 'Implement lesson plans', 'Interact positively with clients and staff', 'Manage client records']\n",
            "['Development of EDA software tools and flows', 'Support test chip layout', 'Collaborate with process technologists', 'Troubleshoot design and verification issues', 'Build Design of Experiments', 'Develop Design Rule Check software']\n",
            "['Testing', 'Software Process', 'Coding', 'Requirements Documentation/Software Project Scope']\n",
            "['Design and develop software systems for research and production processes', 'Manage CI/CD pipeline and troubleshoot issues', 'Assist in building efficient tools for data organization and visualization', 'Automate order generation and integrate compliance checks', 'Build desktop UI tools for portfolio monitoring']\n",
            "['Algorithm analysis', 'Performance analysis', 'Architecture definition', 'Map Data Center workloads', 'Perform detailed calculations', 'Drive architecture definition', 'Identify appropriate workloads', 'Evangelize architectural solutions', 'Collaborate with cross functional teams']\n",
            "['Deep learning engineering', 'NLP/LLM processing', 'PySpark/Databricks programming', 'Model deployment', 'Backend application building', 'Vector databases knowledge', 'Transformers experience', 'Cloud computing']\n",
            "['Consult with customers to identify appropriate Google data management solutions', 'Work with Product Management teams to assist market adoption and solution alignment with enterprise customer requirements', 'Manage Databases workloads within assigned solutions and region', \"Lead field teams and Databases counterparts to drive demand, accelerate execution, and bring real digital transformation through Google's Databases Platform\", 'Scale capability, execution, and experience through Product, Engineering, and Partner teams, and drive customer success through prescriptive activation of internal and external teams']\n",
            "['N/A']\n",
            "['Lead Software Engineer', 'Research fundamental problems', 'Implement algorithm solutions', 'Participate in project leadership', 'Contribute to advanced technical research', 'Provide reliable solutions']\n",
            "['Developing test strategies', 'Creating test harnesses', 'Providing test infrastructure', 'Coordinating with offshore team', 'Developing test plans']\n",
            "['Creative software solutions', 'Technical troubleshooting', 'Production code development', 'Evaluation sessions', 'Maintenance automation']\n",
            "['Guide engineering teams in multi-discipline approach', 'Requirements engineering', 'Solutions engineering', 'Integration', 'Test and evaluation', 'Maintainability analysis']\n",
            "['Develop edge-computing stack', 'Deploy machine learning models', 'Optimize platform performance', 'Build edge applications']\n",
            "['Evaluate AI-generated code', 'Solve coding problems', 'Optimize code efficiency', 'Write robust test cases']\n",
            "['Initial defect analysis - Triage defects - Monitor defect dashboard - Create reports - Work with cross-functional teams - Drive defect triage calls']\n",
            "['Develop software for data processing', 'Contribute to scientific code development', 'Research scientific reference material', 'Document software specifications', 'Support Business Development activities']\n",
            "['Research and apply large-scale models', 'Optimize enterprise applications', 'Implement relevant applications', 'Collaborate with cross-functional teams', 'Research and explore usage scenarios']\n",
            "['Develop software for earth observation data', 'Contribute to scientific processing code', 'Research scientific reference material', 'Document software specifications', 'Refine aerospace/scientific software', 'Collaborate with a dynamic team', 'Support Business Development activities']\n",
            "['Research and Development', 'Taking Initiative on Projects']\n",
            "['Python, Linux, C/C++, Shell Scripting', 'Optimizing and integrating AI model', 'Leading small team']\n",
            "['Provide academic support through tutoring', 'Maintain accurate tutoring records', 'Communicate effectively with faculty']\n",
            "['Maintain marketing and servicing websites', 'Implement customer opt-out choices', 'Collaborate with stakeholders', 'Enforce compliance with regulations']\n",
            "['SEO strategies', 'Content creation', 'Collaboration with teams', 'Keyword research', 'Content calendar management', 'Performance analysis']\n",
            "['Maintain clean, safe space', 'Respond to needs/questions', 'Handle emergencies', 'Work with staff/volunteers/interns', 'Passion for local agriculture']\n",
            "['Write research code', 'Publish research papers', 'Design experiments']\n",
            "['Drive customer engagements', 'Guide account executives', 'Provide feedback on market trends', 'Identify new business opportunities', 'Interact with technical decision makers', 'Facilitate financial analyses']\n",
            "['Lead, collaborate, execute innovative AI research projects', 'Identify new directions, formulate objectives aligned with mission', 'Contribute to development of advanced ML models, algorithms, datasets', 'Collaborate with diverse team, foster open communication, shared learning', 'Engage in interdisciplinary research efforts for high-impact outcomes', 'Publish in top-tier conferences, journals, communicate research findings', 'Create demos/systems to highlight AI innovations, foster collaboration', 'Contribute to mentoring junior team members, foster growth']\n",
            "['Collaborate with cross-functional teams for AI-based product development', 'Design and develop software solutions using AI technologies', 'Implement algorithms, models, and data pipelines for decision-making and automation', 'Optimize and improve existing AI systems and algorithms', 'Conduct testing and debugging of software components', 'Stay up-to-date with advancements in AI technologies', 'Collaborate with stakeholders for feedback on software solutions', 'Document software designs, processes, and technical specifications']\n",
            "['Lead complex software engineering projects', 'Develop and maintain code', 'Integrate software components', 'Create and test procedures', 'Document deployed processes', 'Troubleshoot software issues']\n",
            "['Client strategy design', 'Solution definition', 'Educational support', 'Credibility building']\n",
            "['software design and development on complex, tactical geophysical systems', 'code and unit testing', 'troubleshooting', 'guidance and/or execution on corrective actions', 'recommendations for design enhancement', 'support integration and test actions']\n",
            "['Requirement gathering and documentation', 'Enterprise architecture focus on AI models', 'Model-based development review', 'Data synchronization principles', 'ServiceNow familiarity']\n",
            "['Participant communication', 'Data management', 'Research materials preparation']\n",
            "['Innovate new ways', 'Solve problems', 'Research and solve', 'Provide customer service']\n",
            "['Design data pipelines', 'Develop data quality metrics', 'Communicate project status', 'Collaborate with business', 'Analyzing data integration problems']\n",
            "['Designing and maintaining software services', 'Integration of architectural development', 'Set directions on hardware/software platforms', 'Design and develop GenAI use case model pipeline', 'Interact with industry, standards, suppliers']\n",
            "['Unity development', 'XR application development', 'ARCore', 'ARFoundation', 'OpenXR libraries', 'UX design', 'Programming languages']\n",
            "['Interact web services/APIs', 'Design chatbot structure', 'Implement NLP techniques', 'Train/deploy AI models', 'Analyze chatbot data', 'Comply with data privacy', 'Collaborate with teams', 'Technical writing', 'Initiate proof of concept']\n",
            "['Create and deploy campaigns', 'Analyze campaigns', 'Manage paid search campaigns', 'Communicate with clients and stakeholders', 'Marketing efforts']\n",
            "['Technical Sales management', 'Relationship development', 'Technical proposals development', 'Customer discussions', 'RFQs and RFIs support', 'Forecasting reports generation', 'Product development input', 'Trade shows attendance', 'Interaction with factory management']\n",
            "['Technical Pre-Sales Support', 'Trial Support', 'Post-Sales Technical Support', 'Data Dispute Resolution']\n",
            "['Technical leadership on team building and maintaining services for data science and ML at Klaviyo', 'Develop tools for training, testing, serving, and monitoring models']\n",
            "['Build, install, configure, manage, scale ML platform', 'Implement scalable ML/DL solutions', 'Create & maintain ML/DL pipelines', 'Address performance, scalability, governance of ML models', 'Stay updated on latest ML/DL technologies']\n",
            "['Conduct research on advanced techniques', 'Collaborate with stakeholders', 'Implement strategies for alignment', 'Gather requirements for modeling techniques']\n",
            "['Domain expert in RTML serving technology', 'Define technical strategy and architecture', 'Lead development activities', 'Support internal customers', 'Mentor junior developers', 'Adhere to industry standards']\n",
            "['N/A']\n",
            "['Define architecture and build infrastructure for ML solutions', 'Integrate best practices from ML research into systems', 'Create data preprocessing pipelines', 'Implement CI/CD pipelines for ML model deployment', 'Optimize and scale systems', 'Monitor and analyze system health', 'Identify and solve issues for uninterrupted service']\n",
            "['Develop software for large datasets']\n",
            "['Transportation of passengers', 'Dispatching', 'Scheduling', 'Pre-trip inspections', 'Record maintenance', 'Professional communication']\n",
            "['Trains on specific laboratory aspects', 'Assures lab staff has kits accessible', 'Timely packing and shipping of lab samples', 'Maintains patient data files', 'Collects patient data', 'Assists with scheduling appointments', 'Prepares patient records for audits', 'Maintains supplies for clinical research', 'May assist with direct patient care']\n",
            "['Solution Development - Hands-on Prototyping - Strategy - Collaboration - Mentoring']\n",
            "['Develop, design, test software of embedded devices/systems', 'Monitor, enhance system efficiency/stability', 'Gather/analyze user requirements', 'Implement source codes', 'Test/debug system software', 'Collaborate with other teams', 'Design/Document Cybersecurity features', 'Validate Cybersecurity features', 'Identify Cybersecurity risks']\n",
            "['Partner with research, product, and design', 'Design and build front-end systems', 'Create inclusive culture', 'Plan and deploy infrastructure']\n",
            "['Developing Microservices', 'Utilizing Python, AWS, Gitlab', 'CICD, REST APIs, Kubernetes', 'Docker, Messaging Tools (Kafka, Redis)']\n",
            "['Troubleshooting software-centric electromechanical vehicle systems', 'Software design, coding, testing, debugging', 'Collaborating with Customer Support, Engineering, Manufacturing']\n",
            "['Optimize and fine-tune existing training and inference platform', 'Collaborate with cross-functional teams', 'Develop own ideas to optimize platforms', 'Stay up-to-date with latest advancements in ML systems']\n",
            "['Design, architect, and build AI functionality', 'Use large language model APIs', 'Participate in AI technical process standards', 'Build machine learning models', 'Participate in team building activities', 'Encourage and mentor Open Source contributors', 'Represent OEF at standards discussions']\n",
            "['Developing software for finance industry']\n",
            "['Design, develop, maintain NWP workflow features', 'Follow software best practices', 'Create documentation for software']\n",
            "['Drive security tests', 'Perform vulnerability assessments', 'Assist with security automation', 'Coordinate findings with leadership']\n",
            "['Collaborating multidisciplinary teams', 'Implementing AI solutions', 'Designing, developing AI algorithms', 'Ensuring software reliability', 'Documenting code and processes', 'Expressing AI dedication']\n",
            "['Build awareness-increase knowledge-drive adoption-operate as trusted advisor-lead talent development-collaborate on key innovation initiatives']\n",
            "['Engage with customers', 'Demonstrate proof of concept', 'Define technical specifications', 'Create technical content', 'Represent QES with business development']\n",
            "['Holography and Laser Beam Optimization', 'Algorithm Development', 'Experimental Support and Data Analysis']\n",
            "['Problem identification', 'Formulation', 'Hypotheses', 'Publishing results', 'Collaboration']\n",
            "['Designing qubit devices for error rates, qubit physics modeling, communication with team']\n",
            "['Guide development with C++', 'Analyze legacy systems', 'Design scalable software components', 'Lead modernization initiatives', 'Collaborate with cross-functional teams', 'Provide technical guidance', 'Create architecture documentation', 'Drive system performance tuning', 'Develop algorithms for parallel computation']\n",
            "['Work alongside BE Engineers', 'Build machine learning models', 'Utilize computer vision technology', 'Report directly into the Head of Artificial Intelligence']\n",
            "['Standard software solutions execution', 'Collaboration with Quant Researchers and Business Users', 'Coding in python and React/Java Script', 'Gaining exposure to Pricing, Risk, and Trade Management functions']\n",
            "['Technical Leadership - Backend infrastructure design, development, maintenance', 'Scalability - Architect and implement scalable solutions', 'Integration - Collaborate with teams and services for integration', 'Performance Optimization - Monitor system performance, identify bottlenecks', 'Data Management - Data storage, retrieval, indexing strategies', 'Security - Implement security measures', 'Automation - Develop and maintain automated processes', 'Collaboration - Work with other teams for seamless communication', 'Mentorship - Provide technical guidance and mentorship', 'Problem Solving - Identify challenges, propose solutions']\n",
            "['Translating trading strategies', 'Design, code, build and test algorithms', 'Troubleshooting and debugging programs', 'Collaborating with team members', 'Analyzing user feedback and making adjustments', 'Recommending and executing program improvements']\n",
            "['Develops reusable architectures and designs for kernel software', 'Subject matter expert for kernel internals', 'Organizes backlog for time-phasing', 'Participates in daily scrums']\n",
            "['Develop text processing models', 'Design ML models at scale', 'Collaborate with MLOps team']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['rating']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtN4-aKNYZnZ",
        "outputId": "8011943c-aa41-418b-b69e-f10f56ee7880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "1\n",
            "3\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "0\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "0\n",
            "0\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just thinking about it now and I am fearing that the pre-trained 17GB Word2Vec model I downloaded earlier probably won't work and we may need to fit one to our training data set, which will come from their corresponding text files concatenated."
      ],
      "metadata": {
        "id": "5OxV8UEnYqTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll embark on gathering all the code we've written to get our dataframe data in the desired format. We will gather all columns from the original JSON file but then we will create new columns for city, state, country, min_salary and max_salary. After that we will either represent each unique phrase as a unique number or better yet shrink the feature space to categories of similar enough phrases using some kind of clustering or grouping algorithm."
      ],
      "metadata": {
        "id": "7mZMku-EG081"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_files_path = \"drive/MyDrive/LI-Jobs-JSON/\"\n",
        "links_csv_path = \"drive/MyDrive/successfulLinksGDRIVE.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "links_dataframe = pd.read_csv(links_csv_path, header=None, names=['url', 'rating'])\n",
        "#links_dataframe.head()\n",
        "#print(links_dataframe.loc[0, 'rating'])\n",
        "\n",
        "# let's snakecase our column names to avoid having spaces in them\n",
        "# also we add a rating column at the end of the list to store the target variable\n",
        "df_columns = [\"employment_type\", \"job_function\", \"description_of_product/service\", \"industries\",\n",
        "              \"position_name\", \"broader_role_name\", \"company\", #\"location\", \"salary/compensation_range\",\n",
        "              \"responsibilities\", \"goals/objectives\", \"name_of_department/team\", \"required_qualifications\",\n",
        "              \"preferred_qualifications\", \"benefits\", \"work_arrangement\", \"city\", \"state\", \"country\",\n",
        "              \"min_salary\", \"max_salary\", \"rating\"]\n",
        "# Initialize DataFrame with column names\n",
        "df = pd.DataFrame(columns=df_columns)\n",
        "\n",
        "import os\n",
        "import json\n",
        "locale_json_files_path = \"drive/MyDrive/jobLocations/\"\n",
        "salary_json_files_path = \"drive/MyDrive/jobSalaries/\"\n",
        "\n",
        "for i in range(1, 108):\n",
        "  row_num = df.last_valid_index()\n",
        "  print(row_num)\n",
        "  if row_num == None:\n",
        "    row_num = 0\n",
        "  else:\n",
        "    row_num = row_num + 1\n",
        "  assert i == row_num + 1\n",
        "  json_file_path = os.path.join(json_files_path, f'row{i}.json')\n",
        "  # Read JSON file\n",
        "  with open(json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  if 'fields' in json_data.keys():\n",
        "    #print(json_data)\n",
        "    field_names = json_data['fields']\n",
        "    for index, value in enumerate(field_names):\n",
        "      info = json_data['info'][index]\n",
        "      # Check if the value exists as a column name (ignoring case)\n",
        "      column_name = value.replace(\" \", \"_\").lower()\n",
        "      if column_name in df.columns:\n",
        "        # Add the info to the corresponding column and row\n",
        "        df.at[row_num, column_name] = info\n",
        "  else:\n",
        "    for key, value in json_data.items():\n",
        "      # Check if the key exists as a column name (ignoring case)\n",
        "      column_name = key.replace(\" \", \"_\").lower()\n",
        "      if column_name == 'emploment_type':\n",
        "        column_name = 'employment_type'\n",
        "      if column_name in df.columns:\n",
        "        # Add the value to the corresponding column and row\n",
        "        df.at[row_num, column_name] = value\n",
        "\n",
        "  locale_json_file_path = os.path.join(locale_json_files_path, f'row{i}.json')\n",
        "  salary_json_file_path = os.path.join(salary_json_files_path, f'row{i}.json')\n",
        "  # Read locale JSON file\n",
        "  with open(locale_json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  try:\n",
        "    city = json_data[\"city\"]\n",
        "    state = json_data[\"state\"]\n",
        "    country = json_data[\"country\"]\n",
        "    df.at[row_num, 'city'] = city\n",
        "    df.at[row_num, 'state'] = state\n",
        "    df.at[row_num, 'country'] = country\n",
        "  except:\n",
        "    df.at[row_num, 'city'] = \"N/A\"\n",
        "    df.at[row_num, 'state'] = \"N/A\"\n",
        "    df.at[row_num, 'country'] = \"N/A\"\n",
        "  # Read salary JSON file\n",
        "  with open(salary_json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "  min_salary = json_data[\"salary_min\"]\n",
        "  max_salary = json_data[\"salary_max\"]\n",
        "  df.at[row_num, 'min_salary'] = min_salary\n",
        "  df.at[row_num, 'max_salary'] = max_salary\n",
        "  rating = links_dataframe.loc[row_num, 'rating']\n",
        "  df.at[row_num, 'rating'] = rating\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP3KbtqFGyIm",
        "outputId": "04488335-456b-4bd3-d2e2-fa10e04f7e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "g9Ddk0hQPqR1",
        "outputId": "63be0710-fade-441b-ddfa-7667a92a8466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  employment_type                                       job_function  \\\n",
              "0     [Full-time]              [Engineering, Information Technology]   \n",
              "1     [Full-time]     [Design, Art/Creative, Information Technology]   \n",
              "2     [Full-time]  [Information Technology, Consulting, Engineering]   \n",
              "3     [Full-time]              [Engineering, Information Technology]   \n",
              "4     [Full-time]  [Engineering, Quality Assurance, Information T...   \n",
              "\n",
              "                      description_of_product/service  \\\n",
              "0  [design-led software development, end-to-end d...   \n",
              "1                        [Surgical Robotics Systems]   \n",
              "2  [Amazon Robotics builds high-performance, real...   \n",
              "3                            [Credit scoring models]   \n",
              "4  [Large-scale distributed software applications...   \n",
              "\n",
              "                                          industries  \\\n",
              "0                    [Business Consulting, Services]   \n",
              "1  [Biotechnology Research, Pharmaceutical Manufa...   \n",
              "2  [Software Development, IT Services, IT Consult...   \n",
              "3  [Financial Services, Capital Markets, IT Servi...   \n",
              "4                       [IT Services, IT Consulting]   \n",
              "\n",
              "                                       position_name    broader_role_name  \\\n",
              "0  [Python Software Engineer (Robotics/Mechatroni...                [N/A]   \n",
              "1                                [Robotics Engineer]                [N/A]   \n",
              "2                            [SDE - Amazon Robotics]                [N/A]   \n",
              "3                         [Senior Software Engineer]  [Software Engineer]   \n",
              "4                        [Software Engineer in Test]                [N/A]   \n",
              "\n",
              "              company                                   responsibilities  \\\n",
              "0  [Fresh Consulting]  [integrate software/hardware components, devel...   \n",
              "1  [Barrington James]  [Contribute to cutting-edge robotic systems, C...   \n",
              "2            [Amazon]  [Help with initial robotic deployments, Plan r...   \n",
              "3     [VantageScore®]  [Application Development, Collaboration, Mento...   \n",
              "4            [Optomi]  [Build and maintain automated test infrastruct...   \n",
              "\n",
              "                                    goals/objectives name_of_department/team  \\\n",
              "0             [manage delivery of high-quality work]                   [N/A]   \n",
              "1                                                NaN                     NaN   \n",
              "2  [Build high-performance robotic systems, Inven...                   [N/A]   \n",
              "3                             [Building public APIs]                   [N/A]   \n",
              "4                                              [N/A]                   [N/A]   \n",
              "\n",
              "                             required_qualifications  \\\n",
              "0  [0-1+ years experience, Python skills, program...   \n",
              "1  [Bachelor's, Master's, or Ph.D. in Robotics, M...   \n",
              "2  [3+ years professional software development ex...   \n",
              "3  [Bachelor's degree, Master's degree, Computer ...   \n",
              "4  [5+ years of test automation experience, Profi...   \n",
              "\n",
              "                            preferred_qualifications  \\\n",
              "0  [clear communication, outside the box thinking...   \n",
              "1                                              [N/A]   \n",
              "2  [3+ years full software development life cycle...   \n",
              "3  [Quantitative applications, Fintech experience...   \n",
              "4                                              [N/A]   \n",
              "\n",
              "                                            benefits work_arrangement  \\\n",
              "0        [100% Medical, PTO, Holiday Pay, 401K Plan]            [N/A]   \n",
              "1                                              [N/A]            [N/A]   \n",
              "2                                              [N/A]        [On-site]   \n",
              "3  [401(K) match, Flexible Time Off, 12 Paid Holi...         [Hybrid]   \n",
              "4                                              [N/A]        [On-site]   \n",
              "\n",
              "                city state        country min_salary max_salary rating  \n",
              "0            Redmond    WA            USA       62.4       93.6      1  \n",
              "1           New York        United States        N/A        N/A      2  \n",
              "2      North Reading    MA            USA        N/A        N/A      2  \n",
              "3      San Francisco    CA            USA        150        200      3  \n",
              "4  Dallas-Fort Worth    TX            USA        N/A        N/A      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7155e2a7-6efe-4fb8-a44e-0011453dd0a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>employment_type</th>\n",
              "      <th>job_function</th>\n",
              "      <th>description_of_product/service</th>\n",
              "      <th>industries</th>\n",
              "      <th>position_name</th>\n",
              "      <th>broader_role_name</th>\n",
              "      <th>company</th>\n",
              "      <th>responsibilities</th>\n",
              "      <th>goals/objectives</th>\n",
              "      <th>name_of_department/team</th>\n",
              "      <th>required_qualifications</th>\n",
              "      <th>preferred_qualifications</th>\n",
              "      <th>benefits</th>\n",
              "      <th>work_arrangement</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>min_salary</th>\n",
              "      <th>max_salary</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Full-time]</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[design-led software development, end-to-end d...</td>\n",
              "      <td>[Business Consulting, Services]</td>\n",
              "      <td>[Python Software Engineer (Robotics/Mechatroni...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Fresh Consulting]</td>\n",
              "      <td>[integrate software/hardware components, devel...</td>\n",
              "      <td>[manage delivery of high-quality work]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[0-1+ years experience, Python skills, program...</td>\n",
              "      <td>[clear communication, outside the box thinking...</td>\n",
              "      <td>[100% Medical, PTO, Holiday Pay, 401K Plan]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>Redmond</td>\n",
              "      <td>WA</td>\n",
              "      <td>USA</td>\n",
              "      <td>62.4</td>\n",
              "      <td>93.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Full-time]</td>\n",
              "      <td>[Design, Art/Creative, Information Technology]</td>\n",
              "      <td>[Surgical Robotics Systems]</td>\n",
              "      <td>[Biotechnology Research, Pharmaceutical Manufa...</td>\n",
              "      <td>[Robotics Engineer]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Barrington James]</td>\n",
              "      <td>[Contribute to cutting-edge robotic systems, C...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Bachelor's, Master's, or Ph.D. in Robotics, M...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>New York</td>\n",
              "      <td></td>\n",
              "      <td>United States</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Full-time]</td>\n",
              "      <td>[Information Technology, Consulting, Engineering]</td>\n",
              "      <td>[Amazon Robotics builds high-performance, real...</td>\n",
              "      <td>[Software Development, IT Services, IT Consult...</td>\n",
              "      <td>[SDE - Amazon Robotics]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Amazon]</td>\n",
              "      <td>[Help with initial robotic deployments, Plan r...</td>\n",
              "      <td>[Build high-performance robotic systems, Inven...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[3+ years professional software development ex...</td>\n",
              "      <td>[3+ years full software development life cycle...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[On-site]</td>\n",
              "      <td>North Reading</td>\n",
              "      <td>MA</td>\n",
              "      <td>USA</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Full-time]</td>\n",
              "      <td>[Engineering, Information Technology]</td>\n",
              "      <td>[Credit scoring models]</td>\n",
              "      <td>[Financial Services, Capital Markets, IT Servi...</td>\n",
              "      <td>[Senior Software Engineer]</td>\n",
              "      <td>[Software Engineer]</td>\n",
              "      <td>[VantageScore®]</td>\n",
              "      <td>[Application Development, Collaboration, Mento...</td>\n",
              "      <td>[Building public APIs]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Bachelor's degree, Master's degree, Computer ...</td>\n",
              "      <td>[Quantitative applications, Fintech experience...</td>\n",
              "      <td>[401(K) match, Flexible Time Off, 12 Paid Holi...</td>\n",
              "      <td>[Hybrid]</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>CA</td>\n",
              "      <td>USA</td>\n",
              "      <td>150</td>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Full-time]</td>\n",
              "      <td>[Engineering, Quality Assurance, Information T...</td>\n",
              "      <td>[Large-scale distributed software applications...</td>\n",
              "      <td>[IT Services, IT Consulting]</td>\n",
              "      <td>[Software Engineer in Test]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[Optomi]</td>\n",
              "      <td>[Build and maintain automated test infrastruct...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[5+ years of test automation experience, Profi...</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[N/A]</td>\n",
              "      <td>[On-site]</td>\n",
              "      <td>Dallas-Fort Worth</td>\n",
              "      <td>TX</td>\n",
              "      <td>USA</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7155e2a7-6efe-4fb8-a44e-0011453dd0a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7155e2a7-6efe-4fb8-a44e-0011453dd0a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7155e2a7-6efe-4fb8-a44e-0011453dd0a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7cb0255-237f-442a-8a8b-0e645baaac64\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7cb0255-237f-442a-8a8b-0e645baaac64')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7cb0255-237f-442a-8a8b-0e645baaac64 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 107,\n  \"fields\": [\n    {\n      \"column\": \"employment_type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job_function\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description_of_product/service\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"industries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"position_name\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"broader_role_name\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"company\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"responsibilities\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"goals/objectives\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name_of_department/team\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"required_qualifications\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preferred_qualifications\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"benefits\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"work_arrangement\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 67,\n        \"samples\": [\n          \"Sunnyvale\",\n          \"Boulder\",\n          \"Dallas-Fort Worth\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"VA\",\n          \"FL\",\n          \"WA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"United States\",\n          \"N/A\",\n          \"USA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_salary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          70,\n          181.145,\n          170\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_salary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          100,\n          288,\n          93.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"0\",\n        \"max\": \"3\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2\",\n          \"0\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the non-special columns (not location or salary columns, and possibly not 'employment_type'), we want to iterate over all the elements of the dataframe and split each string in order to get a list of words, and then we will need further logic to fully clean those lists of words so that they do not contain any special characters like ! or ? or / or -. Before we do this we should replace any instances of [\"N/A\"] or \"N/A\" in our dataframe with NaN.    "
      ],
      "metadata": {
        "id": "_r3ul6MSZtWD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjDauA90yJ9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}